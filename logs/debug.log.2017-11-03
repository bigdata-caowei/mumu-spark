2017-11-03 08:57:07,922 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2017-11-03 08:57:08,406 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2017-11-03 08:57:08,438 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2017-11-03 08:57:08,438 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2017-11-03 08:57:08,438 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2017-11-03 08:57:08,438 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2017-11-03 08:57:08,438 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2017-11-03 08:57:08,828 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 54165.
2017-11-03 08:57:08,860 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2017-11-03 08:57:08,875 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2017-11-03 08:57:08,875 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2017-11-03 08:57:08,875 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2017-11-03 08:57:08,891 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-8344e8f6-e6f9-42df-9606-3f81e54280ba
2017-11-03 08:57:08,953 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2017-11-03 08:57:09,000 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2017-11-03 08:57:09,141 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3052ms
2017-11-03 08:57:09,207 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2017-11-03 08:57:09,223 INFO[org.spark_project.jetty.server.Server:403] - Started @3133ms
2017-11-03 08:57:09,254 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@e0d06bf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 08:57:09,254 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2017-11-03 08:57:09,269 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@30bcf3c1{/jobs,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,285 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ee37ca3{/jobs/json,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,285 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53812a9b{/jobs/job,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,285 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@422c3c7a{/jobs/job/json,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,285 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@d8305c2{/stages,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,285 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@75d0911a{/stages/json,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,285 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@60d1a32f{/stages/stage,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,285 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@726e5805{/stages/stage/json,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,285 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4b672daa{/stages/pool,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,285 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e077866{/stages/pool/json,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,285 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c2b6087{/storage,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,285 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a8e6492{/storage/json,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,301 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3b77a04f{/storage/rdd,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,301 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2e11485{/storage/rdd/json,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,301 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@662f5666{/environment,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,301 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5974109{/environment/json,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,301 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ef3efa8{/executors,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,301 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f8f9349{/executors/json,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,301 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7446d8d5{/executors/threadDump,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,301 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4fbda97b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,316 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@37b70343{/static,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,316 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51cd7ffc{/,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,316 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4cc6fa2a{/api,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,316 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6b9ce1bf{/jobs/job/kill,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,316 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@75ed9710{/stages/stage/kill,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,316 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2017-11-03 08:57:09,504 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2017-11-03 08:57:09,551 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54179.
2017-11-03 08:57:09,551 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:54179
2017-11-03 08:57:09,551 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2017-11-03 08:57:09,551 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 54179, None)
2017-11-03 08:57:09,551 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:54179 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 54179, None)
2017-11-03 08:57:09,598 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 54179, None)
2017-11-03 08:57:09,598 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 54179, None)
2017-11-03 08:57:09,879 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63fd4873{/metrics/json,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,941 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2017-11-03 08:57:09,957 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2017-11-03 08:57:09,957 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e9c413e{/SQL,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,957 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5af5def9{/SQL/json,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,957 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@27a0a5a2{/SQL/execution,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,957 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@33aa93c{/SQL/execution/json,null,AVAILABLE,@Spark}
2017-11-03 08:57:09,957 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7dd712e8{/static/sql,null,AVAILABLE,@Spark}
2017-11-03 08:57:11,082 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2017-11-03 08:57:12,395 INFO[org.apache.spark.sql.catalyst.parser.CatalystSqlParser:54] - Parsing command: double
2017-11-03 08:57:13,442 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 180.050838 ms
2017-11-03 08:57:13,583 INFO[org.apache.spark.SparkContext:54] - Starting job: first at ChiSqTest.scala:86
2017-11-03 08:57:13,598 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (first at ChiSqTest.scala:86) with 1 output partitions
2017-11-03 08:57:13,598 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (first at ChiSqTest.scala:86)
2017-11-03 08:57:13,598 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 08:57:13,598 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 08:57:13,692 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[4] at map at ChiSquareTest.scala:74), which has no missing parents
2017-11-03 08:57:13,833 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 8.0 KB, free 631.8 MB)
2017-11-03 08:57:13,864 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.0 KB, free 631.8 MB)
2017-11-03 08:57:13,864 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:54179 (size: 4.0 KB, free: 631.8 MB)
2017-11-03 08:57:13,880 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2017-11-03 08:57:13,895 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at ChiSquareTest.scala:74) (first 15 tasks are for partitions Vector(0))
2017-11-03 08:57:13,895 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2017-11-03 08:57:13,942 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5309 bytes)
2017-11-03 08:57:13,958 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2017-11-03 08:57:14,052 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 7.996729 ms
2017-11-03 08:57:14,083 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1202 bytes result sent to driver
2017-11-03 08:57:14,098 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 141 ms on localhost (executor driver) (1/1)
2017-11-03 08:57:14,098 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-11-03 08:57:14,098 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (first at ChiSqTest.scala:86) finished in 0.187 s
2017-11-03 08:57:14,098 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: first at ChiSqTest.scala:86, took 0.525387 s
2017-11-03 08:57:14,192 INFO[org.apache.spark.SparkContext:54] - Starting job: countByValue at ChiSqTest.scala:124
2017-11-03 08:57:14,192 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 7 (countByValue at ChiSqTest.scala:124)
2017-11-03 08:57:14,208 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (countByValue at ChiSqTest.scala:124) with 2 output partitions
2017-11-03 08:57:14,208 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (countByValue at ChiSqTest.scala:124)
2017-11-03 08:57:14,208 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2017-11-03 08:57:14,208 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2017-11-03 08:57:14,208 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at countByValue at ChiSqTest.scala:124), which has no missing parents
2017-11-03 08:57:14,208 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.4 KB, free 631.8 MB)
2017-11-03 08:57:14,208 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.7 KB, free 631.8 MB)
2017-11-03 08:57:14,208 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:54179 (size: 4.7 KB, free: 631.8 MB)
2017-11-03 08:57:14,223 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2017-11-03 08:57:14,223 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at countByValue at ChiSqTest.scala:124) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 08:57:14,223 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 2 tasks
2017-11-03 08:57:14,223 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5298 bytes)
2017-11-03 08:57:14,223 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 5298 bytes)
2017-11-03 08:57:14,223 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2017-11-03 08:57:14,223 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 1.0 (TID 2)
2017-11-03 08:57:14,817 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1202 bytes result sent to driver
2017-11-03 08:57:14,817 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 1.0 (TID 2). 1202 bytes result sent to driver
2017-11-03 08:57:14,817 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 2) in 594 ms on localhost (executor driver) (1/2)
2017-11-03 08:57:14,817 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 594 ms on localhost (executor driver) (2/2)
2017-11-03 08:57:14,817 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-11-03 08:57:14,817 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (countByValue at ChiSqTest.scala:124) finished in 0.594 s
2017-11-03 08:57:14,817 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2017-11-03 08:57:14,817 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2017-11-03 08:57:14,817 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2017-11-03 08:57:14,833 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2017-11-03 08:57:14,833 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (ShuffledRDD[8] at countByValue at ChiSqTest.scala:124), which has no missing parents
2017-11-03 08:57:14,833 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 2.8 KB, free 631.8 MB)
2017-11-03 08:57:14,833 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1688.0 B, free 631.8 MB)
2017-11-03 08:57:14,833 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:54179 (size: 1688.0 B, free: 631.8 MB)
2017-11-03 08:57:14,833 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2017-11-03 08:57:14,848 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 2 (ShuffledRDD[8] at countByValue at ChiSqTest.scala:124) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 08:57:14,848 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 2 tasks
2017-11-03 08:57:14,848 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, ANY, 4621 bytes)
2017-11-03 08:57:14,848 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, ANY, 4621 bytes)
2017-11-03 08:57:14,848 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 3)
2017-11-03 08:57:14,848 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 2.0 (TID 4)
2017-11-03 08:57:14,864 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 08:57:14,864 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 08:57:14,864 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 08:57:14,864 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 08:57:14,880 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 3). 1596 bytes result sent to driver
2017-11-03 08:57:14,880 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 2.0 (TID 4). 1555 bytes result sent to driver
2017-11-03 08:57:14,880 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 4) in 32 ms on localhost (executor driver) (1/2)
2017-11-03 08:57:14,895 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 3) in 47 ms on localhost (executor driver) (2/2)
2017-11-03 08:57:14,895 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-11-03 08:57:14,895 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (countByValue at ChiSqTest.scala:124) finished in 0.047 s
2017-11-03 08:57:14,895 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: countByValue at ChiSqTest.scala:124, took 0.698212 s
2017-11-03 08:57:15,005 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_2_piece0 on 192.168.11.26:54179 in memory (size: 1688.0 B, free: 631.8 MB)
2017-11-03 08:57:15,005 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_0_piece0 on 192.168.11.26:54179 in memory (size: 4.0 KB, free: 631.8 MB)
2017-11-03 08:57:15,005 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:54179 in memory (size: 4.7 KB, free: 631.8 MB)
2017-11-03 08:57:15,005 INFO[org.apache.spark.ContextCleaner:54] - Cleaned shuffle 0
2017-11-03 08:57:15,380 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 26.623794 ms
2017-11-03 08:57:15,411 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 18.024072 ms
2017-11-03 08:57:15,411 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2017-11-03 08:57:15,427 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@e0d06bf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 08:57:15,427 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2017-11-03 08:57:15,427 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2017-11-03 08:57:15,442 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2017-11-03 08:57:15,442 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2017-11-03 08:57:15,442 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2017-11-03 08:57:15,458 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2017-11-03 08:57:15,458 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2017-11-03 08:57:15,458 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2017-11-03 08:57:15,458 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-e81fc7f4-3430-42cc-a7a4-4505ba87a989
2017-11-03 09:40:28,585 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2017-11-03 09:40:29,036 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2017-11-03 09:40:29,067 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2017-11-03 09:40:29,067 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2017-11-03 09:40:29,067 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2017-11-03 09:40:29,067 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2017-11-03 09:40:29,067 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2017-11-03 09:40:29,473 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 57302.
2017-11-03 09:40:29,520 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2017-11-03 09:40:29,557 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2017-11-03 09:40:29,561 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2017-11-03 09:40:29,561 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2017-11-03 09:40:29,568 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-a92c59d8-302d-4020-a01a-1ac9716499b4
2017-11-03 09:40:29,584 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2017-11-03 09:40:29,630 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2017-11-03 09:40:29,754 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2186ms
2017-11-03 09:40:29,832 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2017-11-03 09:40:29,832 INFO[org.spark_project.jetty.server.Server:403] - Started @2269ms
2017-11-03 09:40:29,865 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@72a85671{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 09:40:29,865 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2017-11-03 09:40:29,880 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@29ef6856{/jobs,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,880 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@423e4cbb{/jobs/json,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,880 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@43b4fe19{/jobs/job,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ebea12c{/jobs/job/json,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6256ac4f{/stages,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7fcbe147{/stages/json,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@743cb8e0{/stages/stage,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@649725e3{/stages/stage/json,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c168660{/stages/pool,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@fd0e5b6{/stages/pool/json,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@36b0fcd5{/storage,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@475835b1{/storage/json,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5241cf67{/storage/rdd,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77192705{/storage/rdd/json,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e809b79{/environment,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@625e134e{/environment/json,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@89c10b7{/executors,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4fe89c24{/executors/json,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3d08f3f5{/executors/threadDump,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1a1da881{/executors/threadDump/json,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,912 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7fd4acee{/static,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,912 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@52500920{/,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,912 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18a3962d{/api,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,912 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@452ba1db{/jobs/job/kill,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,912 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@76a36b71{/stages/stage/kill,null,AVAILABLE,@Spark}
2017-11-03 09:40:29,912 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2017-11-03 09:40:30,021 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2017-11-03 09:40:30,068 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57315.
2017-11-03 09:40:30,068 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:57315
2017-11-03 09:40:30,083 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2017-11-03 09:40:30,083 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 57315, None)
2017-11-03 09:40:30,224 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:57315 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 57315, None)
2017-11-03 09:40:30,224 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 57315, None)
2017-11-03 09:40:30,224 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 57315, None)
2017-11-03 09:40:30,460 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9fecdf1{/metrics/json,null,AVAILABLE,@Spark}
2017-11-03 09:40:30,554 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2017-11-03 09:40:30,554 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2017-11-03 09:40:30,569 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2577d6c8{/SQL,null,AVAILABLE,@Spark}
2017-11-03 09:40:30,569 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6c000e0c{/SQL/json,null,AVAILABLE,@Spark}
2017-11-03 09:40:30,569 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@43d455c9{/SQL/execution,null,AVAILABLE,@Spark}
2017-11-03 09:40:30,569 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9ec531{/SQL/execution/json,null,AVAILABLE,@Spark}
2017-11-03 09:40:30,569 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55caeb35{/static/sql,null,AVAILABLE,@Spark}
2017-11-03 09:40:31,620 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2017-11-03 09:40:32,522 WARN[org.apache.spark.sql.SparkSession$Builder:66] - Using an existing SparkSession; some configuration may not take effect.
2017-11-03 09:40:34,509 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 259.256682 ms
2017-11-03 09:40:34,539 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 23.824362 ms
2017-11-03 09:40:34,555 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: person
2017-11-03 09:40:34,700 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select *from person
2017-11-03 09:40:34,792 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select max(age) from person
2017-11-03 09:40:35,344 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 14.946556 ms
2017-11-03 09:40:35,375 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 8.670927 ms
2017-11-03 09:40:35,375 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 5.843911 ms
2017-11-03 09:40:35,484 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:69
2017-11-03 09:40:35,500 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (show at SchemaRDDSparkSQL.java:69)
2017-11-03 09:40:35,500 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at SchemaRDDSparkSQL.java:69) with 1 output partitions
2017-11-03 09:40:35,516 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at SchemaRDDSparkSQL.java:69)
2017-11-03 09:40:35,516 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 0)
2017-11-03 09:40:35,516 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 0)
2017-11-03 09:40:35,516 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at SchemaRDDSparkSQL.java:69), which has no missing parents
2017-11-03 09:40:35,667 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 8.1 KB, free 631.8 MB)
2017-11-03 09:40:35,707 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.3 KB, free 631.8 MB)
2017-11-03 09:40:35,707 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:57315 (size: 4.3 KB, free: 631.8 MB)
2017-11-03 09:40:35,707 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2017-11-03 09:40:35,723 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at SchemaRDDSparkSQL.java:69) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 09:40:35,723 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 2 tasks
2017-11-03 09:40:35,771 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4992 bytes)
2017-11-03 09:40:35,771 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 5025 bytes)
2017-11-03 09:40:35,787 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2017-11-03 09:40:35,787 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 0.0 (TID 1)
2017-11-03 09:40:35,893 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1460 bytes result sent to driver
2017-11-03 09:40:35,893 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 0.0 (TID 1). 1503 bytes result sent to driver
2017-11-03 09:40:35,893 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 122 ms on localhost (executor driver) (1/2)
2017-11-03 09:40:35,893 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 138 ms on localhost (executor driver) (2/2)
2017-11-03 09:40:35,893 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-11-03 09:40:35,909 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 0 (show at SchemaRDDSparkSQL.java:69) finished in 0.171 s
2017-11-03 09:40:35,909 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2017-11-03 09:40:35,909 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2017-11-03 09:40:35,909 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 1)
2017-11-03 09:40:35,909 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2017-11-03 09:40:35,909 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SchemaRDDSparkSQL.java:69), which has no missing parents
2017-11-03 09:40:35,925 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 7.1 KB, free 631.8 MB)
2017-11-03 09:40:35,925 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.8 KB, free 631.8 MB)
2017-11-03 09:40:35,940 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:57315 (size: 3.8 KB, free: 631.8 MB)
2017-11-03 09:40:35,940 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2017-11-03 09:40:35,940 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SchemaRDDSparkSQL.java:69) (first 15 tasks are for partitions Vector(0))
2017-11-03 09:40:35,940 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2017-11-03 09:40:35,940 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
2017-11-03 09:40:35,940 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 2)
2017-11-03 09:40:35,956 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 09:40:35,956 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:40:35,971 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 2). 1471 bytes result sent to driver
2017-11-03 09:40:35,971 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 2) in 31 ms on localhost (executor driver) (1/1)
2017-11-03 09:40:35,971 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at SchemaRDDSparkSQL.java:69) finished in 0.031 s
2017-11-03 09:40:35,987 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-11-03 09:40:35,987 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at SchemaRDDSparkSQL.java:69, took 0.501280 s
2017-11-03 09:40:36,003 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 7.842772 ms
2017-11-03 09:40:36,034 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 7.353321 ms
2017-11-03 09:40:36,050 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 7.054069 ms
2017-11-03 09:40:36,098 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:72
2017-11-03 09:40:36,098 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 10 (show at SchemaRDDSparkSQL.java:72)
2017-11-03 09:40:36,098 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at SchemaRDDSparkSQL.java:72) with 1 output partitions
2017-11-03 09:40:36,098 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 3 (show at SchemaRDDSparkSQL.java:72)
2017-11-03 09:40:36,098 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 2)
2017-11-03 09:40:36,098 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 2)
2017-11-03 09:40:36,098 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at show at SchemaRDDSparkSQL.java:72), which has no missing parents
2017-11-03 09:40:36,098 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 8.1 KB, free 631.8 MB)
2017-11-03 09:40:36,098 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.3 KB, free 631.8 MB)
2017-11-03 09:40:36,098 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:57315 (size: 4.3 KB, free: 631.8 MB)
2017-11-03 09:40:36,098 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2017-11-03 09:40:36,098 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at show at SchemaRDDSparkSQL.java:72) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 09:40:36,098 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 2 tasks
2017-11-03 09:40:36,098 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4992 bytes)
2017-11-03 09:40:36,114 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 5025 bytes)
2017-11-03 09:40:36,114 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 3)
2017-11-03 09:40:36,114 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 2.0 (TID 4)
2017-11-03 09:40:36,129 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 3). 1417 bytes result sent to driver
2017-11-03 09:40:36,129 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 2.0 (TID 4). 1374 bytes result sent to driver
2017-11-03 09:40:36,129 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 4) in 15 ms on localhost (executor driver) (1/2)
2017-11-03 09:40:36,129 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 3) in 31 ms on localhost (executor driver) (2/2)
2017-11-03 09:40:36,129 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-11-03 09:40:36,129 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 2 (show at SchemaRDDSparkSQL.java:72) finished in 0.031 s
2017-11-03 09:40:36,129 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2017-11-03 09:40:36,129 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2017-11-03 09:40:36,129 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 3)
2017-11-03 09:40:36,129 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2017-11-03 09:40:36,129 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 3 (MapPartitionsRDD[13] at show at SchemaRDDSparkSQL.java:72), which has no missing parents
2017-11-03 09:40:36,145 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 7.1 KB, free 631.8 MB)
2017-11-03 09:40:36,145 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.8 KB, free 631.8 MB)
2017-11-03 09:40:36,145 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:57315 (size: 3.8 KB, free: 631.8 MB)
2017-11-03 09:40:36,145 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2017-11-03 09:40:36,145 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at show at SchemaRDDSparkSQL.java:72) (first 15 tasks are for partitions Vector(0))
2017-11-03 09:40:36,145 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 1 tasks
2017-11-03 09:40:36,145 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 5, localhost, executor driver, partition 0, ANY, 4726 bytes)
2017-11-03 09:40:36,145 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 5)
2017-11-03 09:40:36,145 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 09:40:36,145 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:40:36,161 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 5). 1428 bytes result sent to driver
2017-11-03 09:40:36,161 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 5) in 16 ms on localhost (executor driver) (1/1)
2017-11-03 09:40:36,161 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2017-11-03 09:40:36,161 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 3 (show at SchemaRDDSparkSQL.java:72) finished in 0.016 s
2017-11-03 09:40:36,161 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at SchemaRDDSparkSQL.java:72, took 0.068058 s
2017-11-03 09:40:36,161 WARN[org.apache.spark.sql.SparkSession$Builder:66] - Using an existing SparkSession; some configuration may not take effect.
2017-11-03 09:40:36,364 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 100
2017-11-03 09:40:36,364 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 8
2017-11-03 09:40:36,364 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 9
2017-11-03 09:40:36,379 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 14
2017-11-03 09:40:36,379 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 99
2017-11-03 09:40:36,379 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 91
2017-11-03 09:40:36,411 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:57315 in memory (size: 3.8 KB, free: 631.8 MB)
2017-11-03 09:40:36,411 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 5
2017-11-03 09:40:36,411 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 4
2017-11-03 09:40:36,426 INFO[org.apache.spark.ContextCleaner:54] - Cleaned shuffle 1
2017-11-03 09:40:36,426 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 96
2017-11-03 09:40:36,426 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 93
2017-11-03 09:40:36,426 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 3
2017-11-03 09:40:36,426 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 88
2017-11-03 09:40:36,426 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 92
2017-11-03 09:40:36,426 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 7
2017-11-03 09:40:36,426 INFO[org.apache.spark.ContextCleaner:54] - Cleaned shuffle 0
2017-11-03 09:40:36,426 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 11
2017-11-03 09:40:36,442 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 89
2017-11-03 09:40:36,442 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 2
2017-11-03 09:40:36,442 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 94
2017-11-03 09:40:36,442 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 6
2017-11-03 09:40:36,442 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 97
2017-11-03 09:40:36,442 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_0_piece0 on 192.168.11.26:57315 in memory (size: 4.3 KB, free: 631.8 MB)
2017-11-03 09:40:36,442 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 1
2017-11-03 09:40:36,442 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 95
2017-11-03 09:40:36,442 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_3_piece0 on 192.168.11.26:57315 in memory (size: 3.8 KB, free: 631.8 MB)
2017-11-03 09:40:36,442 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 10
2017-11-03 09:40:36,442 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 98
2017-11-03 09:40:36,442 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 13
2017-11-03 09:40:36,442 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 90
2017-11-03 09:40:36,442 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 12
2017-11-03 09:40:36,442 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_2_piece0 on 192.168.11.26:57315 in memory (size: 4.3 KB, free: 631.8 MB)
2017-11-03 09:40:36,442 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 87
2017-11-03 09:40:36,458 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 10.191884 ms
2017-11-03 09:40:36,492 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 5.739991 ms
2017-11-03 09:40:36,542 INFO[org.apache.spark.SparkContext:54] - Starting job: json at SchemaRDDSparkSQL.java:86
2017-11-03 09:40:36,542 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 2 (json at SchemaRDDSparkSQL.java:86) with 1 output partitions
2017-11-03 09:40:36,542 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 4 (json at SchemaRDDSparkSQL.java:86)
2017-11-03 09:40:36,542 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 09:40:36,542 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 09:40:36,542 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 4 (MapPartitionsRDD[17] at json at SchemaRDDSparkSQL.java:86), which has no missing parents
2017-11-03 09:40:36,542 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 5.8 KB, free 631.8 MB)
2017-11-03 09:40:36,542 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.6 KB, free 631.8 MB)
2017-11-03 09:40:36,542 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.11.26:57315 (size: 3.6 KB, free: 631.8 MB)
2017-11-03 09:40:36,542 INFO[org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2017-11-03 09:40:36,558 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[17] at json at SchemaRDDSparkSQL.java:86) (first 15 tasks are for partitions Vector(0))
2017-11-03 09:40:36,558 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 4.0 with 1 tasks
2017-11-03 09:40:36,558 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 4.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 5067 bytes)
2017-11-03 09:40:36,558 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 4.0 (TID 6)
2017-11-03 09:40:36,558 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 4.0 (TID 6). 1582 bytes result sent to driver
2017-11-03 09:40:36,558 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 4.0 (TID 6) in 0 ms on localhost (executor driver) (1/1)
2017-11-03 09:40:36,558 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2017-11-03 09:40:36,558 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 4 (json at SchemaRDDSparkSQL.java:86) finished in 0.000 s
2017-11-03 09:40:36,558 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 2 finished: json at SchemaRDDSparkSQL.java:86, took 0.021760 s
2017-11-03 09:40:36,637 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:87
2017-11-03 09:40:36,637 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 3 (show at SchemaRDDSparkSQL.java:87) with 1 output partitions
2017-11-03 09:40:36,637 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 5 (show at SchemaRDDSparkSQL.java:87)
2017-11-03 09:40:36,637 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 09:40:36,637 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 09:40:36,637 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 5 (MapPartitionsRDD[24] at show at SchemaRDDSparkSQL.java:87), which has no missing parents
2017-11-03 09:40:36,654 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5 stored as values in memory (estimated size 9.8 KB, free 631.8 MB)
2017-11-03 09:40:36,654 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.5 KB, free 631.8 MB)
2017-11-03 09:40:36,654 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_5_piece0 in memory on 192.168.11.26:57315 (size: 5.5 KB, free: 631.8 MB)
2017-11-03 09:40:36,654 INFO[org.apache.spark.SparkContext:54] - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
2017-11-03 09:40:36,654 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[24] at show at SchemaRDDSparkSQL.java:87) (first 15 tasks are for partitions Vector(0))
2017-11-03 09:40:36,654 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 5.0 with 1 tasks
2017-11-03 09:40:36,654 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 5.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 5067 bytes)
2017-11-03 09:40:36,654 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 5.0 (TID 7)
2017-11-03 09:40:36,670 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 6.744233 ms
2017-11-03 09:40:36,701 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 12.877131 ms
2017-11-03 09:40:36,716 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 5.0 (TID 7). 1039 bytes result sent to driver
2017-11-03 09:40:36,717 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 5.0 (TID 7) in 63 ms on localhost (executor driver) (1/1)
2017-11-03 09:40:36,718 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2017-11-03 09:40:36,718 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 5 (show at SchemaRDDSparkSQL.java:87) finished in 0.064 s
2017-11-03 09:40:36,719 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 3 finished: show at SchemaRDDSparkSQL.java:87, took 0.073505 s
2017-11-03 09:40:36,740 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 28.258293 ms
2017-11-03 09:40:36,755 WARN[org.apache.spark.sql.SparkSession$Builder:66] - Using an existing SparkSession; some configuration may not take effect.
2017-11-03 09:40:36,807 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 13.07952 ms
2017-11-03 09:40:36,823 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 6.499186 ms
2017-11-03 09:40:36,838 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 6.093127 ms
2017-11-03 09:40:36,901 INFO[org.apache.spark.streaming.CheckpointReader:54] - Checkpoint files found: file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000.bk
2017-11-03 09:40:36,901 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000
2017-11-03 09:40:36,948 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:36,963 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:40:36,963 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000.bk
2017-11-03 09:40:36,963 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:36,963 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:40:36,963 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000
2017-11-03 09:40:36,979 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:36,979 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:40:36,979 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000.bk
2017-11-03 09:40:36,994 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:36,994 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:40:36,994 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000
2017-11-03 09:40:37,026 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,026 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:40:37,041 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000.bk
2017-11-03 09:40:37,041 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,041 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:40:37,041 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000
2017-11-03 09:40:37,093 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,093 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:40:37,093 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000.bk
2017-11-03 09:40:37,093 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,093 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:40:37,093 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000
2017-11-03 09:40:37,093 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,108 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:40:37,108 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000.bk
2017-11-03 09:40:37,108 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,108 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:40:37,124 INFO[org.apache.spark.streaming.CheckpointReader:54] - Checkpoint files found: file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000.bk
2017-11-03 09:40:37,124 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000
2017-11-03 09:40:37,124 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,124 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 58 more
2017-11-03 09:40:37,124 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000.bk
2017-11-03 09:40:37,124 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,140 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 58 more
2017-11-03 09:40:37,140 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000
2017-11-03 09:40:37,155 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,155 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 58 more
2017-11-03 09:40:37,155 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000.bk
2017-11-03 09:40:37,155 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,155 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 58 more
2017-11-03 09:40:37,155 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000
2017-11-03 09:40:37,171 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,187 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 58 more
2017-11-03 09:40:37,187 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000.bk
2017-11-03 09:40:37,187 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,202 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 58 more
2017-11-03 09:40:37,202 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000
2017-11-03 09:40:37,202 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,202 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:40:37,202 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000.bk
2017-11-03 09:40:37,202 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,202 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:40:37,218 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000
2017-11-03 09:40:37,218 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,218 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:40:37,218 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000.bk
2017-11-03 09:40:37,218 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:40:37,218 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:40:37,249 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2017-11-03 09:40:37,249 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@72a85671{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 09:40:37,562 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2017-11-03 09:40:37,629 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2017-11-03 09:40:37,682 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2017-11-03 09:40:37,682 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2017-11-03 09:40:37,776 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2017-11-03 09:40:37,776 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2017-11-03 09:40:37,784 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2017-11-03 09:40:37,784 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2017-11-03 09:40:37,785 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-b4b92612-f9c8-4b8d-b90d-f810947a2f97
2017-11-03 09:40:38,302 INFO[net.sourceforge.cobertura.coveragedata.CoverageDataFileHandler:86] - Cobertura: Loaded information on 41 classes.
2017-11-03 09:40:38,318 INFO[net.sourceforge.cobertura.coveragedata.CoverageDataFileHandler:138] - Cobertura: Saved information on 41 classes.
2017-11-03 09:42:45,349 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2017-11-03 09:42:45,694 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2017-11-03 09:42:45,708 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2017-11-03 09:42:45,708 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2017-11-03 09:42:45,708 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2017-11-03 09:42:45,708 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2017-11-03 09:42:45,708 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2017-11-03 09:42:46,086 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 57378.
2017-11-03 09:42:46,117 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2017-11-03 09:42:46,133 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2017-11-03 09:42:46,149 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2017-11-03 09:42:46,149 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2017-11-03 09:42:46,149 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-36021963-021b-48fb-b5f1-45c3e0446b68
2017-11-03 09:42:46,164 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2017-11-03 09:42:46,211 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2017-11-03 09:42:46,275 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @1824ms
2017-11-03 09:42:46,353 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2017-11-03 09:42:46,369 INFO[org.spark_project.jetty.server.Server:403] - Started @1928ms
2017-11-03 09:42:46,400 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@6a66a204{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 09:42:46,400 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2017-11-03 09:42:46,431 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@21d8bcbe{/jobs,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@186978a6{/jobs/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@482d776b{/jobs/job,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@297ea53a{/jobs/job/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5bf22f18{/stages,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a7471ce{/stages/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@62e70ea3{/stages/stage,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2ed3b1f5{/stages/stage/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@68d6972f{/stages/pool,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7651218e{/stages/pool/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d366c9b{/storage,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@24faea88{/storage/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64beebb7{/storage/rdd,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,448 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@bcec031{/storage/rdd/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,448 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@32f0fba8{/environment,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,448 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@29ef6856{/environment/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,448 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3faf2e7d{/executors,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,448 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@569bf9eb{/executors/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,448 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@274872f8{/executors/threadDump,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,448 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@eb6449b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,448 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@180e6ac4{/static,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,448 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44c79f32{/,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,464 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@235f4c10{/api,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,464 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3c0fae6c{/jobs/job/kill,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,464 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@52b56a3e{/stages/stage/kill,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,464 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2017-11-03 09:42:46,542 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2017-11-03 09:42:46,558 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57391.
2017-11-03 09:42:46,558 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:57391
2017-11-03 09:42:46,558 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2017-11-03 09:42:46,558 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 57391, None)
2017-11-03 09:42:46,558 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:57391 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 57391, None)
2017-11-03 09:42:46,558 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 57391, None)
2017-11-03 09:42:46,558 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 57391, None)
2017-11-03 09:42:46,723 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@273c947f{/metrics/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,770 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2017-11-03 09:42:46,770 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2017-11-03 09:42:46,770 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@150d80c4{/SQL,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,770 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3003697{/SQL/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,770 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@410e94e{/SQL/execution,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,770 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bdbf9be{/SQL/execution/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:46,770 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71154f21{/static/sql,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,674 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2017-11-03 09:42:47,694 WARN[org.apache.spark.SparkContext:87] - Multiple running SparkContexts detected in the same JVM!
org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
com.lovecws.mumu.spark.MumuSparkConfiguration.sqlContext(MumuSparkConfiguration.java:55)
com.lovecws.mumu.spark.MumuSparkConfigurationTest.sqlContext(MumuSparkConfigurationTest.java:32)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2472)
	at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2468)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2468)
	at org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2557)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:85)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at com.lovecws.mumu.spark.MumuSparkConfiguration.javaSparkContext(MumuSparkConfiguration.java:39)
	at com.lovecws.mumu.spark.MumuSparkConfiguration.javaStreamingContext(MumuSparkConfiguration.java:46)
	at com.lovecws.mumu.spark.MumuSparkConfigurationTest.javaStreamingContext(MumuSparkConfigurationTest.java:26)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:42:47,697 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2017-11-03 09:42:47,698 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2017-11-03 09:42:47,699 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2017-11-03 09:42:47,699 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2017-11-03 09:42:47,700 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2017-11-03 09:42:47,700 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2017-11-03 09:42:47,700 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2017-11-03 09:42:47,707 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 57408.
2017-11-03 09:42:47,708 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2017-11-03 09:42:47,708 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2017-11-03 09:42:47,708 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2017-11-03 09:42:47,708 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2017-11-03 09:42:47,708 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-6feab976-582c-4525-855e-a614d2609eed
2017-11-03 09:42:47,708 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2017-11-03 09:42:47,708 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2017-11-03 09:42:47,708 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2017-11-03 09:42:47,708 INFO[org.spark_project.jetty.server.Server:403] - Started @3265ms
2017-11-03 09:42:47,724 WARN[org.apache.spark.util.Utils:66] - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
2017-11-03 09:42:47,724 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@58860997{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2017-11-03 09:42:47,724 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4041.
2017-11-03 09:42:47,724 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@81b5db0{/jobs,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,724 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7139bd31{/jobs/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,724 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4b3fe06e{/jobs/job,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,724 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e17a0a1{/jobs/job/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,724 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4d8286c4{/stages,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,724 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@161f6623{/stages/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,724 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6778aea6{/stages/stage,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,724 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@69228e85{/stages/stage/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5853495b{/stages/pool,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f61d591{/stages/pool/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7173ae5b{/storage,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53a9fcfd{/storage/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4d192aef{/storage/rdd,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@84487f4{/storage/rdd/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@fb6097b{/environment,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1290c49{/environment/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55d9b8f0{/executors,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@43d38654{/executors/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d303498{/executors/threadDump,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3419e23b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d75e7af{/static,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34b27915{/,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1b9776f5{/api,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1dd7796b{/jobs/job/kill,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@57402ba1{/stages/stage/kill,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,740 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4041
2017-11-03 09:42:47,771 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2017-11-03 09:42:47,786 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57421.
2017-11-03 09:42:47,786 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:57421
2017-11-03 09:42:47,786 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2017-11-03 09:42:47,786 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 57421, None)
2017-11-03 09:42:47,786 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:57421 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 57421, None)
2017-11-03 09:42:47,786 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 57421, None)
2017-11-03 09:42:47,786 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 57421, None)
2017-11-03 09:42:47,786 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42714a7{/metrics/json,null,AVAILABLE,@Spark}
2017-11-03 09:42:47,802 WARN[org.apache.spark.SparkContext:87] - Multiple running SparkContexts detected in the same JVM!
org.apache.spark.SparkException: Only one SparkContext may be running in this JVM (see SPARK-2243). To ignore this error, set spark.driver.allowMultipleContexts = true. The currently running SparkContext was created at:
org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:901)
com.lovecws.mumu.spark.MumuSparkConfiguration.sqlContext(MumuSparkConfiguration.java:55)
com.lovecws.mumu.spark.MumuSparkConfigurationTest.sqlContext(MumuSparkConfigurationTest.java:32)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2472)
	at org.apache.spark.SparkContext$$anonfun$assertNoOtherContextIsRunning$2.apply(SparkContext.scala:2468)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2468)
	at org.apache.spark.SparkContext$.setActiveContext(SparkContext.scala:2570)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:2424)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at com.lovecws.mumu.spark.MumuSparkConfiguration.javaSparkContext(MumuSparkConfiguration.java:39)
	at com.lovecws.mumu.spark.MumuSparkConfiguration.javaStreamingContext(MumuSparkConfiguration.java:46)
	at com.lovecws.mumu.spark.MumuSparkConfigurationTest.javaStreamingContext(MumuSparkConfigurationTest.java:26)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:42:47,802 INFO[org.apache.spark.SparkContext:54] - Added JAR hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar at hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1509673367802
2017-11-03 09:42:48,671 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 316.2 KB, free 631.5 MB)
2017-11-03 09:42:48,724 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 631.5 MB)
2017-11-03 09:42:48,724 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:57421 (size: 27.2 KB, free: 631.8 MB)
2017-11-03 09:42:48,739 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from textFile at SparkNginxLog.java:31
2017-11-03 09:42:55,824 INFO[org.apache.hadoop.mapred.FileInputFormat:256] - Total input files to process : 175
2017-11-03 09:42:55,891 INFO[org.apache.spark.SparkContext:54] - Starting job: sortByKey at SparkNginxLog.java:47
2017-11-03 09:42:55,906 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 2 (mapToPair at SparkNginxLog.java:33)
2017-11-03 09:42:55,906 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (sortByKey at SparkNginxLog.java:47) with 175 output partitions
2017-11-03 09:42:55,906 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (sortByKey at SparkNginxLog.java:47)
2017-11-03 09:42:55,906 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 0)
2017-11-03 09:42:55,906 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 0)
2017-11-03 09:42:55,922 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at mapToPair at SparkNginxLog.java:33), which has no missing parents
2017-11-03 09:42:55,955 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 4.6 KB, free 631.5 MB)
2017-11-03 09:42:55,955 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.7 KB, free 631.5 MB)
2017-11-03 09:42:55,955 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:57421 (size: 2.7 KB, free: 631.8 MB)
2017-11-03 09:42:55,955 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2017-11-03 09:42:55,970 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 175 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at mapToPair at SparkNginxLog.java:33) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2017-11-03 09:42:55,970 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 175 tasks
2017-11-03 09:42:56,017 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4874 bytes)
2017-11-03 09:42:56,017 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4874 bytes)
2017-11-03 09:42:56,033 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2017-11-03 09:42:56,033 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 0.0 (TID 1)
2017-11-03 09:42:56,033 INFO[org.apache.spark.executor.Executor:54] - Fetching hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1509673367802
2017-11-03 09:42:56,064 INFO[org.apache.spark.util.Utils:54] - Fetching hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar to C:\Users\Administrator\AppData\Local\Temp\spark-4db18d66-fa7a-4e41-a0f8-25991631cd5a\userFiles-d7f2205a-6288-4f36-8ea8-6623f4f5d9b4\fetchFileTemp5400278323102737791.tmp
2017-11-03 09:42:56,785 INFO[org.apache.spark.executor.Executor:54] - Adding file:/C:/Users/Administrator/AppData/Local/Temp/spark-4db18d66-fa7a-4e41-a0f8-25991631cd5a/userFiles-d7f2205a-6288-4f36-8ea8-6623f4f5d9b4/mumu-spark.jar to class loader
2017-11-03 09:42:56,832 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170510:0+17825
2017-11-03 09:42:56,832 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170511:0+775188
2017-11-03 09:42:57,087 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1245 bytes result sent to driver
2017-11-03 09:42:57,087 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4874 bytes)
2017-11-03 09:42:57,118 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 1116 ms on localhost (executor driver) (1/175)
2017-11-03 09:42:57,118 INFO[org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 0.0 (TID 2)
2017-11-03 09:42:57,149 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170512:0+4181517
2017-11-03 09:42:57,462 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 0.0 (TID 1). 1331 bytes result sent to driver
2017-11-03 09:42:57,462 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, ANY, 4874 bytes)
2017-11-03 09:42:57,462 INFO[org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 0.0 (TID 3)
2017-11-03 09:42:57,477 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170513:0+17965
2017-11-03 09:42:57,477 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 1460 ms on localhost (executor driver) (2/175)
2017-11-03 09:42:57,493 INFO[org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 0.0 (TID 3). 1288 bytes result sent to driver
2017-11-03 09:42:57,493 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 0.0 (TID 4, localhost, executor driver, partition 4, ANY, 4874 bytes)
2017-11-03 09:42:57,493 INFO[org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 0.0 (TID 4)
2017-11-03 09:42:57,509 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170514:0+13157
2017-11-03 09:42:57,520 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 0.0 (TID 3) in 57 ms on localhost (executor driver) (3/175)
2017-11-03 09:42:57,536 INFO[org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 0.0 (TID 4). 1288 bytes result sent to driver
2017-11-03 09:42:57,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 0.0 (TID 5, localhost, executor driver, partition 5, ANY, 4874 bytes)
2017-11-03 09:42:57,547 INFO[org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 0.0 (TID 5)
2017-11-03 09:42:57,551 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170515:0+526233
2017-11-03 09:42:57,557 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 0.0 (TID 4) in 64 ms on localhost (executor driver) (4/175)
2017-11-03 09:42:57,807 INFO[org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 0.0 (TID 5). 1331 bytes result sent to driver
2017-11-03 09:42:57,807 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 0.0 (TID 6, localhost, executor driver, partition 6, ANY, 4874 bytes)
2017-11-03 09:42:57,807 INFO[org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 0.0 (TID 6)
2017-11-03 09:42:57,807 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 0.0 (TID 5) in 262 ms on localhost (executor driver) (5/175)
2017-11-03 09:42:57,807 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170516:0+2051059
2017-11-03 09:42:58,284 INFO[org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 0.0 (TID 6). 1245 bytes result sent to driver
2017-11-03 09:42:58,284 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 0.0 (TID 7, localhost, executor driver, partition 7, ANY, 4874 bytes)
2017-11-03 09:42:58,284 INFO[org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 0.0 (TID 7)
2017-11-03 09:42:58,299 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 0.0 (TID 6) in 477 ms on localhost (executor driver) (6/175)
2017-11-03 09:42:58,299 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170517:0+1059012
2017-11-03 09:42:58,409 INFO[org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 0.0 (TID 2). 1245 bytes result sent to driver
2017-11-03 09:42:58,409 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 0.0 (TID 8, localhost, executor driver, partition 8, ANY, 4874 bytes)
2017-11-03 09:42:58,409 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 0.0 (TID 2) in 1322 ms on localhost (executor driver) (7/175)
2017-11-03 09:42:58,409 INFO[org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 0.0 (TID 8)
2017-11-03 09:42:58,409 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170518:0+699889
2017-11-03 09:42:58,503 INFO[org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 0.0 (TID 7). 1331 bytes result sent to driver
2017-11-03 09:42:58,518 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 0.0 (TID 9, localhost, executor driver, partition 9, ANY, 4874 bytes)
2017-11-03 09:42:58,518 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 0.0 (TID 7) in 234 ms on localhost (executor driver) (8/175)
2017-11-03 09:42:58,518 INFO[org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 0.0 (TID 9)
2017-11-03 09:42:58,524 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170519:0+890926
2017-11-03 09:42:58,540 INFO[org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 0.0 (TID 8). 1245 bytes result sent to driver
2017-11-03 09:42:58,556 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 0.0 (TID 10, localhost, executor driver, partition 10, ANY, 4874 bytes)
2017-11-03 09:42:58,556 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 0.0 (TID 8) in 147 ms on localhost (executor driver) (9/175)
2017-11-03 09:42:58,556 INFO[org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 0.0 (TID 10)
2017-11-03 09:42:58,571 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170520:0+113503
2017-11-03 09:42:58,618 INFO[org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 0.0 (TID 10). 1245 bytes result sent to driver
2017-11-03 09:42:58,618 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 0.0 (TID 11, localhost, executor driver, partition 11, ANY, 4874 bytes)
2017-11-03 09:42:58,618 INFO[org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 0.0 (TID 11)
2017-11-03 09:42:58,618 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170521:0+176100
2017-11-03 09:42:58,634 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 0.0 (TID 10) in 94 ms on localhost (executor driver) (10/175)
2017-11-03 09:42:58,665 INFO[org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 0.0 (TID 11). 1202 bytes result sent to driver
2017-11-03 09:42:58,665 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 0.0 (TID 12, localhost, executor driver, partition 12, ANY, 4874 bytes)
2017-11-03 09:42:58,680 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 0.0 (TID 11) in 62 ms on localhost (executor driver) (11/175)
2017-11-03 09:42:58,680 INFO[org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 0.0 (TID 12)
2017-11-03 09:42:58,680 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170522:0+1111957
2017-11-03 09:42:58,704 INFO[org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 0.0 (TID 9). 1331 bytes result sent to driver
2017-11-03 09:42:58,707 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 0.0 (TID 13, localhost, executor driver, partition 13, ANY, 4874 bytes)
2017-11-03 09:42:58,711 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 0.0 (TID 9) in 193 ms on localhost (executor driver) (12/175)
2017-11-03 09:42:58,715 INFO[org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 0.0 (TID 13)
2017-11-03 09:42:58,720 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170523:0+441089
2017-11-03 09:42:58,828 INFO[org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 0.0 (TID 13). 1288 bytes result sent to driver
2017-11-03 09:42:58,828 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 0.0 (TID 14, localhost, executor driver, partition 14, ANY, 4874 bytes)
2017-11-03 09:42:58,828 INFO[org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 0.0 (TID 14)
2017-11-03 09:42:58,844 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170524:0+414912
2017-11-03 09:42:58,844 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 0.0 (TID 13) in 138 ms on localhost (executor driver) (13/175)
2017-11-03 09:42:58,875 INFO[org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 0.0 (TID 12). 1245 bytes result sent to driver
2017-11-03 09:42:58,875 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 0.0 (TID 15, localhost, executor driver, partition 15, ANY, 4874 bytes)
2017-11-03 09:42:58,875 INFO[org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 0.0 (TID 15)
2017-11-03 09:42:58,875 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 0.0 (TID 12) in 210 ms on localhost (executor driver) (14/175)
2017-11-03 09:42:58,875 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170525:0+1223925
2017-11-03 09:42:58,907 INFO[org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 0.0 (TID 14). 1288 bytes result sent to driver
2017-11-03 09:42:58,907 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 16.0 in stage 0.0 (TID 16, localhost, executor driver, partition 16, ANY, 4874 bytes)
2017-11-03 09:42:58,907 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 0.0 (TID 14) in 79 ms on localhost (executor driver) (15/175)
2017-11-03 09:42:58,907 INFO[org.apache.spark.executor.Executor:54] - Running task 16.0 in stage 0.0 (TID 16)
2017-11-03 09:42:58,907 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170526:0+685198
2017-11-03 09:42:59,063 INFO[org.apache.spark.executor.Executor:54] - Finished task 16.0 in stage 0.0 (TID 16). 1202 bytes result sent to driver
2017-11-03 09:42:59,063 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 17.0 in stage 0.0 (TID 17, localhost, executor driver, partition 17, ANY, 4874 bytes)
2017-11-03 09:42:59,063 INFO[org.apache.spark.executor.Executor:54] - Running task 17.0 in stage 0.0 (TID 17)
2017-11-03 09:42:59,063 INFO[org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 0.0 (TID 15). 1202 bytes result sent to driver
2017-11-03 09:42:59,063 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 18.0 in stage 0.0 (TID 18, localhost, executor driver, partition 18, ANY, 4874 bytes)
2017-11-03 09:42:59,063 INFO[org.apache.spark.executor.Executor:54] - Running task 18.0 in stage 0.0 (TID 18)
2017-11-03 09:42:59,063 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170527:0+1463759
2017-11-03 09:42:59,063 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 16.0 in stage 0.0 (TID 16) in 156 ms on localhost (executor driver) (16/175)
2017-11-03 09:42:59,063 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170528:0+271469
2017-11-03 09:42:59,063 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 0.0 (TID 15) in 188 ms on localhost (executor driver) (17/175)
2017-11-03 09:42:59,133 INFO[org.apache.spark.executor.Executor:54] - Finished task 18.0 in stage 0.0 (TID 18). 1245 bytes result sent to driver
2017-11-03 09:42:59,133 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 19.0 in stage 0.0 (TID 19, localhost, executor driver, partition 19, ANY, 4874 bytes)
2017-11-03 09:42:59,133 INFO[org.apache.spark.executor.Executor:54] - Running task 19.0 in stage 0.0 (TID 19)
2017-11-03 09:42:59,133 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170529:0+92650
2017-11-03 09:42:59,133 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 18.0 in stage 0.0 (TID 18) in 70 ms on localhost (executor driver) (18/175)
2017-11-03 09:42:59,180 INFO[org.apache.spark.executor.Executor:54] - Finished task 19.0 in stage 0.0 (TID 19). 1202 bytes result sent to driver
2017-11-03 09:42:59,180 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 20.0 in stage 0.0 (TID 20, localhost, executor driver, partition 20, ANY, 4874 bytes)
2017-11-03 09:42:59,180 INFO[org.apache.spark.executor.Executor:54] - Running task 20.0 in stage 0.0 (TID 20)
2017-11-03 09:42:59,180 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 19.0 in stage 0.0 (TID 19) in 47 ms on localhost (executor driver) (19/175)
2017-11-03 09:42:59,180 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170530:0+37536
2017-11-03 09:42:59,227 INFO[org.apache.spark.executor.Executor:54] - Finished task 20.0 in stage 0.0 (TID 20). 1202 bytes result sent to driver
2017-11-03 09:42:59,227 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 21.0 in stage 0.0 (TID 21, localhost, executor driver, partition 21, ANY, 4874 bytes)
2017-11-03 09:42:59,227 INFO[org.apache.spark.executor.Executor:54] - Running task 21.0 in stage 0.0 (TID 21)
2017-11-03 09:42:59,227 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 20.0 in stage 0.0 (TID 20) in 47 ms on localhost (executor driver) (20/175)
2017-11-03 09:42:59,227 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170531:0+746503
2017-11-03 09:42:59,258 INFO[org.apache.spark.executor.Executor:54] - Finished task 17.0 in stage 0.0 (TID 17). 1245 bytes result sent to driver
2017-11-03 09:42:59,258 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 22.0 in stage 0.0 (TID 22, localhost, executor driver, partition 22, ANY, 4874 bytes)
2017-11-03 09:42:59,258 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 17.0 in stage 0.0 (TID 17) in 195 ms on localhost (executor driver) (21/175)
2017-11-03 09:42:59,258 INFO[org.apache.spark.executor.Executor:54] - Running task 22.0 in stage 0.0 (TID 22)
2017-11-03 09:42:59,258 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170601:0+1630048
2017-11-03 09:42:59,337 INFO[org.apache.spark.executor.Executor:54] - Finished task 21.0 in stage 0.0 (TID 21). 1202 bytes result sent to driver
2017-11-03 09:42:59,337 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 23.0 in stage 0.0 (TID 23, localhost, executor driver, partition 23, ANY, 4874 bytes)
2017-11-03 09:42:59,337 INFO[org.apache.spark.executor.Executor:54] - Running task 23.0 in stage 0.0 (TID 23)
2017-11-03 09:42:59,337 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 21.0 in stage 0.0 (TID 21) in 110 ms on localhost (executor driver) (22/175)
2017-11-03 09:42:59,337 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170602:0+1824947
2017-11-03 09:42:59,525 INFO[org.apache.spark.executor.Executor:54] - Finished task 22.0 in stage 0.0 (TID 22). 1245 bytes result sent to driver
2017-11-03 09:42:59,525 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 24.0 in stage 0.0 (TID 24, localhost, executor driver, partition 24, ANY, 4874 bytes)
2017-11-03 09:42:59,525 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 22.0 in stage 0.0 (TID 22) in 267 ms on localhost (executor driver) (23/175)
2017-11-03 09:42:59,525 INFO[org.apache.spark.executor.Executor:54] - Running task 24.0 in stage 0.0 (TID 24)
2017-11-03 09:42:59,525 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170603:0+499517
2017-11-03 09:42:59,652 INFO[org.apache.spark.executor.Executor:54] - Finished task 24.0 in stage 0.0 (TID 24). 1245 bytes result sent to driver
2017-11-03 09:42:59,652 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 25.0 in stage 0.0 (TID 25, localhost, executor driver, partition 25, ANY, 4874 bytes)
2017-11-03 09:42:59,652 INFO[org.apache.spark.executor.Executor:54] - Finished task 23.0 in stage 0.0 (TID 23). 1245 bytes result sent to driver
2017-11-03 09:42:59,652 INFO[org.apache.spark.executor.Executor:54] - Running task 25.0 in stage 0.0 (TID 25)
2017-11-03 09:42:59,652 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 24.0 in stage 0.0 (TID 24) in 127 ms on localhost (executor driver) (24/175)
2017-11-03 09:42:59,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 26.0 in stage 0.0 (TID 26, localhost, executor driver, partition 26, ANY, 4874 bytes)
2017-11-03 09:42:59,668 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170604:0+248722
2017-11-03 09:42:59,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 23.0 in stage 0.0 (TID 23) in 331 ms on localhost (executor driver) (25/175)
2017-11-03 09:42:59,668 INFO[org.apache.spark.executor.Executor:54] - Running task 26.0 in stage 0.0 (TID 26)
2017-11-03 09:42:59,668 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170605:0+4207838
2017-11-03 09:42:59,723 INFO[org.apache.spark.executor.Executor:54] - Finished task 25.0 in stage 0.0 (TID 25). 1288 bytes result sent to driver
2017-11-03 09:42:59,723 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 27.0 in stage 0.0 (TID 27, localhost, executor driver, partition 27, ANY, 4874 bytes)
2017-11-03 09:42:59,723 INFO[org.apache.spark.executor.Executor:54] - Running task 27.0 in stage 0.0 (TID 27)
2017-11-03 09:42:59,723 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 25.0 in stage 0.0 (TID 25) in 71 ms on localhost (executor driver) (26/175)
2017-11-03 09:42:59,723 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170606:0+3028515
2017-11-03 09:43:00,272 INFO[org.apache.spark.executor.Executor:54] - Finished task 27.0 in stage 0.0 (TID 27). 1245 bytes result sent to driver
2017-11-03 09:43:00,272 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 28.0 in stage 0.0 (TID 28, localhost, executor driver, partition 28, ANY, 4874 bytes)
2017-11-03 09:43:00,272 INFO[org.apache.spark.executor.Executor:54] - Running task 28.0 in stage 0.0 (TID 28)
2017-11-03 09:43:00,272 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 27.0 in stage 0.0 (TID 27) in 549 ms on localhost (executor driver) (27/175)
2017-11-03 09:43:00,272 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170607:0+1979782
2017-11-03 09:43:00,350 INFO[org.apache.spark.executor.Executor:54] - Finished task 26.0 in stage 0.0 (TID 26). 1245 bytes result sent to driver
2017-11-03 09:43:00,350 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 29.0 in stage 0.0 (TID 29, localhost, executor driver, partition 29, ANY, 4874 bytes)
2017-11-03 09:43:00,350 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 26.0 in stage 0.0 (TID 26) in 698 ms on localhost (executor driver) (28/175)
2017-11-03 09:43:00,350 INFO[org.apache.spark.executor.Executor:54] - Running task 29.0 in stage 0.0 (TID 29)
2017-11-03 09:43:00,350 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170608:0+3015575
2017-11-03 09:43:00,601 INFO[org.apache.spark.executor.Executor:54] - Finished task 28.0 in stage 0.0 (TID 28). 1245 bytes result sent to driver
2017-11-03 09:43:00,601 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 30.0 in stage 0.0 (TID 30, localhost, executor driver, partition 30, ANY, 4874 bytes)
2017-11-03 09:43:00,601 INFO[org.apache.spark.executor.Executor:54] - Running task 30.0 in stage 0.0 (TID 30)
2017-11-03 09:43:00,601 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 28.0 in stage 0.0 (TID 28) in 329 ms on localhost (executor driver) (29/175)
2017-11-03 09:43:00,601 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170609:0+1860146
2017-11-03 09:43:00,875 INFO[org.apache.spark.executor.Executor:54] - Finished task 29.0 in stage 0.0 (TID 29). 1245 bytes result sent to driver
2017-11-03 09:43:00,875 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 31.0 in stage 0.0 (TID 31, localhost, executor driver, partition 31, ANY, 4874 bytes)
2017-11-03 09:43:00,875 INFO[org.apache.spark.executor.Executor:54] - Running task 31.0 in stage 0.0 (TID 31)
2017-11-03 09:43:00,875 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 29.0 in stage 0.0 (TID 29) in 525 ms on localhost (executor driver) (30/175)
2017-11-03 09:43:00,875 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170610:0+270985
2017-11-03 09:43:00,922 INFO[org.apache.spark.executor.Executor:54] - Finished task 30.0 in stage 0.0 (TID 30). 1245 bytes result sent to driver
2017-11-03 09:43:00,922 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 32.0 in stage 0.0 (TID 32, localhost, executor driver, partition 32, ANY, 4874 bytes)
2017-11-03 09:43:00,922 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 30.0 in stage 0.0 (TID 30) in 321 ms on localhost (executor driver) (31/175)
2017-11-03 09:43:00,922 INFO[org.apache.spark.executor.Executor:54] - Running task 32.0 in stage 0.0 (TID 32)
2017-11-03 09:43:00,922 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170611:0+186937
2017-11-03 09:43:00,937 INFO[org.apache.spark.executor.Executor:54] - Finished task 31.0 in stage 0.0 (TID 31). 1202 bytes result sent to driver
2017-11-03 09:43:00,953 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 33.0 in stage 0.0 (TID 33, localhost, executor driver, partition 33, ANY, 4874 bytes)
2017-11-03 09:43:00,953 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 31.0 in stage 0.0 (TID 31) in 78 ms on localhost (executor driver) (32/175)
2017-11-03 09:43:00,953 INFO[org.apache.spark.executor.Executor:54] - Running task 33.0 in stage 0.0 (TID 33)
2017-11-03 09:43:00,953 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170612:0+2251117
2017-11-03 09:43:00,969 INFO[org.apache.spark.executor.Executor:54] - Finished task 32.0 in stage 0.0 (TID 32). 1202 bytes result sent to driver
2017-11-03 09:43:00,969 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 34.0 in stage 0.0 (TID 34, localhost, executor driver, partition 34, ANY, 4874 bytes)
2017-11-03 09:43:00,969 INFO[org.apache.spark.executor.Executor:54] - Running task 34.0 in stage 0.0 (TID 34)
2017-11-03 09:43:00,969 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 32.0 in stage 0.0 (TID 32) in 47 ms on localhost (executor driver) (33/175)
2017-11-03 09:43:00,969 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170613:0+1991138
2017-11-03 09:43:01,330 INFO[org.apache.spark.executor.Executor:54] - Finished task 33.0 in stage 0.0 (TID 33). 1245 bytes result sent to driver
2017-11-03 09:43:01,330 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 35.0 in stage 0.0 (TID 35, localhost, executor driver, partition 35, ANY, 4874 bytes)
2017-11-03 09:43:01,330 INFO[org.apache.spark.executor.Executor:54] - Running task 35.0 in stage 0.0 (TID 35)
2017-11-03 09:43:01,330 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 33.0 in stage 0.0 (TID 33) in 377 ms on localhost (executor driver) (34/175)
2017-11-03 09:43:01,330 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170614:0+2079646
2017-11-03 09:43:01,345 INFO[org.apache.spark.executor.Executor:54] - Finished task 34.0 in stage 0.0 (TID 34). 1245 bytes result sent to driver
2017-11-03 09:43:01,345 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 36.0 in stage 0.0 (TID 36, localhost, executor driver, partition 36, ANY, 4874 bytes)
2017-11-03 09:43:01,345 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 34.0 in stage 0.0 (TID 34) in 376 ms on localhost (executor driver) (35/175)
2017-11-03 09:43:01,345 INFO[org.apache.spark.executor.Executor:54] - Running task 36.0 in stage 0.0 (TID 36)
2017-11-03 09:43:01,345 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170615:0+2338613
2017-11-03 09:43:01,693 INFO[org.apache.spark.executor.Executor:54] - Finished task 35.0 in stage 0.0 (TID 35). 1245 bytes result sent to driver
2017-11-03 09:43:01,694 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 37.0 in stage 0.0 (TID 37, localhost, executor driver, partition 37, ANY, 4874 bytes)
2017-11-03 09:43:01,694 INFO[org.apache.spark.executor.Executor:54] - Running task 37.0 in stage 0.0 (TID 37)
2017-11-03 09:43:01,694 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 35.0 in stage 0.0 (TID 35) in 364 ms on localhost (executor driver) (36/175)
2017-11-03 09:43:01,696 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170616:0+1732249
2017-11-03 09:43:01,739 INFO[org.apache.spark.executor.Executor:54] - Finished task 36.0 in stage 0.0 (TID 36). 1245 bytes result sent to driver
2017-11-03 09:43:01,739 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 38.0 in stage 0.0 (TID 38, localhost, executor driver, partition 38, ANY, 4874 bytes)
2017-11-03 09:43:01,739 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 36.0 in stage 0.0 (TID 36) in 394 ms on localhost (executor driver) (37/175)
2017-11-03 09:43:01,739 INFO[org.apache.spark.executor.Executor:54] - Running task 38.0 in stage 0.0 (TID 38)
2017-11-03 09:43:01,739 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170617:0+270147
2017-11-03 09:43:01,821 INFO[org.apache.spark.executor.Executor:54] - Finished task 38.0 in stage 0.0 (TID 38). 1245 bytes result sent to driver
2017-11-03 09:43:01,821 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 39.0 in stage 0.0 (TID 39, localhost, executor driver, partition 39, ANY, 4874 bytes)
2017-11-03 09:43:01,821 INFO[org.apache.spark.executor.Executor:54] - Running task 39.0 in stage 0.0 (TID 39)
2017-11-03 09:43:01,821 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 38.0 in stage 0.0 (TID 38) in 82 ms on localhost (executor driver) (38/175)
2017-11-03 09:43:01,821 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170618:0+162544
2017-11-03 09:43:01,884 INFO[org.apache.spark.executor.Executor:54] - Finished task 39.0 in stage 0.0 (TID 39). 1202 bytes result sent to driver
2017-11-03 09:43:01,884 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 40.0 in stage 0.0 (TID 40, localhost, executor driver, partition 40, ANY, 4874 bytes)
2017-11-03 09:43:01,884 INFO[org.apache.spark.executor.Executor:54] - Running task 40.0 in stage 0.0 (TID 40)
2017-11-03 09:43:01,884 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 39.0 in stage 0.0 (TID 39) in 63 ms on localhost (executor driver) (39/175)
2017-11-03 09:43:01,884 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170619:0+2048985
2017-11-03 09:43:01,915 INFO[org.apache.spark.executor.Executor:54] - Finished task 37.0 in stage 0.0 (TID 37). 1288 bytes result sent to driver
2017-11-03 09:43:01,930 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 41.0 in stage 0.0 (TID 41, localhost, executor driver, partition 41, ANY, 4874 bytes)
2017-11-03 09:43:01,930 INFO[org.apache.spark.executor.Executor:54] - Running task 41.0 in stage 0.0 (TID 41)
2017-11-03 09:43:01,930 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 37.0 in stage 0.0 (TID 37) in 236 ms on localhost (executor driver) (40/175)
2017-11-03 09:43:01,930 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170620:0+1874096
2017-11-03 09:43:02,241 INFO[org.apache.spark.executor.Executor:54] - Finished task 40.0 in stage 0.0 (TID 40). 1245 bytes result sent to driver
2017-11-03 09:43:02,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 42.0 in stage 0.0 (TID 42, localhost, executor driver, partition 42, ANY, 4874 bytes)
2017-11-03 09:43:02,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 40.0 in stage 0.0 (TID 40) in 357 ms on localhost (executor driver) (41/175)
2017-11-03 09:43:02,241 INFO[org.apache.spark.executor.Executor:54] - Running task 42.0 in stage 0.0 (TID 42)
2017-11-03 09:43:02,241 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170621:0+2132269
2017-11-03 09:43:02,257 INFO[org.apache.spark.executor.Executor:54] - Finished task 41.0 in stage 0.0 (TID 41). 1245 bytes result sent to driver
2017-11-03 09:43:02,257 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 43.0 in stage 0.0 (TID 43, localhost, executor driver, partition 43, ANY, 4874 bytes)
2017-11-03 09:43:02,257 INFO[org.apache.spark.executor.Executor:54] - Running task 43.0 in stage 0.0 (TID 43)
2017-11-03 09:43:02,257 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170622:0+2300815
2017-11-03 09:43:02,257 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 41.0 in stage 0.0 (TID 41) in 327 ms on localhost (executor driver) (42/175)
2017-11-03 09:43:02,587 INFO[org.apache.spark.executor.Executor:54] - Finished task 42.0 in stage 0.0 (TID 42). 1245 bytes result sent to driver
2017-11-03 09:43:02,587 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 44.0 in stage 0.0 (TID 44, localhost, executor driver, partition 44, ANY, 4874 bytes)
2017-11-03 09:43:02,587 INFO[org.apache.spark.executor.Executor:54] - Running task 44.0 in stage 0.0 (TID 44)
2017-11-03 09:43:02,587 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 42.0 in stage 0.0 (TID 42) in 346 ms on localhost (executor driver) (43/175)
2017-11-03 09:43:02,587 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170623:0+2995790
2017-11-03 09:43:02,665 INFO[org.apache.spark.executor.Executor:54] - Finished task 43.0 in stage 0.0 (TID 43). 1245 bytes result sent to driver
2017-11-03 09:43:02,665 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 45.0 in stage 0.0 (TID 45, localhost, executor driver, partition 45, ANY, 4874 bytes)
2017-11-03 09:43:02,665 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 43.0 in stage 0.0 (TID 43) in 408 ms on localhost (executor driver) (44/175)
2017-11-03 09:43:02,665 INFO[org.apache.spark.executor.Executor:54] - Running task 45.0 in stage 0.0 (TID 45)
2017-11-03 09:43:02,665 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170624:0+657184
2017-11-03 09:43:02,829 INFO[org.apache.spark.executor.Executor:54] - Finished task 45.0 in stage 0.0 (TID 45). 1245 bytes result sent to driver
2017-11-03 09:43:02,830 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 46.0 in stage 0.0 (TID 46, localhost, executor driver, partition 46, ANY, 4874 bytes)
2017-11-03 09:43:02,830 INFO[org.apache.spark.executor.Executor:54] - Running task 46.0 in stage 0.0 (TID 46)
2017-11-03 09:43:02,832 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170625:0+616272
2017-11-03 09:43:02,832 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 45.0 in stage 0.0 (TID 45) in 165 ms on localhost (executor driver) (45/175)
2017-11-03 09:43:02,960 INFO[org.apache.spark.executor.Executor:54] - Finished task 46.0 in stage 0.0 (TID 46). 1288 bytes result sent to driver
2017-11-03 09:43:02,960 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 47.0 in stage 0.0 (TID 47, localhost, executor driver, partition 47, ANY, 4874 bytes)
2017-11-03 09:43:02,960 INFO[org.apache.spark.executor.Executor:54] - Running task 47.0 in stage 0.0 (TID 47)
2017-11-03 09:43:02,960 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 46.0 in stage 0.0 (TID 46) in 131 ms on localhost (executor driver) (46/175)
2017-11-03 09:43:02,960 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170626:0+2674205
2017-11-03 09:43:03,023 INFO[org.apache.spark.executor.Executor:54] - Finished task 44.0 in stage 0.0 (TID 44). 1245 bytes result sent to driver
2017-11-03 09:43:03,023 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 48.0 in stage 0.0 (TID 48, localhost, executor driver, partition 48, ANY, 4874 bytes)
2017-11-03 09:43:03,023 INFO[org.apache.spark.executor.Executor:54] - Running task 48.0 in stage 0.0 (TID 48)
2017-11-03 09:43:03,023 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 44.0 in stage 0.0 (TID 44) in 436 ms on localhost (executor driver) (47/175)
2017-11-03 09:43:03,023 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170627:0+2924710
2017-11-03 09:43:03,401 INFO[org.apache.spark.executor.Executor:54] - Finished task 47.0 in stage 0.0 (TID 47). 1245 bytes result sent to driver
2017-11-03 09:43:03,401 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 49.0 in stage 0.0 (TID 49, localhost, executor driver, partition 49, ANY, 4874 bytes)
2017-11-03 09:43:03,401 INFO[org.apache.spark.executor.Executor:54] - Running task 49.0 in stage 0.0 (TID 49)
2017-11-03 09:43:03,401 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 47.0 in stage 0.0 (TID 47) in 441 ms on localhost (executor driver) (48/175)
2017-11-03 09:43:03,416 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170628:0+2558815
2017-11-03 09:43:03,526 INFO[org.apache.spark.executor.Executor:54] - Finished task 48.0 in stage 0.0 (TID 48). 1245 bytes result sent to driver
2017-11-03 09:43:03,526 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 50.0 in stage 0.0 (TID 50, localhost, executor driver, partition 50, ANY, 4874 bytes)
2017-11-03 09:43:03,526 INFO[org.apache.spark.executor.Executor:54] - Running task 50.0 in stage 0.0 (TID 50)
2017-11-03 09:43:03,526 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 48.0 in stage 0.0 (TID 48) in 503 ms on localhost (executor driver) (49/175)
2017-11-03 09:43:03,526 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170629:0+2641959
2017-11-03 09:43:03,854 INFO[org.apache.spark.executor.Executor:54] - Finished task 49.0 in stage 0.0 (TID 49). 1245 bytes result sent to driver
2017-11-03 09:43:03,855 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 51.0 in stage 0.0 (TID 51, localhost, executor driver, partition 51, ANY, 4874 bytes)
2017-11-03 09:43:03,855 INFO[org.apache.spark.executor.Executor:54] - Running task 51.0 in stage 0.0 (TID 51)
2017-11-03 09:43:03,856 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 49.0 in stage 0.0 (TID 49) in 455 ms on localhost (executor driver) (50/175)
2017-11-03 09:43:03,858 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170630:0+2278383
2017-11-03 09:43:03,970 INFO[org.apache.spark.executor.Executor:54] - Finished task 50.0 in stage 0.0 (TID 50). 1245 bytes result sent to driver
2017-11-03 09:43:03,970 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 52.0 in stage 0.0 (TID 52, localhost, executor driver, partition 52, ANY, 4874 bytes)
2017-11-03 09:43:03,970 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 50.0 in stage 0.0 (TID 50) in 444 ms on localhost (executor driver) (51/175)
2017-11-03 09:43:03,970 INFO[org.apache.spark.executor.Executor:54] - Running task 52.0 in stage 0.0 (TID 52)
2017-11-03 09:43:03,985 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170701:0+1162324
2017-11-03 09:43:04,207 INFO[org.apache.spark.executor.Executor:54] - Finished task 52.0 in stage 0.0 (TID 52). 1288 bytes result sent to driver
2017-11-03 09:43:04,207 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 53.0 in stage 0.0 (TID 53, localhost, executor driver, partition 53, ANY, 4874 bytes)
2017-11-03 09:43:04,207 INFO[org.apache.spark.executor.Executor:54] - Running task 53.0 in stage 0.0 (TID 53)
2017-11-03 09:43:04,207 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 52.0 in stage 0.0 (TID 52) in 237 ms on localhost (executor driver) (52/175)
2017-11-03 09:43:04,207 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170702:0+937235
2017-11-03 09:43:04,223 INFO[org.apache.spark.executor.Executor:54] - Finished task 51.0 in stage 0.0 (TID 51). 1288 bytes result sent to driver
2017-11-03 09:43:04,223 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 54.0 in stage 0.0 (TID 54, localhost, executor driver, partition 54, ANY, 4874 bytes)
2017-11-03 09:43:04,223 INFO[org.apache.spark.executor.Executor:54] - Running task 54.0 in stage 0.0 (TID 54)
2017-11-03 09:43:04,223 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 51.0 in stage 0.0 (TID 51) in 368 ms on localhost (executor driver) (53/175)
2017-11-03 09:43:04,223 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170703:0+2491976
2017-11-03 09:43:04,364 INFO[org.apache.spark.executor.Executor:54] - Finished task 53.0 in stage 0.0 (TID 53). 1245 bytes result sent to driver
2017-11-03 09:43:04,364 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 55.0 in stage 0.0 (TID 55, localhost, executor driver, partition 55, ANY, 4874 bytes)
2017-11-03 09:43:04,364 INFO[org.apache.spark.executor.Executor:54] - Running task 55.0 in stage 0.0 (TID 55)
2017-11-03 09:43:04,364 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 53.0 in stage 0.0 (TID 53) in 157 ms on localhost (executor driver) (54/175)
2017-11-03 09:43:04,364 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170704:0+2414237
2017-11-03 09:43:04,663 INFO[org.apache.spark.executor.Executor:54] - Finished task 54.0 in stage 0.0 (TID 54). 1245 bytes result sent to driver
2017-11-03 09:43:04,663 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 56.0 in stage 0.0 (TID 56, localhost, executor driver, partition 56, ANY, 4874 bytes)
2017-11-03 09:43:04,663 INFO[org.apache.spark.executor.Executor:54] - Running task 56.0 in stage 0.0 (TID 56)
2017-11-03 09:43:04,663 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 54.0 in stage 0.0 (TID 54) in 440 ms on localhost (executor driver) (55/175)
2017-11-03 09:43:04,663 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170705:0+3384045
2017-11-03 09:43:04,786 INFO[org.apache.spark.executor.Executor:54] - Finished task 55.0 in stage 0.0 (TID 55). 1245 bytes result sent to driver
2017-11-03 09:43:04,786 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 57.0 in stage 0.0 (TID 57, localhost, executor driver, partition 57, ANY, 4874 bytes)
2017-11-03 09:43:04,786 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 55.0 in stage 0.0 (TID 55) in 422 ms on localhost (executor driver) (56/175)
2017-11-03 09:43:04,786 INFO[org.apache.spark.executor.Executor:54] - Running task 57.0 in stage 0.0 (TID 57)
2017-11-03 09:43:04,786 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170706:0+3827135
2017-11-03 09:43:05,218 INFO[org.apache.spark.executor.Executor:54] - Finished task 56.0 in stage 0.0 (TID 56). 1245 bytes result sent to driver
2017-11-03 09:43:05,218 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 58.0 in stage 0.0 (TID 58, localhost, executor driver, partition 58, ANY, 4874 bytes)
2017-11-03 09:43:05,218 INFO[org.apache.spark.executor.Executor:54] - Running task 58.0 in stage 0.0 (TID 58)
2017-11-03 09:43:05,218 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 56.0 in stage 0.0 (TID 56) in 555 ms on localhost (executor driver) (57/175)
2017-11-03 09:43:05,234 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170707:0+5032728
2017-11-03 09:43:05,437 INFO[org.apache.spark.executor.Executor:54] - Finished task 57.0 in stage 0.0 (TID 57). 1245 bytes result sent to driver
2017-11-03 09:43:05,437 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 59.0 in stage 0.0 (TID 59, localhost, executor driver, partition 59, ANY, 4874 bytes)
2017-11-03 09:43:05,437 INFO[org.apache.spark.executor.Executor:54] - Running task 59.0 in stage 0.0 (TID 59)
2017-11-03 09:43:05,437 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 57.0 in stage 0.0 (TID 57) in 651 ms on localhost (executor driver) (58/175)
2017-11-03 09:43:05,437 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170708:0+842464
2017-11-03 09:43:05,628 INFO[org.apache.spark.executor.Executor:54] - Finished task 59.0 in stage 0.0 (TID 59). 1202 bytes result sent to driver
2017-11-03 09:43:05,628 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 60.0 in stage 0.0 (TID 60, localhost, executor driver, partition 60, ANY, 4874 bytes)
2017-11-03 09:43:05,628 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 59.0 in stage 0.0 (TID 59) in 191 ms on localhost (executor driver) (59/175)
2017-11-03 09:43:05,628 INFO[org.apache.spark.executor.Executor:54] - Running task 60.0 in stage 0.0 (TID 60)
2017-11-03 09:43:05,628 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170709:0+907574
2017-11-03 09:43:05,823 INFO[org.apache.spark.executor.Executor:54] - Finished task 60.0 in stage 0.0 (TID 60). 1245 bytes result sent to driver
2017-11-03 09:43:05,827 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 61.0 in stage 0.0 (TID 61, localhost, executor driver, partition 61, ANY, 4874 bytes)
2017-11-03 09:43:05,829 INFO[org.apache.spark.executor.Executor:54] - Running task 61.0 in stage 0.0 (TID 61)
2017-11-03 09:43:05,829 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 60.0 in stage 0.0 (TID 60) in 201 ms on localhost (executor driver) (60/175)
2017-11-03 09:43:05,829 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170710:0+5285742
2017-11-03 09:43:06,097 INFO[org.apache.spark.executor.Executor:54] - Finished task 58.0 in stage 0.0 (TID 58). 1288 bytes result sent to driver
2017-11-03 09:43:06,097 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 62.0 in stage 0.0 (TID 62, localhost, executor driver, partition 62, ANY, 4874 bytes)
2017-11-03 09:43:06,097 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 58.0 in stage 0.0 (TID 58) in 879 ms on localhost (executor driver) (61/175)
2017-11-03 09:43:06,097 INFO[org.apache.spark.executor.Executor:54] - Running task 62.0 in stage 0.0 (TID 62)
2017-11-03 09:43:06,097 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170711:0+4366041
2017-11-03 09:43:06,785 INFO[org.apache.spark.executor.Executor:54] - Finished task 62.0 in stage 0.0 (TID 62). 1245 bytes result sent to driver
2017-11-03 09:43:06,785 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 63.0 in stage 0.0 (TID 63, localhost, executor driver, partition 63, ANY, 4874 bytes)
2017-11-03 09:43:06,785 INFO[org.apache.spark.executor.Executor:54] - Running task 63.0 in stage 0.0 (TID 63)
2017-11-03 09:43:06,785 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 62.0 in stage 0.0 (TID 62) in 688 ms on localhost (executor driver) (62/175)
2017-11-03 09:43:06,785 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170712:0+1816679
2017-11-03 09:43:06,801 INFO[org.apache.spark.executor.Executor:54] - Finished task 61.0 in stage 0.0 (TID 61). 1245 bytes result sent to driver
2017-11-03 09:43:06,801 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 64.0 in stage 0.0 (TID 64, localhost, executor driver, partition 64, ANY, 4874 bytes)
2017-11-03 09:43:06,801 INFO[org.apache.spark.executor.Executor:54] - Running task 64.0 in stage 0.0 (TID 64)
2017-11-03 09:43:06,801 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 61.0 in stage 0.0 (TID 61) in 976 ms on localhost (executor driver) (63/175)
2017-11-03 09:43:06,801 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170713:0+1208381
2017-11-03 09:43:07,044 INFO[org.apache.spark.executor.Executor:54] - Finished task 64.0 in stage 0.0 (TID 64). 1202 bytes result sent to driver
2017-11-03 09:43:07,059 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 65.0 in stage 0.0 (TID 65, localhost, executor driver, partition 65, ANY, 4874 bytes)
2017-11-03 09:43:07,059 INFO[org.apache.spark.executor.Executor:54] - Running task 65.0 in stage 0.0 (TID 65)
2017-11-03 09:43:07,059 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 64.0 in stage 0.0 (TID 64) in 258 ms on localhost (executor driver) (64/175)
2017-11-03 09:43:07,059 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170714:0+1159302
2017-11-03 09:43:07,079 INFO[org.apache.spark.executor.Executor:54] - Finished task 63.0 in stage 0.0 (TID 63). 1202 bytes result sent to driver
2017-11-03 09:43:07,080 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 66.0 in stage 0.0 (TID 66, localhost, executor driver, partition 66, ANY, 4874 bytes)
2017-11-03 09:43:07,080 INFO[org.apache.spark.executor.Executor:54] - Running task 66.0 in stage 0.0 (TID 66)
2017-11-03 09:43:07,080 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 63.0 in stage 0.0 (TID 63) in 295 ms on localhost (executor driver) (65/175)
2017-11-03 09:43:07,082 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170715:0+553654
2017-11-03 09:43:07,187 INFO[org.apache.spark.executor.Executor:54] - Finished task 66.0 in stage 0.0 (TID 66). 1288 bytes result sent to driver
2017-11-03 09:43:07,187 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 67.0 in stage 0.0 (TID 67, localhost, executor driver, partition 67, ANY, 4874 bytes)
2017-11-03 09:43:07,187 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 66.0 in stage 0.0 (TID 66) in 108 ms on localhost (executor driver) (66/175)
2017-11-03 09:43:07,187 INFO[org.apache.spark.executor.Executor:54] - Running task 67.0 in stage 0.0 (TID 67)
2017-11-03 09:43:07,187 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170716:0+536799
2017-11-03 09:43:07,234 INFO[org.apache.spark.executor.Executor:54] - Finished task 65.0 in stage 0.0 (TID 65). 1245 bytes result sent to driver
2017-11-03 09:43:07,234 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 68.0 in stage 0.0 (TID 68, localhost, executor driver, partition 68, ANY, 4874 bytes)
2017-11-03 09:43:07,234 INFO[org.apache.spark.executor.Executor:54] - Running task 68.0 in stage 0.0 (TID 68)
2017-11-03 09:43:07,234 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 65.0 in stage 0.0 (TID 65) in 175 ms on localhost (executor driver) (67/175)
2017-11-03 09:43:07,234 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170717:0+1056541
2017-11-03 09:43:07,281 INFO[org.apache.spark.executor.Executor:54] - Finished task 67.0 in stage 0.0 (TID 67). 1202 bytes result sent to driver
2017-11-03 09:43:07,281 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 69.0 in stage 0.0 (TID 69, localhost, executor driver, partition 69, ANY, 4874 bytes)
2017-11-03 09:43:07,281 INFO[org.apache.spark.executor.Executor:54] - Running task 69.0 in stage 0.0 (TID 69)
2017-11-03 09:43:07,281 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 67.0 in stage 0.0 (TID 67) in 94 ms on localhost (executor driver) (68/175)
2017-11-03 09:43:07,281 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170718:0+912008
2017-11-03 09:43:07,408 INFO[org.apache.spark.executor.Executor:54] - Finished task 68.0 in stage 0.0 (TID 68). 1202 bytes result sent to driver
2017-11-03 09:43:07,408 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 70.0 in stage 0.0 (TID 70, localhost, executor driver, partition 70, ANY, 4874 bytes)
2017-11-03 09:43:07,408 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 68.0 in stage 0.0 (TID 68) in 174 ms on localhost (executor driver) (69/175)
2017-11-03 09:43:07,408 INFO[org.apache.spark.executor.Executor:54] - Running task 70.0 in stage 0.0 (TID 70)
2017-11-03 09:43:07,408 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170719:0+2331303
2017-11-03 09:43:07,455 INFO[org.apache.spark.executor.Executor:54] - Finished task 69.0 in stage 0.0 (TID 69). 1202 bytes result sent to driver
2017-11-03 09:43:07,455 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 71.0 in stage 0.0 (TID 71, localhost, executor driver, partition 71, ANY, 4874 bytes)
2017-11-03 09:43:07,455 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 69.0 in stage 0.0 (TID 69) in 174 ms on localhost (executor driver) (70/175)
2017-11-03 09:43:07,455 INFO[org.apache.spark.executor.Executor:54] - Running task 71.0 in stage 0.0 (TID 71)
2017-11-03 09:43:07,455 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170720:0+1438100
2017-11-03 09:43:07,708 INFO[org.apache.spark.executor.Executor:54] - Finished task 71.0 in stage 0.0 (TID 71). 1245 bytes result sent to driver
2017-11-03 09:43:07,708 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 72.0 in stage 0.0 (TID 72, localhost, executor driver, partition 72, ANY, 4874 bytes)
2017-11-03 09:43:07,708 INFO[org.apache.spark.executor.Executor:54] - Running task 72.0 in stage 0.0 (TID 72)
2017-11-03 09:43:07,708 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 71.0 in stage 0.0 (TID 71) in 253 ms on localhost (executor driver) (71/175)
2017-11-03 09:43:07,708 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170721:0+1746836
2017-11-03 09:43:07,786 INFO[org.apache.spark.executor.Executor:54] - Finished task 70.0 in stage 0.0 (TID 70). 1245 bytes result sent to driver
2017-11-03 09:43:07,786 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 73.0 in stage 0.0 (TID 73, localhost, executor driver, partition 73, ANY, 4874 bytes)
2017-11-03 09:43:07,786 INFO[org.apache.spark.executor.Executor:54] - Running task 73.0 in stage 0.0 (TID 73)
2017-11-03 09:43:07,786 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 70.0 in stage 0.0 (TID 70) in 378 ms on localhost (executor driver) (72/175)
2017-11-03 09:43:07,786 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170722:0+706675
2017-11-03 09:43:07,942 INFO[org.apache.spark.executor.Executor:54] - Finished task 73.0 in stage 0.0 (TID 73). 1245 bytes result sent to driver
2017-11-03 09:43:07,942 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 74.0 in stage 0.0 (TID 74, localhost, executor driver, partition 74, ANY, 4874 bytes)
2017-11-03 09:43:07,942 INFO[org.apache.spark.executor.Executor:54] - Running task 74.0 in stage 0.0 (TID 74)
2017-11-03 09:43:07,942 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 73.0 in stage 0.0 (TID 73) in 156 ms on localhost (executor driver) (73/175)
2017-11-03 09:43:07,942 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170723:0+784427
2017-11-03 09:43:07,973 INFO[org.apache.spark.executor.Executor:54] - Finished task 72.0 in stage 0.0 (TID 72). 1245 bytes result sent to driver
2017-11-03 09:43:07,973 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 75.0 in stage 0.0 (TID 75, localhost, executor driver, partition 75, ANY, 4874 bytes)
2017-11-03 09:43:07,973 INFO[org.apache.spark.executor.Executor:54] - Running task 75.0 in stage 0.0 (TID 75)
2017-11-03 09:43:07,973 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 72.0 in stage 0.0 (TID 72) in 265 ms on localhost (executor driver) (74/175)
2017-11-03 09:43:07,973 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170724:0+1871802
2017-11-03 09:43:08,098 INFO[org.apache.spark.executor.Executor:54] - Finished task 74.0 in stage 0.0 (TID 74). 1202 bytes result sent to driver
2017-11-03 09:43:08,098 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 76.0 in stage 0.0 (TID 76, localhost, executor driver, partition 76, ANY, 4874 bytes)
2017-11-03 09:43:08,098 INFO[org.apache.spark.executor.Executor:54] - Running task 76.0 in stage 0.0 (TID 76)
2017-11-03 09:43:08,098 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 74.0 in stage 0.0 (TID 74) in 156 ms on localhost (executor driver) (75/175)
2017-11-03 09:43:08,098 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170725:0+1456598
2017-11-03 09:43:08,256 INFO[org.apache.spark.executor.Executor:54] - Finished task 75.0 in stage 0.0 (TID 75). 1245 bytes result sent to driver
2017-11-03 09:43:08,256 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 77.0 in stage 0.0 (TID 77, localhost, executor driver, partition 77, ANY, 4874 bytes)
2017-11-03 09:43:08,256 INFO[org.apache.spark.executor.Executor:54] - Running task 77.0 in stage 0.0 (TID 77)
2017-11-03 09:43:08,256 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 75.0 in stage 0.0 (TID 75) in 283 ms on localhost (executor driver) (76/175)
2017-11-03 09:43:08,256 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170726:0+1628433
2017-11-03 09:43:08,365 INFO[org.apache.spark.executor.Executor:54] - Finished task 76.0 in stage 0.0 (TID 76). 1245 bytes result sent to driver
2017-11-03 09:43:08,365 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 78.0 in stage 0.0 (TID 78, localhost, executor driver, partition 78, ANY, 4874 bytes)
2017-11-03 09:43:08,365 INFO[org.apache.spark.executor.Executor:54] - Running task 78.0 in stage 0.0 (TID 78)
2017-11-03 09:43:08,365 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 76.0 in stage 0.0 (TID 76) in 267 ms on localhost (executor driver) (77/175)
2017-11-03 09:43:08,365 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170727:0+1421817
2017-11-03 09:43:08,537 INFO[org.apache.spark.executor.Executor:54] - Finished task 77.0 in stage 0.0 (TID 77). 1245 bytes result sent to driver
2017-11-03 09:43:08,537 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 79.0 in stage 0.0 (TID 79, localhost, executor driver, partition 79, ANY, 4874 bytes)
2017-11-03 09:43:08,537 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 77.0 in stage 0.0 (TID 77) in 281 ms on localhost (executor driver) (78/175)
2017-11-03 09:43:08,537 INFO[org.apache.spark.executor.Executor:54] - Running task 79.0 in stage 0.0 (TID 79)
2017-11-03 09:43:08,537 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170728:0+1107284
2017-11-03 09:43:08,601 INFO[org.apache.spark.executor.Executor:54] - Finished task 78.0 in stage 0.0 (TID 78). 1245 bytes result sent to driver
2017-11-03 09:43:08,601 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 80.0 in stage 0.0 (TID 80, localhost, executor driver, partition 80, ANY, 4874 bytes)
2017-11-03 09:43:08,601 INFO[org.apache.spark.executor.Executor:54] - Running task 80.0 in stage 0.0 (TID 80)
2017-11-03 09:43:08,601 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 78.0 in stage 0.0 (TID 78) in 236 ms on localhost (executor driver) (79/175)
2017-11-03 09:43:08,601 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170729:0+660900
2017-11-03 09:43:08,723 INFO[org.apache.spark.executor.Executor:54] - Finished task 79.0 in stage 0.0 (TID 79). 1202 bytes result sent to driver
2017-11-03 09:43:08,723 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 81.0 in stage 0.0 (TID 81, localhost, executor driver, partition 81, ANY, 4874 bytes)
2017-11-03 09:43:08,723 INFO[org.apache.spark.executor.Executor:54] - Running task 81.0 in stage 0.0 (TID 81)
2017-11-03 09:43:08,723 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 79.0 in stage 0.0 (TID 79) in 186 ms on localhost (executor driver) (80/175)
2017-11-03 09:43:08,723 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170730:0+485912
2017-11-03 09:43:08,739 INFO[org.apache.spark.executor.Executor:54] - Finished task 80.0 in stage 0.0 (TID 80). 1202 bytes result sent to driver
2017-11-03 09:43:08,739 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 82.0 in stage 0.0 (TID 82, localhost, executor driver, partition 82, ANY, 4874 bytes)
2017-11-03 09:43:08,739 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 80.0 in stage 0.0 (TID 80) in 138 ms on localhost (executor driver) (81/175)
2017-11-03 09:43:08,739 INFO[org.apache.spark.executor.Executor:54] - Running task 82.0 in stage 0.0 (TID 82)
2017-11-03 09:43:08,739 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170731:0+1471490
2017-11-03 09:43:08,808 INFO[org.apache.spark.executor.Executor:54] - Finished task 81.0 in stage 0.0 (TID 81). 1202 bytes result sent to driver
2017-11-03 09:43:08,809 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 83.0 in stage 0.0 (TID 83, localhost, executor driver, partition 83, ANY, 4874 bytes)
2017-11-03 09:43:08,810 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 81.0 in stage 0.0 (TID 81) in 87 ms on localhost (executor driver) (82/175)
2017-11-03 09:43:08,810 INFO[org.apache.spark.executor.Executor:54] - Running task 83.0 in stage 0.0 (TID 83)
2017-11-03 09:43:08,811 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170801:0+1539952
2017-11-03 09:43:08,993 INFO[org.apache.spark.executor.Executor:54] - Finished task 82.0 in stage 0.0 (TID 82). 1245 bytes result sent to driver
2017-11-03 09:43:08,993 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 84.0 in stage 0.0 (TID 84, localhost, executor driver, partition 84, ANY, 4874 bytes)
2017-11-03 09:43:08,993 INFO[org.apache.spark.executor.Executor:54] - Running task 84.0 in stage 0.0 (TID 84)
2017-11-03 09:43:08,993 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 82.0 in stage 0.0 (TID 82) in 254 ms on localhost (executor driver) (83/175)
2017-11-03 09:43:09,009 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170802:0+1620968
2017-11-03 09:43:09,056 INFO[org.apache.spark.executor.Executor:54] - Finished task 83.0 in stage 0.0 (TID 83). 1245 bytes result sent to driver
2017-11-03 09:43:09,056 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 85.0 in stage 0.0 (TID 85, localhost, executor driver, partition 85, ANY, 4874 bytes)
2017-11-03 09:43:09,056 INFO[org.apache.spark.executor.Executor:54] - Running task 85.0 in stage 0.0 (TID 85)
2017-11-03 09:43:09,056 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 83.0 in stage 0.0 (TID 83) in 247 ms on localhost (executor driver) (84/175)
2017-11-03 09:43:09,056 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170803:0+1396859
2017-11-03 09:43:09,297 INFO[org.apache.spark.executor.Executor:54] - Finished task 85.0 in stage 0.0 (TID 85). 1245 bytes result sent to driver
2017-11-03 09:43:09,297 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 86.0 in stage 0.0 (TID 86, localhost, executor driver, partition 86, ANY, 4874 bytes)
2017-11-03 09:43:09,297 INFO[org.apache.spark.executor.Executor:54] - Running task 86.0 in stage 0.0 (TID 86)
2017-11-03 09:43:09,297 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 85.0 in stage 0.0 (TID 85) in 241 ms on localhost (executor driver) (85/175)
2017-11-03 09:43:09,297 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170804:0+2260266
2017-11-03 09:43:09,297 INFO[org.apache.spark.executor.Executor:54] - Finished task 84.0 in stage 0.0 (TID 84). 1331 bytes result sent to driver
2017-11-03 09:43:09,297 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 87.0 in stage 0.0 (TID 87, localhost, executor driver, partition 87, ANY, 4874 bytes)
2017-11-03 09:43:09,297 INFO[org.apache.spark.executor.Executor:54] - Running task 87.0 in stage 0.0 (TID 87)
2017-11-03 09:43:09,297 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 84.0 in stage 0.0 (TID 84) in 304 ms on localhost (executor driver) (86/175)
2017-11-03 09:43:09,297 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170805:0+903177
2017-11-03 09:43:09,516 INFO[org.apache.spark.executor.Executor:54] - Finished task 87.0 in stage 0.0 (TID 87). 1202 bytes result sent to driver
2017-11-03 09:43:09,516 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 88.0 in stage 0.0 (TID 88, localhost, executor driver, partition 88, ANY, 4874 bytes)
2017-11-03 09:43:09,516 INFO[org.apache.spark.executor.Executor:54] - Running task 88.0 in stage 0.0 (TID 88)
2017-11-03 09:43:09,516 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 87.0 in stage 0.0 (TID 87) in 219 ms on localhost (executor driver) (87/175)
2017-11-03 09:43:09,516 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170806:0+667888
2017-11-03 09:43:09,611 INFO[org.apache.spark.executor.Executor:54] - Finished task 86.0 in stage 0.0 (TID 86). 1245 bytes result sent to driver
2017-11-03 09:43:09,611 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 89.0 in stage 0.0 (TID 89, localhost, executor driver, partition 89, ANY, 4874 bytes)
2017-11-03 09:43:09,611 INFO[org.apache.spark.executor.Executor:54] - Running task 89.0 in stage 0.0 (TID 89)
2017-11-03 09:43:09,611 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 86.0 in stage 0.0 (TID 86) in 314 ms on localhost (executor driver) (88/175)
2017-11-03 09:43:09,611 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170807:0+1970293
2017-11-03 09:43:09,642 INFO[org.apache.spark.executor.Executor:54] - Finished task 88.0 in stage 0.0 (TID 88). 1245 bytes result sent to driver
2017-11-03 09:43:09,642 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 90.0 in stage 0.0 (TID 90, localhost, executor driver, partition 90, ANY, 4874 bytes)
2017-11-03 09:43:09,642 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 88.0 in stage 0.0 (TID 88) in 126 ms on localhost (executor driver) (89/175)
2017-11-03 09:43:09,642 INFO[org.apache.spark.executor.Executor:54] - Running task 90.0 in stage 0.0 (TID 90)
2017-11-03 09:43:09,658 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170808:0+4666501
2017-11-03 09:43:09,921 INFO[org.apache.spark.executor.Executor:54] - Finished task 89.0 in stage 0.0 (TID 89). 1202 bytes result sent to driver
2017-11-03 09:43:09,921 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 91.0 in stage 0.0 (TID 91, localhost, executor driver, partition 91, ANY, 4874 bytes)
2017-11-03 09:43:09,921 INFO[org.apache.spark.executor.Executor:54] - Running task 91.0 in stage 0.0 (TID 91)
2017-11-03 09:43:09,921 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 89.0 in stage 0.0 (TID 89) in 310 ms on localhost (executor driver) (90/175)
2017-11-03 09:43:09,921 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170809:0+2604356
2017-11-03 09:43:10,364 INFO[org.apache.spark.executor.Executor:54] - Finished task 91.0 in stage 0.0 (TID 91). 1245 bytes result sent to driver
2017-11-03 09:43:10,364 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 92.0 in stage 0.0 (TID 92, localhost, executor driver, partition 92, ANY, 4874 bytes)
2017-11-03 09:43:10,364 INFO[org.apache.spark.executor.Executor:54] - Running task 92.0 in stage 0.0 (TID 92)
2017-11-03 09:43:10,364 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 91.0 in stage 0.0 (TID 91) in 443 ms on localhost (executor driver) (91/175)
2017-11-03 09:43:10,364 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170810:0+3408724
2017-11-03 09:43:10,473 INFO[org.apache.spark.executor.Executor:54] - Finished task 90.0 in stage 0.0 (TID 90). 1245 bytes result sent to driver
2017-11-03 09:43:10,473 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 93.0 in stage 0.0 (TID 93, localhost, executor driver, partition 93, ANY, 4874 bytes)
2017-11-03 09:43:10,473 INFO[org.apache.spark.executor.Executor:54] - Running task 93.0 in stage 0.0 (TID 93)
2017-11-03 09:43:10,473 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 90.0 in stage 0.0 (TID 90) in 831 ms on localhost (executor driver) (92/175)
2017-11-03 09:43:10,473 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170811:0+2518210
2017-11-03 09:43:10,909 INFO[org.apache.spark.executor.Executor:54] - Finished task 92.0 in stage 0.0 (TID 92). 1245 bytes result sent to driver
2017-11-03 09:43:10,909 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 94.0 in stage 0.0 (TID 94, localhost, executor driver, partition 94, ANY, 4874 bytes)
2017-11-03 09:43:10,909 INFO[org.apache.spark.executor.Executor:54] - Running task 94.0 in stage 0.0 (TID 94)
2017-11-03 09:43:10,909 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 92.0 in stage 0.0 (TID 92) in 545 ms on localhost (executor driver) (93/175)
2017-11-03 09:43:10,909 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170812:0+2083124
2017-11-03 09:43:10,956 INFO[org.apache.spark.executor.Executor:54] - Finished task 93.0 in stage 0.0 (TID 93). 1245 bytes result sent to driver
2017-11-03 09:43:10,957 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 95.0 in stage 0.0 (TID 95, localhost, executor driver, partition 95, ANY, 4874 bytes)
2017-11-03 09:43:10,957 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 93.0 in stage 0.0 (TID 93) in 484 ms on localhost (executor driver) (94/175)
2017-11-03 09:43:10,960 INFO[org.apache.spark.executor.Executor:54] - Running task 95.0 in stage 0.0 (TID 95)
2017-11-03 09:43:10,963 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170813:0+815479
2017-11-03 09:43:11,133 INFO[org.apache.spark.executor.Executor:54] - Finished task 95.0 in stage 0.0 (TID 95). 1288 bytes result sent to driver
2017-11-03 09:43:11,133 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 96.0 in stage 0.0 (TID 96, localhost, executor driver, partition 96, ANY, 4874 bytes)
2017-11-03 09:43:11,133 INFO[org.apache.spark.executor.Executor:54] - Running task 96.0 in stage 0.0 (TID 96)
2017-11-03 09:43:11,133 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170814:0+3338002
2017-11-03 09:43:11,133 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 95.0 in stage 0.0 (TID 95) in 177 ms on localhost (executor driver) (95/175)
2017-11-03 09:43:11,211 INFO[org.apache.spark.executor.Executor:54] - Finished task 94.0 in stage 0.0 (TID 94). 1245 bytes result sent to driver
2017-11-03 09:43:11,211 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 97.0 in stage 0.0 (TID 97, localhost, executor driver, partition 97, ANY, 4874 bytes)
2017-11-03 09:43:11,211 INFO[org.apache.spark.executor.Executor:54] - Running task 97.0 in stage 0.0 (TID 97)
2017-11-03 09:43:11,211 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 94.0 in stage 0.0 (TID 94) in 302 ms on localhost (executor driver) (96/175)
2017-11-03 09:43:11,211 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170815:0+1644033
2017-11-03 09:43:11,492 INFO[org.apache.spark.executor.Executor:54] - Finished task 97.0 in stage 0.0 (TID 97). 1245 bytes result sent to driver
2017-11-03 09:43:11,492 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 98.0 in stage 0.0 (TID 98, localhost, executor driver, partition 98, ANY, 4874 bytes)
2017-11-03 09:43:11,492 INFO[org.apache.spark.executor.Executor:54] - Running task 98.0 in stage 0.0 (TID 98)
2017-11-03 09:43:11,492 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 97.0 in stage 0.0 (TID 97) in 281 ms on localhost (executor driver) (97/175)
2017-11-03 09:43:11,492 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170816:0+3514558
2017-11-03 09:43:11,708 INFO[org.apache.spark.executor.Executor:54] - Finished task 96.0 in stage 0.0 (TID 96). 1245 bytes result sent to driver
2017-11-03 09:43:11,708 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 99.0 in stage 0.0 (TID 99, localhost, executor driver, partition 99, ANY, 4874 bytes)
2017-11-03 09:43:11,708 INFO[org.apache.spark.executor.Executor:54] - Running task 99.0 in stage 0.0 (TID 99)
2017-11-03 09:43:11,708 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 96.0 in stage 0.0 (TID 96) in 575 ms on localhost (executor driver) (98/175)
2017-11-03 09:43:11,708 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170817:0+3738762
2017-11-03 09:43:12,084 INFO[org.apache.spark.executor.Executor:54] - Finished task 98.0 in stage 0.0 (TID 98). 1245 bytes result sent to driver
2017-11-03 09:43:12,084 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 100.0 in stage 0.0 (TID 100, localhost, executor driver, partition 100, ANY, 4874 bytes)
2017-11-03 09:43:12,084 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 98.0 in stage 0.0 (TID 98) in 592 ms on localhost (executor driver) (99/175)
2017-11-03 09:43:12,084 INFO[org.apache.spark.executor.Executor:54] - Running task 100.0 in stage 0.0 (TID 100)
2017-11-03 09:43:12,100 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170818:0+2790964
2017-11-03 09:43:12,381 INFO[org.apache.spark.executor.Executor:54] - Finished task 99.0 in stage 0.0 (TID 99). 1245 bytes result sent to driver
2017-11-03 09:43:12,381 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 101.0 in stage 0.0 (TID 101, localhost, executor driver, partition 101, ANY, 4874 bytes)
2017-11-03 09:43:12,381 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 99.0 in stage 0.0 (TID 99) in 673 ms on localhost (executor driver) (100/175)
2017-11-03 09:43:12,381 INFO[org.apache.spark.executor.Executor:54] - Running task 101.0 in stage 0.0 (TID 101)
2017-11-03 09:43:12,381 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170819:0+1167927
2017-11-03 09:43:12,537 INFO[org.apache.spark.executor.Executor:54] - Finished task 100.0 in stage 0.0 (TID 100). 1331 bytes result sent to driver
2017-11-03 09:43:12,537 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 102.0 in stage 0.0 (TID 102, localhost, executor driver, partition 102, ANY, 4874 bytes)
2017-11-03 09:43:12,537 INFO[org.apache.spark.executor.Executor:54] - Running task 102.0 in stage 0.0 (TID 102)
2017-11-03 09:43:12,537 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 100.0 in stage 0.0 (TID 100) in 453 ms on localhost (executor driver) (101/175)
2017-11-03 09:43:12,537 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170820:0+689054
2017-11-03 09:43:12,584 INFO[org.apache.spark.executor.Executor:54] - Finished task 101.0 in stage 0.0 (TID 101). 1245 bytes result sent to driver
2017-11-03 09:43:12,600 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 103.0 in stage 0.0 (TID 103, localhost, executor driver, partition 103, ANY, 4874 bytes)
2017-11-03 09:43:12,600 INFO[org.apache.spark.executor.Executor:54] - Running task 103.0 in stage 0.0 (TID 103)
2017-11-03 09:43:12,600 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 101.0 in stage 0.0 (TID 101) in 219 ms on localhost (executor driver) (102/175)
2017-11-03 09:43:12,600 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170821:0+3575149
2017-11-03 09:43:12,665 INFO[org.apache.spark.executor.Executor:54] - Finished task 102.0 in stage 0.0 (TID 102). 1202 bytes result sent to driver
2017-11-03 09:43:12,665 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 104.0 in stage 0.0 (TID 104, localhost, executor driver, partition 104, ANY, 4874 bytes)
2017-11-03 09:43:12,665 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 102.0 in stage 0.0 (TID 102) in 128 ms on localhost (executor driver) (103/175)
2017-11-03 09:43:12,665 INFO[org.apache.spark.executor.Executor:54] - Running task 104.0 in stage 0.0 (TID 104)
2017-11-03 09:43:12,665 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170822:0+3158794
2017-11-03 09:43:13,207 INFO[org.apache.spark.executor.Executor:54] - Finished task 104.0 in stage 0.0 (TID 104). 1245 bytes result sent to driver
2017-11-03 09:43:13,207 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 105.0 in stage 0.0 (TID 105, localhost, executor driver, partition 105, ANY, 4874 bytes)
2017-11-03 09:43:13,207 INFO[org.apache.spark.executor.Executor:54] - Running task 105.0 in stage 0.0 (TID 105)
2017-11-03 09:43:13,207 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 104.0 in stage 0.0 (TID 104) in 542 ms on localhost (executor driver) (104/175)
2017-11-03 09:43:13,207 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170823:0+3929645
2017-11-03 09:43:13,223 INFO[org.apache.spark.executor.Executor:54] - Finished task 103.0 in stage 0.0 (TID 103). 1245 bytes result sent to driver
2017-11-03 09:43:13,223 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 106.0 in stage 0.0 (TID 106, localhost, executor driver, partition 106, ANY, 4874 bytes)
2017-11-03 09:43:13,223 INFO[org.apache.spark.executor.Executor:54] - Running task 106.0 in stage 0.0 (TID 106)
2017-11-03 09:43:13,223 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 103.0 in stage 0.0 (TID 103) in 623 ms on localhost (executor driver) (105/175)
2017-11-03 09:43:13,223 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170824:0+4254889
2017-11-03 09:43:13,833 INFO[org.apache.spark.executor.Executor:54] - Finished task 105.0 in stage 0.0 (TID 105). 1245 bytes result sent to driver
2017-11-03 09:43:13,833 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 107.0 in stage 0.0 (TID 107, localhost, executor driver, partition 107, ANY, 4874 bytes)
2017-11-03 09:43:13,833 INFO[org.apache.spark.executor.Executor:54] - Running task 107.0 in stage 0.0 (TID 107)
2017-11-03 09:43:13,833 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 105.0 in stage 0.0 (TID 105) in 626 ms on localhost (executor driver) (106/175)
2017-11-03 09:43:13,833 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170825:0+2937467
2017-11-03 09:43:13,980 INFO[org.apache.spark.executor.Executor:54] - Finished task 106.0 in stage 0.0 (TID 106). 1245 bytes result sent to driver
2017-11-03 09:43:13,980 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 108.0 in stage 0.0 (TID 108, localhost, executor driver, partition 108, ANY, 4874 bytes)
2017-11-03 09:43:13,980 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 106.0 in stage 0.0 (TID 106) in 757 ms on localhost (executor driver) (107/175)
2017-11-03 09:43:13,980 INFO[org.apache.spark.executor.Executor:54] - Running task 108.0 in stage 0.0 (TID 108)
2017-11-03 09:43:13,980 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170826:0+683002
2017-11-03 09:43:14,150 INFO[org.apache.spark.executor.Executor:54] - Finished task 108.0 in stage 0.0 (TID 108). 1202 bytes result sent to driver
2017-11-03 09:43:14,150 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 109.0 in stage 0.0 (TID 109, localhost, executor driver, partition 109, ANY, 4874 bytes)
2017-11-03 09:43:14,150 INFO[org.apache.spark.executor.Executor:54] - Running task 109.0 in stage 0.0 (TID 109)
2017-11-03 09:43:14,150 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 108.0 in stage 0.0 (TID 108) in 170 ms on localhost (executor driver) (108/175)
2017-11-03 09:43:14,150 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170827:0+999700
2017-11-03 09:43:14,265 INFO[org.apache.spark.executor.Executor:54] - Finished task 107.0 in stage 0.0 (TID 107). 1245 bytes result sent to driver
2017-11-03 09:43:14,265 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 110.0 in stage 0.0 (TID 110, localhost, executor driver, partition 110, ANY, 4874 bytes)
2017-11-03 09:43:14,265 INFO[org.apache.spark.executor.Executor:54] - Running task 110.0 in stage 0.0 (TID 110)
2017-11-03 09:43:14,265 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 107.0 in stage 0.0 (TID 107) in 432 ms on localhost (executor driver) (109/175)
2017-11-03 09:43:14,265 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170828:0+1934680
2017-11-03 09:43:14,382 INFO[org.apache.spark.executor.Executor:54] - Finished task 109.0 in stage 0.0 (TID 109). 1245 bytes result sent to driver
2017-11-03 09:43:14,383 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 111.0 in stage 0.0 (TID 111, localhost, executor driver, partition 111, ANY, 4874 bytes)
2017-11-03 09:43:14,383 INFO[org.apache.spark.executor.Executor:54] - Running task 111.0 in stage 0.0 (TID 111)
2017-11-03 09:43:14,383 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 109.0 in stage 0.0 (TID 109) in 233 ms on localhost (executor driver) (110/175)
2017-11-03 09:43:14,385 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170829:0+2934899
2017-11-03 09:43:14,553 INFO[org.apache.spark.executor.Executor:54] - Finished task 110.0 in stage 0.0 (TID 110). 1245 bytes result sent to driver
2017-11-03 09:43:14,553 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 112.0 in stage 0.0 (TID 112, localhost, executor driver, partition 112, ANY, 4874 bytes)
2017-11-03 09:43:14,553 INFO[org.apache.spark.executor.Executor:54] - Running task 112.0 in stage 0.0 (TID 112)
2017-11-03 09:43:14,553 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 110.0 in stage 0.0 (TID 110) in 288 ms on localhost (executor driver) (111/175)
2017-11-03 09:43:14,553 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170830:0+2412984
2017-11-03 09:43:14,929 INFO[org.apache.spark.executor.Executor:54] - Finished task 111.0 in stage 0.0 (TID 111). 1288 bytes result sent to driver
2017-11-03 09:43:14,929 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 113.0 in stage 0.0 (TID 113, localhost, executor driver, partition 113, ANY, 4874 bytes)
2017-11-03 09:43:14,929 INFO[org.apache.spark.executor.Executor:54] - Running task 113.0 in stage 0.0 (TID 113)
2017-11-03 09:43:14,929 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 111.0 in stage 0.0 (TID 111) in 546 ms on localhost (executor driver) (112/175)
2017-11-03 09:43:14,929 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170831:0+1792725
2017-11-03 09:43:14,945 INFO[org.apache.spark.executor.Executor:54] - Finished task 112.0 in stage 0.0 (TID 112). 1245 bytes result sent to driver
2017-11-03 09:43:14,945 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 114.0 in stage 0.0 (TID 114, localhost, executor driver, partition 114, ANY, 4874 bytes)
2017-11-03 09:43:14,945 INFO[org.apache.spark.executor.Executor:54] - Running task 114.0 in stage 0.0 (TID 114)
2017-11-03 09:43:14,945 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 112.0 in stage 0.0 (TID 112) in 392 ms on localhost (executor driver) (113/175)
2017-11-03 09:43:14,945 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170901:0+1383896
2017-11-03 09:43:15,198 INFO[org.apache.spark.executor.Executor:54] - Finished task 114.0 in stage 0.0 (TID 114). 1245 bytes result sent to driver
2017-11-03 09:43:15,198 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 115.0 in stage 0.0 (TID 115, localhost, executor driver, partition 115, ANY, 4874 bytes)
2017-11-03 09:43:15,198 INFO[org.apache.spark.executor.Executor:54] - Running task 115.0 in stage 0.0 (TID 115)
2017-11-03 09:43:15,198 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 114.0 in stage 0.0 (TID 114) in 253 ms on localhost (executor driver) (114/175)
2017-11-03 09:43:15,198 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170902:0+829640
2017-11-03 09:43:15,229 INFO[org.apache.spark.executor.Executor:54] - Finished task 113.0 in stage 0.0 (TID 113). 1245 bytes result sent to driver
2017-11-03 09:43:15,229 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 116.0 in stage 0.0 (TID 116, localhost, executor driver, partition 116, ANY, 4874 bytes)
2017-11-03 09:43:15,229 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 113.0 in stage 0.0 (TID 113) in 300 ms on localhost (executor driver) (115/175)
2017-11-03 09:43:15,229 INFO[org.apache.spark.executor.Executor:54] - Running task 116.0 in stage 0.0 (TID 116)
2017-11-03 09:43:15,229 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170903:0+959349
2017-11-03 09:43:15,323 INFO[org.apache.spark.executor.Executor:54] - Finished task 115.0 in stage 0.0 (TID 115). 1202 bytes result sent to driver
2017-11-03 09:43:15,323 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 117.0 in stage 0.0 (TID 117, localhost, executor driver, partition 117, ANY, 4874 bytes)
2017-11-03 09:43:15,323 INFO[org.apache.spark.executor.Executor:54] - Running task 117.0 in stage 0.0 (TID 117)
2017-11-03 09:43:15,323 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 115.0 in stage 0.0 (TID 115) in 125 ms on localhost (executor driver) (116/175)
2017-11-03 09:43:15,323 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170904:0+1769399
2017-11-03 09:43:15,417 INFO[org.apache.spark.executor.Executor:54] - Finished task 116.0 in stage 0.0 (TID 116). 1245 bytes result sent to driver
2017-11-03 09:43:15,417 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 118.0 in stage 0.0 (TID 118, localhost, executor driver, partition 118, ANY, 4874 bytes)
2017-11-03 09:43:15,417 INFO[org.apache.spark.executor.Executor:54] - Running task 118.0 in stage 0.0 (TID 118)
2017-11-03 09:43:15,417 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170905:0+2059107
2017-11-03 09:43:15,417 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 116.0 in stage 0.0 (TID 116) in 188 ms on localhost (executor driver) (117/175)
2017-11-03 09:43:15,604 INFO[org.apache.spark.executor.Executor:54] - Finished task 117.0 in stage 0.0 (TID 117). 1245 bytes result sent to driver
2017-11-03 09:43:15,604 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 119.0 in stage 0.0 (TID 119, localhost, executor driver, partition 119, ANY, 4874 bytes)
2017-11-03 09:43:15,604 INFO[org.apache.spark.executor.Executor:54] - Running task 119.0 in stage 0.0 (TID 119)
2017-11-03 09:43:15,604 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 117.0 in stage 0.0 (TID 117) in 281 ms on localhost (executor driver) (118/175)
2017-11-03 09:43:15,604 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170906:0+2470789
2017-11-03 09:43:15,801 INFO[org.apache.spark.executor.Executor:54] - Finished task 118.0 in stage 0.0 (TID 118). 1245 bytes result sent to driver
2017-11-03 09:43:15,801 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 120.0 in stage 0.0 (TID 120, localhost, executor driver, partition 120, ANY, 4874 bytes)
2017-11-03 09:43:15,801 INFO[org.apache.spark.executor.Executor:54] - Running task 120.0 in stage 0.0 (TID 120)
2017-11-03 09:43:15,801 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 118.0 in stage 0.0 (TID 118) in 384 ms on localhost (executor driver) (119/175)
2017-11-03 09:43:15,801 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170907:0+1379264
2017-11-03 09:43:15,979 INFO[org.apache.spark.executor.Executor:54] - Finished task 119.0 in stage 0.0 (TID 119). 1245 bytes result sent to driver
2017-11-03 09:43:15,979 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 121.0 in stage 0.0 (TID 121, localhost, executor driver, partition 121, ANY, 4874 bytes)
2017-11-03 09:43:15,979 INFO[org.apache.spark.executor.Executor:54] - Running task 121.0 in stage 0.0 (TID 121)
2017-11-03 09:43:15,979 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 119.0 in stage 0.0 (TID 119) in 375 ms on localhost (executor driver) (120/175)
2017-11-03 09:43:15,979 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170908:0+948462
2017-11-03 09:43:16,057 INFO[org.apache.spark.executor.Executor:54] - Finished task 120.0 in stage 0.0 (TID 120). 1245 bytes result sent to driver
2017-11-03 09:43:16,057 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 122.0 in stage 0.0 (TID 122, localhost, executor driver, partition 122, ANY, 4874 bytes)
2017-11-03 09:43:16,057 INFO[org.apache.spark.executor.Executor:54] - Running task 122.0 in stage 0.0 (TID 122)
2017-11-03 09:43:16,057 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 120.0 in stage 0.0 (TID 120) in 256 ms on localhost (executor driver) (121/175)
2017-11-03 09:43:16,057 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170909:0+811776
2017-11-03 09:43:16,135 INFO[org.apache.spark.executor.Executor:54] - Finished task 121.0 in stage 0.0 (TID 121). 1245 bytes result sent to driver
2017-11-03 09:43:16,135 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 123.0 in stage 0.0 (TID 123, localhost, executor driver, partition 123, ANY, 4874 bytes)
2017-11-03 09:43:16,135 INFO[org.apache.spark.executor.Executor:54] - Running task 123.0 in stage 0.0 (TID 123)
2017-11-03 09:43:16,135 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 121.0 in stage 0.0 (TID 121) in 156 ms on localhost (executor driver) (122/175)
2017-11-03 09:43:16,135 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170910:0+651273
2017-11-03 09:43:16,199 INFO[org.apache.spark.executor.Executor:54] - Finished task 122.0 in stage 0.0 (TID 122). 1202 bytes result sent to driver
2017-11-03 09:43:16,199 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 124.0 in stage 0.0 (TID 124, localhost, executor driver, partition 124, ANY, 4874 bytes)
2017-11-03 09:43:16,199 INFO[org.apache.spark.executor.Executor:54] - Running task 124.0 in stage 0.0 (TID 124)
2017-11-03 09:43:16,199 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 122.0 in stage 0.0 (TID 122) in 142 ms on localhost (executor driver) (123/175)
2017-11-03 09:43:16,199 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170911:0+1083540
2017-11-03 09:43:16,261 INFO[org.apache.spark.executor.Executor:54] - Finished task 123.0 in stage 0.0 (TID 123). 1202 bytes result sent to driver
2017-11-03 09:43:16,261 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 125.0 in stage 0.0 (TID 125, localhost, executor driver, partition 125, ANY, 4874 bytes)
2017-11-03 09:43:16,261 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 123.0 in stage 0.0 (TID 123) in 126 ms on localhost (executor driver) (124/175)
2017-11-03 09:43:16,261 INFO[org.apache.spark.executor.Executor:54] - Running task 125.0 in stage 0.0 (TID 125)
2017-11-03 09:43:16,261 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170912:0+962622
2017-11-03 09:43:16,386 INFO[org.apache.spark.executor.Executor:54] - Finished task 124.0 in stage 0.0 (TID 124). 1245 bytes result sent to driver
2017-11-03 09:43:16,386 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 126.0 in stage 0.0 (TID 126, localhost, executor driver, partition 126, ANY, 4874 bytes)
2017-11-03 09:43:16,386 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 124.0 in stage 0.0 (TID 124) in 187 ms on localhost (executor driver) (125/175)
2017-11-03 09:43:16,386 INFO[org.apache.spark.executor.Executor:54] - Running task 126.0 in stage 0.0 (TID 126)
2017-11-03 09:43:16,386 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170913:0+790039
2017-11-03 09:43:16,433 INFO[org.apache.spark.executor.Executor:54] - Finished task 125.0 in stage 0.0 (TID 125). 1245 bytes result sent to driver
2017-11-03 09:43:16,433 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 127.0 in stage 0.0 (TID 127, localhost, executor driver, partition 127, ANY, 4874 bytes)
2017-11-03 09:43:16,433 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 125.0 in stage 0.0 (TID 125) in 172 ms on localhost (executor driver) (126/175)
2017-11-03 09:43:16,433 INFO[org.apache.spark.executor.Executor:54] - Running task 127.0 in stage 0.0 (TID 127)
2017-11-03 09:43:16,433 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170914:0+1589250
2017-11-03 09:43:16,511 INFO[org.apache.spark.executor.Executor:54] - Finished task 126.0 in stage 0.0 (TID 126). 1202 bytes result sent to driver
2017-11-03 09:43:16,527 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 128.0 in stage 0.0 (TID 128, localhost, executor driver, partition 128, ANY, 4874 bytes)
2017-11-03 09:43:16,527 INFO[org.apache.spark.executor.Executor:54] - Running task 128.0 in stage 0.0 (TID 128)
2017-11-03 09:43:16,527 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 126.0 in stage 0.0 (TID 126) in 141 ms on localhost (executor driver) (127/175)
2017-11-03 09:43:16,527 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170915:0+810356
2017-11-03 09:43:16,687 INFO[org.apache.spark.executor.Executor:54] - Finished task 127.0 in stage 0.0 (TID 127). 1202 bytes result sent to driver
2017-11-03 09:43:16,687 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 129.0 in stage 0.0 (TID 129, localhost, executor driver, partition 129, ANY, 4874 bytes)
2017-11-03 09:43:16,687 INFO[org.apache.spark.executor.Executor:54] - Running task 129.0 in stage 0.0 (TID 129)
2017-11-03 09:43:16,687 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 127.0 in stage 0.0 (TID 127) in 254 ms on localhost (executor driver) (128/175)
2017-11-03 09:43:16,687 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170916:0+1175322
2017-11-03 09:43:16,687 INFO[org.apache.spark.executor.Executor:54] - Finished task 128.0 in stage 0.0 (TID 128). 1202 bytes result sent to driver
2017-11-03 09:43:16,687 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 130.0 in stage 0.0 (TID 130, localhost, executor driver, partition 130, ANY, 4874 bytes)
2017-11-03 09:43:16,687 INFO[org.apache.spark.executor.Executor:54] - Running task 130.0 in stage 0.0 (TID 130)
2017-11-03 09:43:16,687 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 128.0 in stage 0.0 (TID 128) in 160 ms on localhost (executor driver) (129/175)
2017-11-03 09:43:16,703 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170917:0+753909
2017-11-03 09:43:16,863 INFO[org.apache.spark.executor.Executor:54] - Finished task 130.0 in stage 0.0 (TID 130). 1288 bytes result sent to driver
2017-11-03 09:43:16,863 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 131.0 in stage 0.0 (TID 131, localhost, executor driver, partition 131, ANY, 4874 bytes)
2017-11-03 09:43:16,863 INFO[org.apache.spark.executor.Executor:54] - Running task 131.0 in stage 0.0 (TID 131)
2017-11-03 09:43:16,863 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 130.0 in stage 0.0 (TID 130) in 176 ms on localhost (executor driver) (130/175)
2017-11-03 09:43:16,863 INFO[org.apache.spark.executor.Executor:54] - Finished task 129.0 in stage 0.0 (TID 129). 1245 bytes result sent to driver
2017-11-03 09:43:16,863 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170918:0+1101935
2017-11-03 09:43:16,863 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 132.0 in stage 0.0 (TID 132, localhost, executor driver, partition 132, ANY, 4874 bytes)
2017-11-03 09:43:16,863 INFO[org.apache.spark.executor.Executor:54] - Running task 132.0 in stage 0.0 (TID 132)
2017-11-03 09:43:16,863 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 129.0 in stage 0.0 (TID 129) in 176 ms on localhost (executor driver) (131/175)
2017-11-03 09:43:16,863 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170919:0+1517541
2017-11-03 09:43:17,113 INFO[org.apache.spark.executor.Executor:54] - Finished task 131.0 in stage 0.0 (TID 131). 1245 bytes result sent to driver
2017-11-03 09:43:17,113 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 133.0 in stage 0.0 (TID 133, localhost, executor driver, partition 133, ANY, 4874 bytes)
2017-11-03 09:43:17,113 INFO[org.apache.spark.executor.Executor:54] - Running task 133.0 in stage 0.0 (TID 133)
2017-11-03 09:43:17,113 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 131.0 in stage 0.0 (TID 131) in 250 ms on localhost (executor driver) (132/175)
2017-11-03 09:43:17,113 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170920:0+1467862
2017-11-03 09:43:17,129 INFO[org.apache.spark.executor.Executor:54] - Finished task 132.0 in stage 0.0 (TID 132). 1245 bytes result sent to driver
2017-11-03 09:43:17,129 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 134.0 in stage 0.0 (TID 134, localhost, executor driver, partition 134, ANY, 4874 bytes)
2017-11-03 09:43:17,129 INFO[org.apache.spark.executor.Executor:54] - Running task 134.0 in stage 0.0 (TID 134)
2017-11-03 09:43:17,129 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 132.0 in stage 0.0 (TID 132) in 266 ms on localhost (executor driver) (133/175)
2017-11-03 09:43:17,129 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170921:0+1484716
2017-11-03 09:43:17,349 INFO[org.apache.spark.executor.Executor:54] - Finished task 133.0 in stage 0.0 (TID 133). 1202 bytes result sent to driver
2017-11-03 09:43:17,349 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 135.0 in stage 0.0 (TID 135, localhost, executor driver, partition 135, ANY, 4874 bytes)
2017-11-03 09:43:17,349 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 133.0 in stage 0.0 (TID 133) in 236 ms on localhost (executor driver) (134/175)
2017-11-03 09:43:17,349 INFO[org.apache.spark.executor.Executor:54] - Running task 135.0 in stage 0.0 (TID 135)
2017-11-03 09:43:17,349 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170922:0+1292720
2017-11-03 09:43:17,396 INFO[org.apache.spark.executor.Executor:54] - Finished task 134.0 in stage 0.0 (TID 134). 1202 bytes result sent to driver
2017-11-03 09:43:17,396 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 136.0 in stage 0.0 (TID 136, localhost, executor driver, partition 136, ANY, 4874 bytes)
2017-11-03 09:43:17,396 INFO[org.apache.spark.executor.Executor:54] - Running task 136.0 in stage 0.0 (TID 136)
2017-11-03 09:43:17,396 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 134.0 in stage 0.0 (TID 134) in 267 ms on localhost (executor driver) (135/175)
2017-11-03 09:43:17,396 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170923:0+780020
2017-11-03 09:43:17,537 INFO[org.apache.spark.executor.Executor:54] - Finished task 136.0 in stage 0.0 (TID 136). 1245 bytes result sent to driver
2017-11-03 09:43:17,537 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 137.0 in stage 0.0 (TID 137, localhost, executor driver, partition 137, ANY, 4874 bytes)
2017-11-03 09:43:17,537 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 136.0 in stage 0.0 (TID 136) in 141 ms on localhost (executor driver) (136/175)
2017-11-03 09:43:17,537 INFO[org.apache.spark.executor.Executor:54] - Running task 137.0 in stage 0.0 (TID 137)
2017-11-03 09:43:17,537 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170924:0+941549
2017-11-03 09:43:17,568 INFO[org.apache.spark.executor.Executor:54] - Finished task 135.0 in stage 0.0 (TID 135). 1245 bytes result sent to driver
2017-11-03 09:43:17,568 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 138.0 in stage 0.0 (TID 138, localhost, executor driver, partition 138, ANY, 4874 bytes)
2017-11-03 09:43:17,568 INFO[org.apache.spark.executor.Executor:54] - Running task 138.0 in stage 0.0 (TID 138)
2017-11-03 09:43:17,568 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 135.0 in stage 0.0 (TID 135) in 219 ms on localhost (executor driver) (137/175)
2017-11-03 09:43:17,568 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170925:0+1302954
2017-11-03 09:43:17,694 INFO[org.apache.spark.executor.Executor:54] - Finished task 137.0 in stage 0.0 (TID 137). 1202 bytes result sent to driver
2017-11-03 09:43:17,695 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 139.0 in stage 0.0 (TID 139, localhost, executor driver, partition 139, ANY, 4874 bytes)
2017-11-03 09:43:17,695 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 137.0 in stage 0.0 (TID 137) in 158 ms on localhost (executor driver) (138/175)
2017-11-03 09:43:17,695 INFO[org.apache.spark.executor.Executor:54] - Running task 139.0 in stage 0.0 (TID 139)
2017-11-03 09:43:17,696 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170926:0+1686651
2017-11-03 09:43:17,786 INFO[org.apache.spark.executor.Executor:54] - Finished task 138.0 in stage 0.0 (TID 138). 1245 bytes result sent to driver
2017-11-03 09:43:17,786 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 140.0 in stage 0.0 (TID 140, localhost, executor driver, partition 140, ANY, 4874 bytes)
2017-11-03 09:43:17,786 INFO[org.apache.spark.executor.Executor:54] - Running task 140.0 in stage 0.0 (TID 140)
2017-11-03 09:43:17,786 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 138.0 in stage 0.0 (TID 138) in 218 ms on localhost (executor driver) (139/175)
2017-11-03 09:43:17,786 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170927:0+1374062
2017-11-03 09:43:17,975 INFO[org.apache.spark.executor.Executor:54] - Finished task 139.0 in stage 0.0 (TID 139). 1245 bytes result sent to driver
2017-11-03 09:43:17,975 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 141.0 in stage 0.0 (TID 141, localhost, executor driver, partition 141, ANY, 4874 bytes)
2017-11-03 09:43:17,975 INFO[org.apache.spark.executor.Executor:54] - Running task 141.0 in stage 0.0 (TID 141)
2017-11-03 09:43:17,975 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 139.0 in stage 0.0 (TID 139) in 281 ms on localhost (executor driver) (140/175)
2017-11-03 09:43:17,975 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170928:0+1153463
2017-11-03 09:43:18,022 INFO[org.apache.spark.executor.Executor:54] - Finished task 140.0 in stage 0.0 (TID 140). 1202 bytes result sent to driver
2017-11-03 09:43:18,022 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 142.0 in stage 0.0 (TID 142, localhost, executor driver, partition 142, ANY, 4874 bytes)
2017-11-03 09:43:18,038 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 140.0 in stage 0.0 (TID 140) in 236 ms on localhost (executor driver) (141/175)
2017-11-03 09:43:18,038 INFO[org.apache.spark.executor.Executor:54] - Running task 142.0 in stage 0.0 (TID 142)
2017-11-03 09:43:18,038 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170929:0+1128437
2017-11-03 09:43:18,163 INFO[org.apache.spark.executor.Executor:54] - Finished task 141.0 in stage 0.0 (TID 141). 1245 bytes result sent to driver
2017-11-03 09:43:18,163 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 143.0 in stage 0.0 (TID 143, localhost, executor driver, partition 143, ANY, 4874 bytes)
2017-11-03 09:43:18,163 INFO[org.apache.spark.executor.Executor:54] - Running task 143.0 in stage 0.0 (TID 143)
2017-11-03 09:43:18,163 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 141.0 in stage 0.0 (TID 141) in 188 ms on localhost (executor driver) (142/175)
2017-11-03 09:43:18,163 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20170930:0+1566092
2017-11-03 09:43:18,227 INFO[org.apache.spark.executor.Executor:54] - Finished task 142.0 in stage 0.0 (TID 142). 1245 bytes result sent to driver
2017-11-03 09:43:18,227 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 144.0 in stage 0.0 (TID 144, localhost, executor driver, partition 144, ANY, 4874 bytes)
2017-11-03 09:43:18,227 INFO[org.apache.spark.executor.Executor:54] - Running task 144.0 in stage 0.0 (TID 144)
2017-11-03 09:43:18,227 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 142.0 in stage 0.0 (TID 142) in 205 ms on localhost (executor driver) (143/175)
2017-11-03 09:43:18,227 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171001:0+1618558
2017-11-03 09:43:18,446 INFO[org.apache.spark.executor.Executor:54] - Finished task 143.0 in stage 0.0 (TID 143). 1245 bytes result sent to driver
2017-11-03 09:43:18,446 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 145.0 in stage 0.0 (TID 145, localhost, executor driver, partition 145, ANY, 4874 bytes)
2017-11-03 09:43:18,446 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 143.0 in stage 0.0 (TID 143) in 283 ms on localhost (executor driver) (144/175)
2017-11-03 09:43:18,446 INFO[org.apache.spark.executor.Executor:54] - Running task 145.0 in stage 0.0 (TID 145)
2017-11-03 09:43:18,446 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171002:0+630584
2017-11-03 09:43:18,492 INFO[org.apache.spark.executor.Executor:54] - Finished task 144.0 in stage 0.0 (TID 144). 1245 bytes result sent to driver
2017-11-03 09:43:18,492 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 146.0 in stage 0.0 (TID 146, localhost, executor driver, partition 146, ANY, 4874 bytes)
2017-11-03 09:43:18,508 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 144.0 in stage 0.0 (TID 144) in 281 ms on localhost (executor driver) (145/175)
2017-11-03 09:43:18,508 INFO[org.apache.spark.executor.Executor:54] - Running task 146.0 in stage 0.0 (TID 146)
2017-11-03 09:43:18,508 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171003:0+1158551
2017-11-03 09:43:18,555 INFO[org.apache.spark.executor.Executor:54] - Finished task 145.0 in stage 0.0 (TID 145). 1202 bytes result sent to driver
2017-11-03 09:43:18,555 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 147.0 in stage 0.0 (TID 147, localhost, executor driver, partition 147, ANY, 4874 bytes)
2017-11-03 09:43:18,555 INFO[org.apache.spark.executor.Executor:54] - Running task 147.0 in stage 0.0 (TID 147)
2017-11-03 09:43:18,555 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 145.0 in stage 0.0 (TID 145) in 109 ms on localhost (executor driver) (146/175)
2017-11-03 09:43:18,555 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171004:0+1137551
2017-11-03 09:43:18,709 INFO[org.apache.spark.executor.Executor:54] - Finished task 146.0 in stage 0.0 (TID 146). 1202 bytes result sent to driver
2017-11-03 09:43:18,709 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 148.0 in stage 0.0 (TID 148, localhost, executor driver, partition 148, ANY, 4874 bytes)
2017-11-03 09:43:18,710 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 146.0 in stage 0.0 (TID 146) in 218 ms on localhost (executor driver) (147/175)
2017-11-03 09:43:18,710 INFO[org.apache.spark.executor.Executor:54] - Running task 148.0 in stage 0.0 (TID 148)
2017-11-03 09:43:18,711 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171005:0+1239006
2017-11-03 09:43:18,741 INFO[org.apache.spark.executor.Executor:54] - Finished task 147.0 in stage 0.0 (TID 147). 1202 bytes result sent to driver
2017-11-03 09:43:18,741 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 149.0 in stage 0.0 (TID 149, localhost, executor driver, partition 149, ANY, 4874 bytes)
2017-11-03 09:43:18,741 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 147.0 in stage 0.0 (TID 147) in 186 ms on localhost (executor driver) (148/175)
2017-11-03 09:43:18,741 INFO[org.apache.spark.executor.Executor:54] - Running task 149.0 in stage 0.0 (TID 149)
2017-11-03 09:43:18,756 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171006:0+775466
2017-11-03 09:43:18,892 INFO[org.apache.spark.executor.Executor:54] - Finished task 148.0 in stage 0.0 (TID 148). 1245 bytes result sent to driver
2017-11-03 09:43:18,892 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 150.0 in stage 0.0 (TID 150, localhost, executor driver, partition 150, ANY, 4874 bytes)
2017-11-03 09:43:18,892 INFO[org.apache.spark.executor.Executor:54] - Running task 150.0 in stage 0.0 (TID 150)
2017-11-03 09:43:18,892 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 148.0 in stage 0.0 (TID 148) in 183 ms on localhost (executor driver) (149/175)
2017-11-03 09:43:18,892 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171007:0+956990
2017-11-03 09:43:18,907 INFO[org.apache.spark.executor.Executor:54] - Finished task 149.0 in stage 0.0 (TID 149). 1331 bytes result sent to driver
2017-11-03 09:43:18,907 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 151.0 in stage 0.0 (TID 151, localhost, executor driver, partition 151, ANY, 4874 bytes)
2017-11-03 09:43:18,907 INFO[org.apache.spark.executor.Executor:54] - Running task 151.0 in stage 0.0 (TID 151)
2017-11-03 09:43:18,907 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 149.0 in stage 0.0 (TID 149) in 166 ms on localhost (executor driver) (150/175)
2017-11-03 09:43:18,907 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171008:0+1272351
2017-11-03 09:43:19,056 INFO[org.apache.spark.executor.Executor:54] - Finished task 150.0 in stage 0.0 (TID 150). 1202 bytes result sent to driver
2017-11-03 09:43:19,056 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 152.0 in stage 0.0 (TID 152, localhost, executor driver, partition 152, ANY, 4874 bytes)
2017-11-03 09:43:19,056 INFO[org.apache.spark.executor.Executor:54] - Running task 152.0 in stage 0.0 (TID 152)
2017-11-03 09:43:19,056 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 150.0 in stage 0.0 (TID 150) in 164 ms on localhost (executor driver) (151/175)
2017-11-03 09:43:19,071 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171009:0+1543548
2017-11-03 09:43:19,118 INFO[org.apache.spark.executor.Executor:54] - Finished task 151.0 in stage 0.0 (TID 151). 1245 bytes result sent to driver
2017-11-03 09:43:19,118 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 153.0 in stage 0.0 (TID 153, localhost, executor driver, partition 153, ANY, 4874 bytes)
2017-11-03 09:43:19,118 INFO[org.apache.spark.executor.Executor:54] - Running task 153.0 in stage 0.0 (TID 153)
2017-11-03 09:43:19,118 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 151.0 in stage 0.0 (TID 151) in 211 ms on localhost (executor driver) (152/175)
2017-11-03 09:43:19,118 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171010:0+2667348
2017-11-03 09:43:19,323 INFO[org.apache.spark.executor.Executor:54] - Finished task 152.0 in stage 0.0 (TID 152). 1331 bytes result sent to driver
2017-11-03 09:43:19,323 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 154.0 in stage 0.0 (TID 154, localhost, executor driver, partition 154, ANY, 4874 bytes)
2017-11-03 09:43:19,338 INFO[org.apache.spark.executor.Executor:54] - Running task 154.0 in stage 0.0 (TID 154)
2017-11-03 09:43:19,338 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171011:0+2591333
2017-11-03 09:43:19,338 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 152.0 in stage 0.0 (TID 152) in 282 ms on localhost (executor driver) (153/175)
2017-11-03 09:43:19,557 INFO[org.apache.spark.executor.Executor:54] - Finished task 153.0 in stage 0.0 (TID 153). 1245 bytes result sent to driver
2017-11-03 09:43:19,573 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 155.0 in stage 0.0 (TID 155, localhost, executor driver, partition 155, ANY, 4874 bytes)
2017-11-03 09:43:19,573 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 153.0 in stage 0.0 (TID 153) in 455 ms on localhost (executor driver) (154/175)
2017-11-03 09:43:19,573 INFO[org.apache.spark.executor.Executor:54] - Running task 155.0 in stage 0.0 (TID 155)
2017-11-03 09:43:19,573 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171012:0+2232499
2017-11-03 09:43:19,770 INFO[org.apache.spark.executor.Executor:54] - Finished task 154.0 in stage 0.0 (TID 154). 1245 bytes result sent to driver
2017-11-03 09:43:19,770 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 156.0 in stage 0.0 (TID 156, localhost, executor driver, partition 156, ANY, 4874 bytes)
2017-11-03 09:43:19,770 INFO[org.apache.spark.executor.Executor:54] - Running task 156.0 in stage 0.0 (TID 156)
2017-11-03 09:43:19,770 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 154.0 in stage 0.0 (TID 154) in 447 ms on localhost (executor driver) (155/175)
2017-11-03 09:43:19,770 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171013:0+1803419
2017-11-03 09:43:19,968 INFO[org.apache.spark.executor.Executor:54] - Finished task 155.0 in stage 0.0 (TID 155). 1245 bytes result sent to driver
2017-11-03 09:43:19,969 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 157.0 in stage 0.0 (TID 157, localhost, executor driver, partition 157, ANY, 4874 bytes)
2017-11-03 09:43:19,969 INFO[org.apache.spark.executor.Executor:54] - Running task 157.0 in stage 0.0 (TID 157)
2017-11-03 09:43:19,971 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171014:0+1539061
2017-11-03 09:43:19,971 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 155.0 in stage 0.0 (TID 155) in 414 ms on localhost (executor driver) (156/175)
2017-11-03 09:43:20,066 INFO[org.apache.spark.executor.Executor:54] - Finished task 156.0 in stage 0.0 (TID 156). 1245 bytes result sent to driver
2017-11-03 09:43:20,066 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 158.0 in stage 0.0 (TID 158, localhost, executor driver, partition 158, ANY, 4874 bytes)
2017-11-03 09:43:20,066 INFO[org.apache.spark.executor.Executor:54] - Running task 158.0 in stage 0.0 (TID 158)
2017-11-03 09:43:20,066 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 156.0 in stage 0.0 (TID 156) in 296 ms on localhost (executor driver) (157/175)
2017-11-03 09:43:20,066 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171015:0+1658562
2017-11-03 09:43:20,244 INFO[org.apache.spark.executor.Executor:54] - Finished task 157.0 in stage 0.0 (TID 157). 1288 bytes result sent to driver
2017-11-03 09:43:20,244 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 159.0 in stage 0.0 (TID 159, localhost, executor driver, partition 159, ANY, 4874 bytes)
2017-11-03 09:43:20,244 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 157.0 in stage 0.0 (TID 157) in 275 ms on localhost (executor driver) (158/175)
2017-11-03 09:43:20,244 INFO[org.apache.spark.executor.Executor:54] - Running task 159.0 in stage 0.0 (TID 159)
2017-11-03 09:43:20,244 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171016:0+1941068
2017-11-03 09:43:20,337 INFO[org.apache.spark.executor.Executor:54] - Finished task 158.0 in stage 0.0 (TID 158). 1245 bytes result sent to driver
2017-11-03 09:43:20,337 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 160.0 in stage 0.0 (TID 160, localhost, executor driver, partition 160, ANY, 4874 bytes)
2017-11-03 09:43:20,337 INFO[org.apache.spark.executor.Executor:54] - Running task 160.0 in stage 0.0 (TID 160)
2017-11-03 09:43:20,337 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 158.0 in stage 0.0 (TID 158) in 271 ms on localhost (executor driver) (159/175)
2017-11-03 09:43:20,337 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171017:0+2610307
2017-11-03 09:43:20,587 INFO[org.apache.spark.executor.Executor:54] - Finished task 159.0 in stage 0.0 (TID 159). 1245 bytes result sent to driver
2017-11-03 09:43:20,587 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 161.0 in stage 0.0 (TID 161, localhost, executor driver, partition 161, ANY, 4874 bytes)
2017-11-03 09:43:20,587 INFO[org.apache.spark.executor.Executor:54] - Running task 161.0 in stage 0.0 (TID 161)
2017-11-03 09:43:20,587 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 159.0 in stage 0.0 (TID 159) in 343 ms on localhost (executor driver) (160/175)
2017-11-03 09:43:20,587 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171018:0+1965814
2017-11-03 09:43:20,786 INFO[org.apache.spark.executor.Executor:54] - Finished task 160.0 in stage 0.0 (TID 160). 1245 bytes result sent to driver
2017-11-03 09:43:20,786 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 162.0 in stage 0.0 (TID 162, localhost, executor driver, partition 162, ANY, 4874 bytes)
2017-11-03 09:43:20,786 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 160.0 in stage 0.0 (TID 160) in 449 ms on localhost (executor driver) (161/175)
2017-11-03 09:43:20,786 INFO[org.apache.spark.executor.Executor:54] - Running task 162.0 in stage 0.0 (TID 162)
2017-11-03 09:43:20,786 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171019:0+2732504
2017-11-03 09:43:20,924 INFO[org.apache.spark.executor.Executor:54] - Finished task 161.0 in stage 0.0 (TID 161). 1245 bytes result sent to driver
2017-11-03 09:43:20,924 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 163.0 in stage 0.0 (TID 163, localhost, executor driver, partition 163, ANY, 4874 bytes)
2017-11-03 09:43:20,924 INFO[org.apache.spark.executor.Executor:54] - Running task 163.0 in stage 0.0 (TID 163)
2017-11-03 09:43:20,924 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 161.0 in stage 0.0 (TID 161) in 337 ms on localhost (executor driver) (162/175)
2017-11-03 09:43:20,924 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171020:0+1971654
2017-11-03 09:43:21,237 INFO[org.apache.spark.executor.Executor:54] - Finished task 162.0 in stage 0.0 (TID 162). 1245 bytes result sent to driver
2017-11-03 09:43:21,237 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 164.0 in stage 0.0 (TID 164, localhost, executor driver, partition 164, ANY, 4874 bytes)
2017-11-03 09:43:21,237 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 162.0 in stage 0.0 (TID 162) in 451 ms on localhost (executor driver) (163/175)
2017-11-03 09:43:21,237 INFO[org.apache.spark.executor.Executor:54] - Running task 164.0 in stage 0.0 (TID 164)
2017-11-03 09:43:21,237 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171021:0+1695183
2017-11-03 09:43:21,284 INFO[org.apache.spark.executor.Executor:54] - Finished task 163.0 in stage 0.0 (TID 163). 1245 bytes result sent to driver
2017-11-03 09:43:21,284 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 165.0 in stage 0.0 (TID 165, localhost, executor driver, partition 165, ANY, 4874 bytes)
2017-11-03 09:43:21,284 INFO[org.apache.spark.executor.Executor:54] - Running task 165.0 in stage 0.0 (TID 165)
2017-11-03 09:43:21,284 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 163.0 in stage 0.0 (TID 163) in 360 ms on localhost (executor driver) (164/175)
2017-11-03 09:43:21,300 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171022:0+1217918
2017-11-03 09:43:21,503 INFO[org.apache.spark.executor.Executor:54] - Finished task 164.0 in stage 0.0 (TID 164). 1202 bytes result sent to driver
2017-11-03 09:43:21,503 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 166.0 in stage 0.0 (TID 166, localhost, executor driver, partition 166, ANY, 4874 bytes)
2017-11-03 09:43:21,503 INFO[org.apache.spark.executor.Executor:54] - Running task 166.0 in stage 0.0 (TID 166)
2017-11-03 09:43:21,503 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 164.0 in stage 0.0 (TID 164) in 266 ms on localhost (executor driver) (165/175)
2017-11-03 09:43:21,518 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171023:0+1583380
2017-11-03 09:43:21,518 INFO[org.apache.spark.executor.Executor:54] - Finished task 165.0 in stage 0.0 (TID 165). 1288 bytes result sent to driver
2017-11-03 09:43:21,518 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 167.0 in stage 0.0 (TID 167, localhost, executor driver, partition 167, ANY, 4874 bytes)
2017-11-03 09:43:21,518 INFO[org.apache.spark.executor.Executor:54] - Running task 167.0 in stage 0.0 (TID 167)
2017-11-03 09:43:21,518 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 165.0 in stage 0.0 (TID 165) in 234 ms on localhost (executor driver) (166/175)
2017-11-03 09:43:21,518 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171024:0+2476640
2017-11-03 09:43:21,770 INFO[org.apache.spark.executor.Executor:54] - Finished task 166.0 in stage 0.0 (TID 166). 1331 bytes result sent to driver
2017-11-03 09:43:21,770 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 168.0 in stage 0.0 (TID 168, localhost, executor driver, partition 168, ANY, 4874 bytes)
2017-11-03 09:43:21,770 INFO[org.apache.spark.executor.Executor:54] - Running task 168.0 in stage 0.0 (TID 168)
2017-11-03 09:43:21,770 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 166.0 in stage 0.0 (TID 166) in 267 ms on localhost (executor driver) (167/175)
2017-11-03 09:43:21,785 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171025:0+2439237
2017-11-03 09:43:21,957 INFO[org.apache.spark.executor.Executor:54] - Finished task 167.0 in stage 0.0 (TID 167). 1245 bytes result sent to driver
2017-11-03 09:43:21,957 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 169.0 in stage 0.0 (TID 169, localhost, executor driver, partition 169, ANY, 4874 bytes)
2017-11-03 09:43:21,957 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 167.0 in stage 0.0 (TID 167) in 439 ms on localhost (executor driver) (168/175)
2017-11-03 09:43:21,957 INFO[org.apache.spark.executor.Executor:54] - Running task 169.0 in stage 0.0 (TID 169)
2017-11-03 09:43:21,957 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171026:0+2602463
2017-11-03 09:43:22,174 INFO[org.apache.spark.executor.Executor:54] - Finished task 168.0 in stage 0.0 (TID 168). 1331 bytes result sent to driver
2017-11-03 09:43:22,174 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 170.0 in stage 0.0 (TID 170, localhost, executor driver, partition 170, ANY, 4874 bytes)
2017-11-03 09:43:22,174 INFO[org.apache.spark.executor.Executor:54] - Running task 170.0 in stage 0.0 (TID 170)
2017-11-03 09:43:22,174 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 168.0 in stage 0.0 (TID 168) in 404 ms on localhost (executor driver) (169/175)
2017-11-03 09:43:22,174 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171027:0+2301440
2017-11-03 09:43:22,396 INFO[org.apache.spark.executor.Executor:54] - Finished task 169.0 in stage 0.0 (TID 169). 1245 bytes result sent to driver
2017-11-03 09:43:22,411 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 171.0 in stage 0.0 (TID 171, localhost, executor driver, partition 171, ANY, 4874 bytes)
2017-11-03 09:43:22,411 INFO[org.apache.spark.executor.Executor:54] - Running task 171.0 in stage 0.0 (TID 171)
2017-11-03 09:43:22,411 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 169.0 in stage 0.0 (TID 169) in 454 ms on localhost (executor driver) (170/175)
2017-11-03 09:43:22,411 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171028:0+2005800
2017-11-03 09:43:22,583 INFO[org.apache.spark.executor.Executor:54] - Finished task 170.0 in stage 0.0 (TID 170). 1245 bytes result sent to driver
2017-11-03 09:43:22,583 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 172.0 in stage 0.0 (TID 172, localhost, executor driver, partition 172, ANY, 4874 bytes)
2017-11-03 09:43:22,583 INFO[org.apache.spark.executor.Executor:54] - Running task 172.0 in stage 0.0 (TID 172)
2017-11-03 09:43:22,583 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 170.0 in stage 0.0 (TID 170) in 409 ms on localhost (executor driver) (171/175)
2017-11-03 09:43:22,583 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171029:0+1000998
2017-11-03 09:43:22,756 INFO[org.apache.spark.executor.Executor:54] - Finished task 171.0 in stage 0.0 (TID 171). 1245 bytes result sent to driver
2017-11-03 09:43:22,756 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 173.0 in stage 0.0 (TID 173, localhost, executor driver, partition 173, ANY, 4874 bytes)
2017-11-03 09:43:22,757 INFO[org.apache.spark.executor.Executor:54] - Running task 173.0 in stage 0.0 (TID 173)
2017-11-03 09:43:22,757 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 171.0 in stage 0.0 (TID 171) in 346 ms on localhost (executor driver) (172/175)
2017-11-03 09:43:22,758 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171030:0+1954571
2017-11-03 09:43:22,772 INFO[org.apache.spark.executor.Executor:54] - Finished task 172.0 in stage 0.0 (TID 172). 1245 bytes result sent to driver
2017-11-03 09:43:22,772 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 174.0 in stage 0.0 (TID 174, localhost, executor driver, partition 174, ANY, 4874 bytes)
2017-11-03 09:43:22,772 INFO[org.apache.spark.executor.Executor:54] - Running task 174.0 in stage 0.0 (TID 174)
2017-11-03 09:43:22,772 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 172.0 in stage 0.0 (TID 172) in 189 ms on localhost (executor driver) (173/175)
2017-11-03 09:43:22,772 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171031:0+2524948
2017-11-03 09:43:23,066 INFO[org.apache.spark.executor.Executor:54] - Finished task 173.0 in stage 0.0 (TID 173). 1288 bytes result sent to driver
2017-11-03 09:43:23,082 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 173.0 in stage 0.0 (TID 173) in 326 ms on localhost (executor driver) (174/175)
2017-11-03 09:43:23,176 INFO[org.apache.spark.executor.Executor:54] - Finished task 174.0 in stage 0.0 (TID 174). 1245 bytes result sent to driver
2017-11-03 09:43:23,176 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 174.0 in stage 0.0 (TID 174) in 404 ms on localhost (executor driver) (175/175)
2017-11-03 09:43:23,191 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-11-03 09:43:23,191 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 0 (mapToPair at SparkNginxLog.java:33) finished in 27.190 s
2017-11-03 09:43:23,191 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2017-11-03 09:43:23,191 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2017-11-03 09:43:23,191 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 1)
2017-11-03 09:43:23,191 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2017-11-03 09:43:23,191 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[5] at sortByKey at SparkNginxLog.java:47), which has no missing parents
2017-11-03 09:43:23,191 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 4.0 KB, free 631.5 MB)
2017-11-03 09:43:23,207 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.3 KB, free 631.5 MB)
2017-11-03 09:43:23,207 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:57421 (size: 2.3 KB, free: 631.8 MB)
2017-11-03 09:43:23,207 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2017-11-03 09:43:23,207 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 175 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at sortByKey at SparkNginxLog.java:47) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2017-11-03 09:43:23,207 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 175 tasks
2017-11-03 09:43:23,207 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 175, localhost, executor driver, partition 0, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,207 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 176, localhost, executor driver, partition 1, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,207 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 175)
2017-11-03 09:43:23,207 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 1.0 (TID 176)
2017-11-03 09:43:23,223 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,223 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,223 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 16 ms
2017-11-03 09:43:23,223 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 16 ms
2017-11-03 09:43:23,223 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 1.0 (TID 176). 1251 bytes result sent to driver
2017-11-03 09:43:23,223 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 175). 1251 bytes result sent to driver
2017-11-03 09:43:23,223 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 1.0 (TID 177, localhost, executor driver, partition 2, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,223 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 1.0 (TID 178, localhost, executor driver, partition 3, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,223 INFO[org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 1.0 (TID 177)
2017-11-03 09:43:23,223 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 175) in 16 ms on localhost (executor driver) (1/175)
2017-11-03 09:43:23,223 INFO[org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 1.0 (TID 178)
2017-11-03 09:43:23,239 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,239 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,240 INFO[org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 1.0 (TID 177). 1208 bytes result sent to driver
2017-11-03 09:43:23,240 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 176) in 33 ms on localhost (executor driver) (2/175)
2017-11-03 09:43:23,241 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,241 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,241 INFO[org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 1.0 (TID 178). 1251 bytes result sent to driver
2017-11-03 09:43:23,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 1.0 (TID 179, localhost, executor driver, partition 4, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 1.0 (TID 180, localhost, executor driver, partition 5, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 1.0 (TID 177) in 18 ms on localhost (executor driver) (3/175)
2017-11-03 09:43:23,241 INFO[org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 1.0 (TID 179)
2017-11-03 09:43:23,241 INFO[org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 1.0 (TID 180)
2017-11-03 09:43:23,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 1.0 (TID 178) in 18 ms on localhost (executor driver) (4/175)
2017-11-03 09:43:23,241 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,241 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,241 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,241 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,241 INFO[org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 1.0 (TID 179). 1165 bytes result sent to driver
2017-11-03 09:43:23,241 INFO[org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 1.0 (TID 180). 1165 bytes result sent to driver
2017-11-03 09:43:23,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 1.0 (TID 181, localhost, executor driver, partition 6, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,241 INFO[org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 1.0 (TID 181)
2017-11-03 09:43:23,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 1.0 (TID 182, localhost, executor driver, partition 7, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 1.0 (TID 179) in 0 ms on localhost (executor driver) (5/175)
2017-11-03 09:43:23,241 INFO[org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 1.0 (TID 182)
2017-11-03 09:43:23,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 1.0 (TID 180) in 0 ms on localhost (executor driver) (6/175)
2017-11-03 09:43:23,241 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,241 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,241 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,241 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,241 INFO[org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 1.0 (TID 181). 1165 bytes result sent to driver
2017-11-03 09:43:23,241 INFO[org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 1.0 (TID 182). 1165 bytes result sent to driver
2017-11-03 09:43:23,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 1.0 (TID 183, localhost, executor driver, partition 8, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 1.0 (TID 184, localhost, executor driver, partition 9, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 1.0 (TID 181) in 0 ms on localhost (executor driver) (7/175)
2017-11-03 09:43:23,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 1.0 (TID 182) in 0 ms on localhost (executor driver) (8/175)
2017-11-03 09:43:23,241 INFO[org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 1.0 (TID 183)
2017-11-03 09:43:23,241 INFO[org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 1.0 (TID 184)
2017-11-03 09:43:23,241 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,241 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,241 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,241 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,241 INFO[org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 1.0 (TID 183). 1165 bytes result sent to driver
2017-11-03 09:43:23,241 INFO[org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 1.0 (TID 184). 1165 bytes result sent to driver
2017-11-03 09:43:23,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 1.0 (TID 185, localhost, executor driver, partition 10, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,241 INFO[org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 1.0 (TID 185)
2017-11-03 09:43:23,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 1.0 (TID 186, localhost, executor driver, partition 11, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,257 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,257 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,257 INFO[org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 1.0 (TID 185). 1251 bytes result sent to driver
2017-11-03 09:43:23,257 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 1.0 (TID 187, localhost, executor driver, partition 12, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,257 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 1.0 (TID 183) in 16 ms on localhost (executor driver) (9/175)
2017-11-03 09:43:23,257 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 1.0 (TID 185) in 16 ms on localhost (executor driver) (10/175)
2017-11-03 09:43:23,257 INFO[org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 1.0 (TID 186)
2017-11-03 09:43:23,257 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 1.0 (TID 184) in 16 ms on localhost (executor driver) (11/175)
2017-11-03 09:43:23,257 INFO[org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 1.0 (TID 187)
2017-11-03 09:43:23,257 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,257 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,257 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,257 INFO[org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 1.0 (TID 186). 1165 bytes result sent to driver
2017-11-03 09:43:23,257 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,257 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 1.0 (TID 188, localhost, executor driver, partition 13, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,257 INFO[org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 1.0 (TID 187). 1165 bytes result sent to driver
2017-11-03 09:43:23,257 INFO[org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 1.0 (TID 188)
2017-11-03 09:43:23,257 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 1.0 (TID 189, localhost, executor driver, partition 14, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,257 INFO[org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 1.0 (TID 189)
2017-11-03 09:43:23,257 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 1.0 (TID 186) in 16 ms on localhost (executor driver) (12/175)
2017-11-03 09:43:23,257 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,257 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,257 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 1.0 (TID 187) in 0 ms on localhost (executor driver) (13/175)
2017-11-03 09:43:23,257 INFO[org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 1.0 (TID 188). 1165 bytes result sent to driver
2017-11-03 09:43:23,257 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,257 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,257 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 1.0 (TID 190, localhost, executor driver, partition 15, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,257 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 1.0 (TID 188) in 0 ms on localhost (executor driver) (14/175)
2017-11-03 09:43:23,257 INFO[org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 1.0 (TID 189). 1165 bytes result sent to driver
2017-11-03 09:43:23,257 INFO[org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 1.0 (TID 190)
2017-11-03 09:43:23,257 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 16.0 in stage 1.0 (TID 191, localhost, executor driver, partition 16, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,257 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 1.0 (TID 189) in 0 ms on localhost (executor driver) (15/175)
2017-11-03 09:43:23,257 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,257 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,270 INFO[org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 1.0 (TID 190). 1251 bytes result sent to driver
2017-11-03 09:43:23,272 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 17.0 in stage 1.0 (TID 192, localhost, executor driver, partition 17, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,273 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 1.0 (TID 190) in 15 ms on localhost (executor driver) (16/175)
2017-11-03 09:43:23,274 INFO[org.apache.spark.executor.Executor:54] - Running task 17.0 in stage 1.0 (TID 192)
2017-11-03 09:43:23,277 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,277 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,257 INFO[org.apache.spark.executor.Executor:54] - Running task 16.0 in stage 1.0 (TID 191)
2017-11-03 09:43:23,280 INFO[org.apache.spark.executor.Executor:54] - Finished task 17.0 in stage 1.0 (TID 192). 1251 bytes result sent to driver
2017-11-03 09:43:23,280 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 18.0 in stage 1.0 (TID 193, localhost, executor driver, partition 18, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,280 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 17.0 in stage 1.0 (TID 192) in 9 ms on localhost (executor driver) (17/175)
2017-11-03 09:43:23,280 INFO[org.apache.spark.executor.Executor:54] - Running task 18.0 in stage 1.0 (TID 193)
2017-11-03 09:43:23,280 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,280 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,280 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,280 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,280 INFO[org.apache.spark.executor.Executor:54] - Finished task 16.0 in stage 1.0 (TID 191). 1251 bytes result sent to driver
2017-11-03 09:43:23,280 INFO[org.apache.spark.executor.Executor:54] - Finished task 18.0 in stage 1.0 (TID 193). 1165 bytes result sent to driver
2017-11-03 09:43:23,280 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 19.0 in stage 1.0 (TID 194, localhost, executor driver, partition 19, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,280 INFO[org.apache.spark.executor.Executor:54] - Running task 19.0 in stage 1.0 (TID 194)
2017-11-03 09:43:23,280 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 20.0 in stage 1.0 (TID 195, localhost, executor driver, partition 20, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,280 INFO[org.apache.spark.executor.Executor:54] - Running task 20.0 in stage 1.0 (TID 195)
2017-11-03 09:43:23,280 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 18.0 in stage 1.0 (TID 193) in 0 ms on localhost (executor driver) (18/175)
2017-11-03 09:43:23,280 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 16.0 in stage 1.0 (TID 191) in 23 ms on localhost (executor driver) (19/175)
2017-11-03 09:43:23,280 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,280 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,280 INFO[org.apache.spark.executor.Executor:54] - Finished task 19.0 in stage 1.0 (TID 194). 1165 bytes result sent to driver
2017-11-03 09:43:23,280 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,280 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,280 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 21.0 in stage 1.0 (TID 196, localhost, executor driver, partition 21, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,280 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 19.0 in stage 1.0 (TID 194) in 0 ms on localhost (executor driver) (20/175)
2017-11-03 09:43:23,280 INFO[org.apache.spark.executor.Executor:54] - Running task 21.0 in stage 1.0 (TID 196)
2017-11-03 09:43:23,280 INFO[org.apache.spark.executor.Executor:54] - Finished task 20.0 in stage 1.0 (TID 195). 1165 bytes result sent to driver
2017-11-03 09:43:23,280 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 22.0 in stage 1.0 (TID 197, localhost, executor driver, partition 22, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,280 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 20.0 in stage 1.0 (TID 195) in 0 ms on localhost (executor driver) (21/175)
2017-11-03 09:43:23,280 INFO[org.apache.spark.executor.Executor:54] - Running task 22.0 in stage 1.0 (TID 197)
2017-11-03 09:43:23,280 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,280 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,280 INFO[org.apache.spark.executor.Executor:54] - Finished task 22.0 in stage 1.0 (TID 197). 1165 bytes result sent to driver
2017-11-03 09:43:23,280 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,280 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,295 INFO[org.apache.spark.executor.Executor:54] - Finished task 21.0 in stage 1.0 (TID 196). 1251 bytes result sent to driver
2017-11-03 09:43:23,295 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 23.0 in stage 1.0 (TID 198, localhost, executor driver, partition 23, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,295 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 24.0 in stage 1.0 (TID 199, localhost, executor driver, partition 24, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,295 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 22.0 in stage 1.0 (TID 197) in 15 ms on localhost (executor driver) (22/175)
2017-11-03 09:43:23,295 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 21.0 in stage 1.0 (TID 196) in 15 ms on localhost (executor driver) (23/175)
2017-11-03 09:43:23,295 INFO[org.apache.spark.executor.Executor:54] - Running task 23.0 in stage 1.0 (TID 198)
2017-11-03 09:43:23,295 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,295 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,295 INFO[org.apache.spark.executor.Executor:54] - Running task 24.0 in stage 1.0 (TID 199)
2017-11-03 09:43:23,295 INFO[org.apache.spark.executor.Executor:54] - Finished task 23.0 in stage 1.0 (TID 198). 1165 bytes result sent to driver
2017-11-03 09:43:23,295 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,295 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,295 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 25.0 in stage 1.0 (TID 200, localhost, executor driver, partition 25, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,295 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 23.0 in stage 1.0 (TID 198) in 0 ms on localhost (executor driver) (24/175)
2017-11-03 09:43:23,295 INFO[org.apache.spark.executor.Executor:54] - Finished task 24.0 in stage 1.0 (TID 199). 1165 bytes result sent to driver
2017-11-03 09:43:23,295 INFO[org.apache.spark.executor.Executor:54] - Running task 25.0 in stage 1.0 (TID 200)
2017-11-03 09:43:23,311 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 26.0 in stage 1.0 (TID 201, localhost, executor driver, partition 26, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,311 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,311 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,311 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 24.0 in stage 1.0 (TID 199) in 16 ms on localhost (executor driver) (25/175)
2017-11-03 09:43:23,311 INFO[org.apache.spark.executor.Executor:54] - Running task 26.0 in stage 1.0 (TID 201)
2017-11-03 09:43:23,311 INFO[org.apache.spark.executor.Executor:54] - Finished task 25.0 in stage 1.0 (TID 200). 1251 bytes result sent to driver
2017-11-03 09:43:23,311 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,311 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,311 INFO[org.apache.spark.executor.Executor:54] - Finished task 26.0 in stage 1.0 (TID 201). 1165 bytes result sent to driver
2017-11-03 09:43:23,311 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 27.0 in stage 1.0 (TID 202, localhost, executor driver, partition 27, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,311 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 28.0 in stage 1.0 (TID 203, localhost, executor driver, partition 28, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,311 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 25.0 in stage 1.0 (TID 200) in 16 ms on localhost (executor driver) (26/175)
2017-11-03 09:43:23,311 INFO[org.apache.spark.executor.Executor:54] - Running task 27.0 in stage 1.0 (TID 202)
2017-11-03 09:43:23,311 INFO[org.apache.spark.executor.Executor:54] - Running task 28.0 in stage 1.0 (TID 203)
2017-11-03 09:43:23,311 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,311 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,311 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 26.0 in stage 1.0 (TID 201) in 0 ms on localhost (executor driver) (27/175)
2017-11-03 09:43:23,327 INFO[org.apache.spark.executor.Executor:54] - Finished task 28.0 in stage 1.0 (TID 203). 1251 bytes result sent to driver
2017-11-03 09:43:23,327 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 29.0 in stage 1.0 (TID 204, localhost, executor driver, partition 29, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,327 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,327 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,327 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 28.0 in stage 1.0 (TID 203) in 16 ms on localhost (executor driver) (28/175)
2017-11-03 09:43:23,327 INFO[org.apache.spark.executor.Executor:54] - Running task 29.0 in stage 1.0 (TID 204)
2017-11-03 09:43:23,327 INFO[org.apache.spark.executor.Executor:54] - Finished task 27.0 in stage 1.0 (TID 202). 1208 bytes result sent to driver
2017-11-03 09:43:23,327 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,327 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,327 INFO[org.apache.spark.executor.Executor:54] - Finished task 29.0 in stage 1.0 (TID 204). 1165 bytes result sent to driver
2017-11-03 09:43:23,327 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 30.0 in stage 1.0 (TID 205, localhost, executor driver, partition 30, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,327 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 31.0 in stage 1.0 (TID 206, localhost, executor driver, partition 31, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,327 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 27.0 in stage 1.0 (TID 202) in 16 ms on localhost (executor driver) (29/175)
2017-11-03 09:43:23,327 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 29.0 in stage 1.0 (TID 204) in 0 ms on localhost (executor driver) (30/175)
2017-11-03 09:43:23,327 INFO[org.apache.spark.executor.Executor:54] - Running task 30.0 in stage 1.0 (TID 205)
2017-11-03 09:43:23,327 INFO[org.apache.spark.executor.Executor:54] - Running task 31.0 in stage 1.0 (TID 206)
2017-11-03 09:43:23,327 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,327 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,342 INFO[org.apache.spark.executor.Executor:54] - Finished task 30.0 in stage 1.0 (TID 205). 1251 bytes result sent to driver
2017-11-03 09:43:23,342 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,342 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,342 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 32.0 in stage 1.0 (TID 207, localhost, executor driver, partition 32, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,342 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 30.0 in stage 1.0 (TID 205) in 15 ms on localhost (executor driver) (31/175)
2017-11-03 09:43:23,342 INFO[org.apache.spark.executor.Executor:54] - Finished task 31.0 in stage 1.0 (TID 206). 1208 bytes result sent to driver
2017-11-03 09:43:23,342 INFO[org.apache.spark.executor.Executor:54] - Running task 32.0 in stage 1.0 (TID 207)
2017-11-03 09:43:23,342 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 33.0 in stage 1.0 (TID 208, localhost, executor driver, partition 33, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,342 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 31.0 in stage 1.0 (TID 206) in 15 ms on localhost (executor driver) (32/175)
2017-11-03 09:43:23,342 INFO[org.apache.spark.executor.Executor:54] - Running task 33.0 in stage 1.0 (TID 208)
2017-11-03 09:43:23,342 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,342 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,342 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,342 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,342 INFO[org.apache.spark.executor.Executor:54] - Finished task 33.0 in stage 1.0 (TID 208). 1165 bytes result sent to driver
2017-11-03 09:43:23,342 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 34.0 in stage 1.0 (TID 209, localhost, executor driver, partition 34, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,358 INFO[org.apache.spark.executor.Executor:54] - Running task 34.0 in stage 1.0 (TID 209)
2017-11-03 09:43:23,358 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 33.0 in stage 1.0 (TID 208) in 16 ms on localhost (executor driver) (33/175)
2017-11-03 09:43:23,358 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,358 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,358 INFO[org.apache.spark.executor.Executor:54] - Finished task 34.0 in stage 1.0 (TID 209). 1251 bytes result sent to driver
2017-11-03 09:43:23,358 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 35.0 in stage 1.0 (TID 210, localhost, executor driver, partition 35, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,358 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 34.0 in stage 1.0 (TID 209) in 16 ms on localhost (executor driver) (34/175)
2017-11-03 09:43:23,358 INFO[org.apache.spark.executor.Executor:54] - Running task 35.0 in stage 1.0 (TID 210)
2017-11-03 09:43:23,358 INFO[org.apache.spark.executor.Executor:54] - Finished task 32.0 in stage 1.0 (TID 207). 1208 bytes result sent to driver
2017-11-03 09:43:23,358 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,358 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,358 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 36.0 in stage 1.0 (TID 211, localhost, executor driver, partition 36, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,358 INFO[org.apache.spark.executor.Executor:54] - Running task 36.0 in stage 1.0 (TID 211)
2017-11-03 09:43:23,358 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 32.0 in stage 1.0 (TID 207) in 16 ms on localhost (executor driver) (35/175)
2017-11-03 09:43:23,358 INFO[org.apache.spark.executor.Executor:54] - Finished task 35.0 in stage 1.0 (TID 210). 1165 bytes result sent to driver
2017-11-03 09:43:23,358 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 37.0 in stage 1.0 (TID 212, localhost, executor driver, partition 37, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,358 INFO[org.apache.spark.executor.Executor:54] - Running task 37.0 in stage 1.0 (TID 212)
2017-11-03 09:43:23,358 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 35.0 in stage 1.0 (TID 210) in 0 ms on localhost (executor driver) (36/175)
2017-11-03 09:43:23,358 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,358 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,358 INFO[org.apache.spark.executor.Executor:54] - Finished task 36.0 in stage 1.0 (TID 211). 1165 bytes result sent to driver
2017-11-03 09:43:23,358 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 38.0 in stage 1.0 (TID 213, localhost, executor driver, partition 38, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,358 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,358 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,358 INFO[org.apache.spark.executor.Executor:54] - Running task 38.0 in stage 1.0 (TID 213)
2017-11-03 09:43:23,358 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 36.0 in stage 1.0 (TID 211) in 0 ms on localhost (executor driver) (37/175)
2017-11-03 09:43:23,358 INFO[org.apache.spark.executor.Executor:54] - Finished task 37.0 in stage 1.0 (TID 212). 1165 bytes result sent to driver
2017-11-03 09:43:23,358 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,358 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,358 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 39.0 in stage 1.0 (TID 214, localhost, executor driver, partition 39, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,373 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 37.0 in stage 1.0 (TID 212) in 15 ms on localhost (executor driver) (38/175)
2017-11-03 09:43:23,373 INFO[org.apache.spark.executor.Executor:54] - Running task 39.0 in stage 1.0 (TID 214)
2017-11-03 09:43:23,373 INFO[org.apache.spark.executor.Executor:54] - Finished task 38.0 in stage 1.0 (TID 213). 1251 bytes result sent to driver
2017-11-03 09:43:23,373 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 40.0 in stage 1.0 (TID 215, localhost, executor driver, partition 40, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,373 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 38.0 in stage 1.0 (TID 213) in 15 ms on localhost (executor driver) (39/175)
2017-11-03 09:43:23,373 INFO[org.apache.spark.executor.Executor:54] - Running task 40.0 in stage 1.0 (TID 215)
2017-11-03 09:43:23,373 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,373 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,373 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,373 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,373 INFO[org.apache.spark.executor.Executor:54] - Finished task 39.0 in stage 1.0 (TID 214). 1165 bytes result sent to driver
2017-11-03 09:43:23,373 INFO[org.apache.spark.executor.Executor:54] - Finished task 40.0 in stage 1.0 (TID 215). 1165 bytes result sent to driver
2017-11-03 09:43:23,373 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 41.0 in stage 1.0 (TID 216, localhost, executor driver, partition 41, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,373 INFO[org.apache.spark.executor.Executor:54] - Running task 41.0 in stage 1.0 (TID 216)
2017-11-03 09:43:23,373 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 42.0 in stage 1.0 (TID 217, localhost, executor driver, partition 42, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,373 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 39.0 in stage 1.0 (TID 214) in 15 ms on localhost (executor driver) (40/175)
2017-11-03 09:43:23,373 INFO[org.apache.spark.executor.Executor:54] - Running task 42.0 in stage 1.0 (TID 217)
2017-11-03 09:43:23,373 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,373 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,373 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 40.0 in stage 1.0 (TID 215) in 0 ms on localhost (executor driver) (41/175)
2017-11-03 09:43:23,373 INFO[org.apache.spark.executor.Executor:54] - Finished task 41.0 in stage 1.0 (TID 216). 1165 bytes result sent to driver
2017-11-03 09:43:23,373 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,373 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 43.0 in stage 1.0 (TID 218, localhost, executor driver, partition 43, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,373 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,373 INFO[org.apache.spark.executor.Executor:54] - Running task 43.0 in stage 1.0 (TID 218)
2017-11-03 09:43:23,373 INFO[org.apache.spark.executor.Executor:54] - Finished task 42.0 in stage 1.0 (TID 217). 1165 bytes result sent to driver
2017-11-03 09:43:23,373 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 41.0 in stage 1.0 (TID 216) in 0 ms on localhost (executor driver) (42/175)
2017-11-03 09:43:23,373 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 44.0 in stage 1.0 (TID 219, localhost, executor driver, partition 44, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,373 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 42.0 in stage 1.0 (TID 217) in 0 ms on localhost (executor driver) (43/175)
2017-11-03 09:43:23,373 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,373 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,373 INFO[org.apache.spark.executor.Executor:54] - Running task 44.0 in stage 1.0 (TID 219)
2017-11-03 09:43:23,373 INFO[org.apache.spark.executor.Executor:54] - Finished task 43.0 in stage 1.0 (TID 218). 1165 bytes result sent to driver
2017-11-03 09:43:23,373 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,373 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,373 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 45.0 in stage 1.0 (TID 220, localhost, executor driver, partition 45, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,373 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 43.0 in stage 1.0 (TID 218) in 0 ms on localhost (executor driver) (44/175)
2017-11-03 09:43:23,373 INFO[org.apache.spark.executor.Executor:54] - Running task 45.0 in stage 1.0 (TID 220)
2017-11-03 09:43:23,373 INFO[org.apache.spark.executor.Executor:54] - Finished task 44.0 in stage 1.0 (TID 219). 1165 bytes result sent to driver
2017-11-03 09:43:23,373 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 46.0 in stage 1.0 (TID 221, localhost, executor driver, partition 46, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,389 INFO[org.apache.spark.executor.Executor:54] - Running task 46.0 in stage 1.0 (TID 221)
2017-11-03 09:43:23,389 INFO[org.apache.spark.executor.Executor:54] - Finished task 45.0 in stage 1.0 (TID 220). 1251 bytes result sent to driver
2017-11-03 09:43:23,389 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 44.0 in stage 1.0 (TID 219) in 16 ms on localhost (executor driver) (45/175)
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,389 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 47.0 in stage 1.0 (TID 222, localhost, executor driver, partition 47, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,389 INFO[org.apache.spark.executor.Executor:54] - Finished task 46.0 in stage 1.0 (TID 221). 1165 bytes result sent to driver
2017-11-03 09:43:23,389 INFO[org.apache.spark.executor.Executor:54] - Running task 47.0 in stage 1.0 (TID 222)
2017-11-03 09:43:23,389 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 45.0 in stage 1.0 (TID 220) in 16 ms on localhost (executor driver) (46/175)
2017-11-03 09:43:23,389 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 48.0 in stage 1.0 (TID 223, localhost, executor driver, partition 48, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,389 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 46.0 in stage 1.0 (TID 221) in 16 ms on localhost (executor driver) (47/175)
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,389 INFO[org.apache.spark.executor.Executor:54] - Running task 48.0 in stage 1.0 (TID 223)
2017-11-03 09:43:23,389 INFO[org.apache.spark.executor.Executor:54] - Finished task 47.0 in stage 1.0 (TID 222). 1165 bytes result sent to driver
2017-11-03 09:43:23,389 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 49.0 in stage 1.0 (TID 224, localhost, executor driver, partition 49, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,389 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 47.0 in stage 1.0 (TID 222) in 0 ms on localhost (executor driver) (48/175)
2017-11-03 09:43:23,389 INFO[org.apache.spark.executor.Executor:54] - Running task 49.0 in stage 1.0 (TID 224)
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,389 INFO[org.apache.spark.executor.Executor:54] - Finished task 48.0 in stage 1.0 (TID 223). 1165 bytes result sent to driver
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,389 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 50.0 in stage 1.0 (TID 225, localhost, executor driver, partition 50, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,389 INFO[org.apache.spark.executor.Executor:54] - Finished task 49.0 in stage 1.0 (TID 224). 1165 bytes result sent to driver
2017-11-03 09:43:23,389 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 48.0 in stage 1.0 (TID 223) in 0 ms on localhost (executor driver) (49/175)
2017-11-03 09:43:23,389 INFO[org.apache.spark.executor.Executor:54] - Running task 50.0 in stage 1.0 (TID 225)
2017-11-03 09:43:23,389 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 51.0 in stage 1.0 (TID 226, localhost, executor driver, partition 51, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,389 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 49.0 in stage 1.0 (TID 224) in 0 ms on localhost (executor driver) (50/175)
2017-11-03 09:43:23,389 INFO[org.apache.spark.executor.Executor:54] - Running task 51.0 in stage 1.0 (TID 226)
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,389 INFO[org.apache.spark.executor.Executor:54] - Finished task 50.0 in stage 1.0 (TID 225). 1165 bytes result sent to driver
2017-11-03 09:43:23,389 INFO[org.apache.spark.executor.Executor:54] - Finished task 51.0 in stage 1.0 (TID 226). 1165 bytes result sent to driver
2017-11-03 09:43:23,389 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 53.0 in stage 1.0 (TID 227, localhost, executor driver, partition 53, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,389 INFO[org.apache.spark.executor.Executor:54] - Running task 53.0 in stage 1.0 (TID 227)
2017-11-03 09:43:23,389 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 50.0 in stage 1.0 (TID 225) in 0 ms on localhost (executor driver) (51/175)
2017-11-03 09:43:23,389 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 54.0 in stage 1.0 (TID 228, localhost, executor driver, partition 54, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,389 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,405 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 51.0 in stage 1.0 (TID 226) in 16 ms on localhost (executor driver) (52/175)
2017-11-03 09:43:23,405 INFO[org.apache.spark.executor.Executor:54] - Running task 54.0 in stage 1.0 (TID 228)
2017-11-03 09:43:23,405 INFO[org.apache.spark.executor.Executor:54] - Finished task 53.0 in stage 1.0 (TID 227). 1251 bytes result sent to driver
2017-11-03 09:43:23,405 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 55.0 in stage 1.0 (TID 229, localhost, executor driver, partition 55, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,405 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,405 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,405 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 53.0 in stage 1.0 (TID 227) in 16 ms on localhost (executor driver) (53/175)
2017-11-03 09:43:23,405 INFO[org.apache.spark.executor.Executor:54] - Finished task 54.0 in stage 1.0 (TID 228). 1165 bytes result sent to driver
2017-11-03 09:43:23,405 INFO[org.apache.spark.executor.Executor:54] - Running task 55.0 in stage 1.0 (TID 229)
2017-11-03 09:43:23,405 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 56.0 in stage 1.0 (TID 230, localhost, executor driver, partition 56, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,405 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 54.0 in stage 1.0 (TID 228) in 16 ms on localhost (executor driver) (54/175)
2017-11-03 09:43:23,405 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,405 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,405 INFO[org.apache.spark.executor.Executor:54] - Running task 56.0 in stage 1.0 (TID 230)
2017-11-03 09:43:23,405 INFO[org.apache.spark.executor.Executor:54] - Finished task 55.0 in stage 1.0 (TID 229). 1165 bytes result sent to driver
2017-11-03 09:43:23,405 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 57.0 in stage 1.0 (TID 231, localhost, executor driver, partition 57, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,405 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,405 INFO[org.apache.spark.executor.Executor:54] - Running task 57.0 in stage 1.0 (TID 231)
2017-11-03 09:43:23,405 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 55.0 in stage 1.0 (TID 229) in 0 ms on localhost (executor driver) (55/175)
2017-11-03 09:43:23,405 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,405 INFO[org.apache.spark.executor.Executor:54] - Finished task 56.0 in stage 1.0 (TID 230). 1165 bytes result sent to driver
2017-11-03 09:43:23,405 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,405 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,405 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 58.0 in stage 1.0 (TID 232, localhost, executor driver, partition 58, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,405 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 56.0 in stage 1.0 (TID 230) in 0 ms on localhost (executor driver) (56/175)
2017-11-03 09:43:23,405 INFO[org.apache.spark.executor.Executor:54] - Running task 58.0 in stage 1.0 (TID 232)
2017-11-03 09:43:23,405 INFO[org.apache.spark.executor.Executor:54] - Finished task 57.0 in stage 1.0 (TID 231). 1165 bytes result sent to driver
2017-11-03 09:43:23,405 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 59.0 in stage 1.0 (TID 233, localhost, executor driver, partition 59, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,405 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 57.0 in stage 1.0 (TID 231) in 0 ms on localhost (executor driver) (57/175)
2017-11-03 09:43:23,405 INFO[org.apache.spark.executor.Executor:54] - Running task 59.0 in stage 1.0 (TID 233)
2017-11-03 09:43:23,405 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,405 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,405 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,405 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,405 INFO[org.apache.spark.executor.Executor:54] - Finished task 58.0 in stage 1.0 (TID 232). 1165 bytes result sent to driver
2017-11-03 09:43:23,405 INFO[org.apache.spark.executor.Executor:54] - Finished task 59.0 in stage 1.0 (TID 233). 1165 bytes result sent to driver
2017-11-03 09:43:23,405 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 60.0 in stage 1.0 (TID 234, localhost, executor driver, partition 60, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,405 INFO[org.apache.spark.executor.Executor:54] - Running task 60.0 in stage 1.0 (TID 234)
2017-11-03 09:43:23,405 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 61.0 in stage 1.0 (TID 235, localhost, executor driver, partition 61, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,405 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 59.0 in stage 1.0 (TID 233) in 0 ms on localhost (executor driver) (58/175)
2017-11-03 09:43:23,405 INFO[org.apache.spark.executor.Executor:54] - Running task 61.0 in stage 1.0 (TID 235)
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 58.0 in stage 1.0 (TID 232) in 15 ms on localhost (executor driver) (59/175)
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Finished task 60.0 in stage 1.0 (TID 234). 1251 bytes result sent to driver
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 62.0 in stage 1.0 (TID 236, localhost, executor driver, partition 62, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Finished task 61.0 in stage 1.0 (TID 235). 1251 bytes result sent to driver
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Running task 62.0 in stage 1.0 (TID 236)
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 60.0 in stage 1.0 (TID 234) in 15 ms on localhost (executor driver) (60/175)
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 63.0 in stage 1.0 (TID 237, localhost, executor driver, partition 63, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Running task 63.0 in stage 1.0 (TID 237)
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 61.0 in stage 1.0 (TID 235) in 15 ms on localhost (executor driver) (61/175)
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Finished task 62.0 in stage 1.0 (TID 236). 1165 bytes result sent to driver
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 64.0 in stage 1.0 (TID 238, localhost, executor driver, partition 64, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Finished task 63.0 in stage 1.0 (TID 237). 1165 bytes result sent to driver
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 62.0 in stage 1.0 (TID 236) in 0 ms on localhost (executor driver) (62/175)
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Running task 64.0 in stage 1.0 (TID 238)
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 65.0 in stage 1.0 (TID 239, localhost, executor driver, partition 65, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Running task 65.0 in stage 1.0 (TID 239)
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 63.0 in stage 1.0 (TID 237) in 0 ms on localhost (executor driver) (63/175)
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Finished task 64.0 in stage 1.0 (TID 238). 1165 bytes result sent to driver
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Finished task 65.0 in stage 1.0 (TID 239). 1165 bytes result sent to driver
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 66.0 in stage 1.0 (TID 240, localhost, executor driver, partition 66, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 64.0 in stage 1.0 (TID 238) in 0 ms on localhost (executor driver) (64/175)
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Running task 66.0 in stage 1.0 (TID 240)
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 67.0 in stage 1.0 (TID 241, localhost, executor driver, partition 67, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 65.0 in stage 1.0 (TID 239) in 0 ms on localhost (executor driver) (65/175)
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Running task 67.0 in stage 1.0 (TID 241)
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Finished task 66.0 in stage 1.0 (TID 240). 1165 bytes result sent to driver
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 68.0 in stage 1.0 (TID 242, localhost, executor driver, partition 68, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 66.0 in stage 1.0 (TID 240) in 0 ms on localhost (executor driver) (66/175)
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Running task 68.0 in stage 1.0 (TID 242)
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Finished task 67.0 in stage 1.0 (TID 241). 1165 bytes result sent to driver
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,420 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 69.0 in stage 1.0 (TID 243, localhost, executor driver, partition 69, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Finished task 68.0 in stage 1.0 (TID 242). 1165 bytes result sent to driver
2017-11-03 09:43:23,420 INFO[org.apache.spark.executor.Executor:54] - Running task 69.0 in stage 1.0 (TID 243)
2017-11-03 09:43:23,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 67.0 in stage 1.0 (TID 241) in 0 ms on localhost (executor driver) (67/175)
2017-11-03 09:43:23,436 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 70.0 in stage 1.0 (TID 244, localhost, executor driver, partition 70, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,436 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 68.0 in stage 1.0 (TID 242) in 16 ms on localhost (executor driver) (68/175)
2017-11-03 09:43:23,436 INFO[org.apache.spark.executor.Executor:54] - Running task 70.0 in stage 1.0 (TID 244)
2017-11-03 09:43:23,436 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,436 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,436 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,436 INFO[org.apache.spark.executor.Executor:54] - Finished task 69.0 in stage 1.0 (TID 243). 1251 bytes result sent to driver
2017-11-03 09:43:23,436 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,436 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 71.0 in stage 1.0 (TID 245, localhost, executor driver, partition 71, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,436 INFO[org.apache.spark.executor.Executor:54] - Running task 71.0 in stage 1.0 (TID 245)
2017-11-03 09:43:23,436 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 69.0 in stage 1.0 (TID 243) in 16 ms on localhost (executor driver) (69/175)
2017-11-03 09:43:23,436 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,436 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,436 INFO[org.apache.spark.executor.Executor:54] - Finished task 71.0 in stage 1.0 (TID 245). 1165 bytes result sent to driver
2017-11-03 09:43:23,436 INFO[org.apache.spark.executor.Executor:54] - Finished task 70.0 in stage 1.0 (TID 244). 1165 bytes result sent to driver
2017-11-03 09:43:23,436 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 72.0 in stage 1.0 (TID 246, localhost, executor driver, partition 72, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,436 INFO[org.apache.spark.executor.Executor:54] - Running task 72.0 in stage 1.0 (TID 246)
2017-11-03 09:43:23,436 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 73.0 in stage 1.0 (TID 247, localhost, executor driver, partition 73, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,436 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,436 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,436 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 70.0 in stage 1.0 (TID 244) in 0 ms on localhost (executor driver) (70/175)
2017-11-03 09:43:23,436 INFO[org.apache.spark.executor.Executor:54] - Running task 73.0 in stage 1.0 (TID 247)
2017-11-03 09:43:23,436 INFO[org.apache.spark.executor.Executor:54] - Finished task 72.0 in stage 1.0 (TID 246). 1165 bytes result sent to driver
2017-11-03 09:43:23,436 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 71.0 in stage 1.0 (TID 245) in 0 ms on localhost (executor driver) (71/175)
2017-11-03 09:43:23,436 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,436 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,436 INFO[org.apache.spark.executor.Executor:54] - Finished task 73.0 in stage 1.0 (TID 247). 1165 bytes result sent to driver
2017-11-03 09:43:23,436 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 74.0 in stage 1.0 (TID 248, localhost, executor driver, partition 74, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,436 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 75.0 in stage 1.0 (TID 249, localhost, executor driver, partition 75, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,436 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 72.0 in stage 1.0 (TID 246) in 0 ms on localhost (executor driver) (72/175)
2017-11-03 09:43:23,436 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 73.0 in stage 1.0 (TID 247) in 0 ms on localhost (executor driver) (73/175)
2017-11-03 09:43:23,436 INFO[org.apache.spark.executor.Executor:54] - Running task 74.0 in stage 1.0 (TID 248)
2017-11-03 09:43:23,436 INFO[org.apache.spark.executor.Executor:54] - Running task 75.0 in stage 1.0 (TID 249)
2017-11-03 09:43:23,452 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,452 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,452 INFO[org.apache.spark.executor.Executor:54] - Finished task 74.0 in stage 1.0 (TID 248). 1251 bytes result sent to driver
2017-11-03 09:43:23,452 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,452 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,452 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 76.0 in stage 1.0 (TID 250, localhost, executor driver, partition 76, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,452 INFO[org.apache.spark.executor.Executor:54] - Finished task 75.0 in stage 1.0 (TID 249). 1251 bytes result sent to driver
2017-11-03 09:43:23,452 INFO[org.apache.spark.executor.Executor:54] - Running task 76.0 in stage 1.0 (TID 250)
2017-11-03 09:43:23,452 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 77.0 in stage 1.0 (TID 251, localhost, executor driver, partition 77, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,452 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 75.0 in stage 1.0 (TID 249) in 16 ms on localhost (executor driver) (74/175)
2017-11-03 09:43:23,452 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 74.0 in stage 1.0 (TID 248) in 16 ms on localhost (executor driver) (75/175)
2017-11-03 09:43:23,452 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,452 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,452 INFO[org.apache.spark.executor.Executor:54] - Running task 77.0 in stage 1.0 (TID 251)
2017-11-03 09:43:23,452 INFO[org.apache.spark.executor.Executor:54] - Finished task 76.0 in stage 1.0 (TID 250). 1165 bytes result sent to driver
2017-11-03 09:43:23,467 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,467 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,467 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 78.0 in stage 1.0 (TID 252, localhost, executor driver, partition 78, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,467 INFO[org.apache.spark.executor.Executor:54] - Finished task 77.0 in stage 1.0 (TID 251). 1251 bytes result sent to driver
2017-11-03 09:43:23,467 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 76.0 in stage 1.0 (TID 250) in 15 ms on localhost (executor driver) (76/175)
2017-11-03 09:43:23,467 INFO[org.apache.spark.executor.Executor:54] - Running task 78.0 in stage 1.0 (TID 252)
2017-11-03 09:43:23,467 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,467 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,467 INFO[org.apache.spark.executor.Executor:54] - Finished task 78.0 in stage 1.0 (TID 252). 1165 bytes result sent to driver
2017-11-03 09:43:23,467 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 79.0 in stage 1.0 (TID 253, localhost, executor driver, partition 79, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,467 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 80.0 in stage 1.0 (TID 254, localhost, executor driver, partition 80, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,467 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 77.0 in stage 1.0 (TID 251) in 15 ms on localhost (executor driver) (77/175)
2017-11-03 09:43:23,467 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 78.0 in stage 1.0 (TID 252) in 0 ms on localhost (executor driver) (78/175)
2017-11-03 09:43:23,467 INFO[org.apache.spark.executor.Executor:54] - Running task 79.0 in stage 1.0 (TID 253)
2017-11-03 09:43:23,467 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,467 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,467 INFO[org.apache.spark.executor.Executor:54] - Running task 80.0 in stage 1.0 (TID 254)
2017-11-03 09:43:23,467 INFO[org.apache.spark.executor.Executor:54] - Finished task 79.0 in stage 1.0 (TID 253). 1165 bytes result sent to driver
2017-11-03 09:43:23,467 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,467 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,467 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 81.0 in stage 1.0 (TID 255, localhost, executor driver, partition 81, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,467 INFO[org.apache.spark.executor.Executor:54] - Finished task 80.0 in stage 1.0 (TID 254). 1165 bytes result sent to driver
2017-11-03 09:43:23,467 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 79.0 in stage 1.0 (TID 253) in 0 ms on localhost (executor driver) (79/175)
2017-11-03 09:43:23,467 INFO[org.apache.spark.executor.Executor:54] - Running task 81.0 in stage 1.0 (TID 255)
2017-11-03 09:43:23,483 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 82.0 in stage 1.0 (TID 256, localhost, executor driver, partition 82, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,498 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,498 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,498 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 80.0 in stage 1.0 (TID 254) in 31 ms on localhost (executor driver) (80/175)
2017-11-03 09:43:23,498 INFO[org.apache.spark.executor.Executor:54] - Running task 82.0 in stage 1.0 (TID 256)
2017-11-03 09:43:23,498 INFO[org.apache.spark.executor.Executor:54] - Finished task 81.0 in stage 1.0 (TID 255). 1208 bytes result sent to driver
2017-11-03 09:43:23,498 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 83.0 in stage 1.0 (TID 257, localhost, executor driver, partition 83, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,498 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,498 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,498 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 81.0 in stage 1.0 (TID 255) in 31 ms on localhost (executor driver) (81/175)
2017-11-03 09:43:23,498 INFO[org.apache.spark.executor.Executor:54] - Running task 83.0 in stage 1.0 (TID 257)
2017-11-03 09:43:23,514 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,514 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 16 ms
2017-11-03 09:43:23,514 INFO[org.apache.spark.executor.Executor:54] - Finished task 83.0 in stage 1.0 (TID 257). 1251 bytes result sent to driver
2017-11-03 09:43:23,514 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 84.0 in stage 1.0 (TID 258, localhost, executor driver, partition 84, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,514 INFO[org.apache.spark.executor.Executor:54] - Finished task 82.0 in stage 1.0 (TID 256). 1165 bytes result sent to driver
2017-11-03 09:43:23,514 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 83.0 in stage 1.0 (TID 257) in 16 ms on localhost (executor driver) (82/175)
2017-11-03 09:43:23,514 INFO[org.apache.spark.executor.Executor:54] - Running task 84.0 in stage 1.0 (TID 258)
2017-11-03 09:43:23,514 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 85.0 in stage 1.0 (TID 259, localhost, executor driver, partition 85, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,514 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 82.0 in stage 1.0 (TID 256) in 31 ms on localhost (executor driver) (83/175)
2017-11-03 09:43:23,514 INFO[org.apache.spark.executor.Executor:54] - Running task 85.0 in stage 1.0 (TID 259)
2017-11-03 09:43:23,514 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,514 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,514 INFO[org.apache.spark.executor.Executor:54] - Finished task 84.0 in stage 1.0 (TID 258). 1165 bytes result sent to driver
2017-11-03 09:43:23,514 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,514 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,514 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 86.0 in stage 1.0 (TID 260, localhost, executor driver, partition 86, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,514 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 84.0 in stage 1.0 (TID 258) in 0 ms on localhost (executor driver) (84/175)
2017-11-03 09:43:23,514 INFO[org.apache.spark.executor.Executor:54] - Running task 86.0 in stage 1.0 (TID 260)
2017-11-03 09:43:23,514 INFO[org.apache.spark.executor.Executor:54] - Finished task 85.0 in stage 1.0 (TID 259). 1165 bytes result sent to driver
2017-11-03 09:43:23,514 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 87.0 in stage 1.0 (TID 261, localhost, executor driver, partition 87, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,514 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 85.0 in stage 1.0 (TID 259) in 0 ms on localhost (executor driver) (85/175)
2017-11-03 09:43:23,514 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,514 INFO[org.apache.spark.executor.Executor:54] - Running task 87.0 in stage 1.0 (TID 261)
2017-11-03 09:43:23,514 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,514 INFO[org.apache.spark.executor.Executor:54] - Finished task 86.0 in stage 1.0 (TID 260). 1165 bytes result sent to driver
2017-11-03 09:43:23,514 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 88.0 in stage 1.0 (TID 262, localhost, executor driver, partition 88, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,514 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,514 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,514 INFO[org.apache.spark.executor.Executor:54] - Running task 88.0 in stage 1.0 (TID 262)
2017-11-03 09:43:23,514 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 86.0 in stage 1.0 (TID 260) in 0 ms on localhost (executor driver) (86/175)
2017-11-03 09:43:23,514 INFO[org.apache.spark.executor.Executor:54] - Finished task 87.0 in stage 1.0 (TID 261). 1165 bytes result sent to driver
2017-11-03 09:43:23,527 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 89.0 in stage 1.0 (TID 263, localhost, executor driver, partition 89, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,527 INFO[org.apache.spark.executor.Executor:54] - Running task 89.0 in stage 1.0 (TID 263)
2017-11-03 09:43:23,527 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,527 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 87.0 in stage 1.0 (TID 261) in 13 ms on localhost (executor driver) (87/175)
2017-11-03 09:43:23,527 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,528 INFO[org.apache.spark.executor.Executor:54] - Finished task 88.0 in stage 1.0 (TID 262). 1294 bytes result sent to driver
2017-11-03 09:43:23,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,529 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 90.0 in stage 1.0 (TID 264, localhost, executor driver, partition 90, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,529 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2017-11-03 09:43:23,529 INFO[org.apache.spark.executor.Executor:54] - Running task 90.0 in stage 1.0 (TID 264)
2017-11-03 09:43:23,529 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 88.0 in stage 1.0 (TID 262) in 15 ms on localhost (executor driver) (88/175)
2017-11-03 09:43:23,529 INFO[org.apache.spark.executor.Executor:54] - Finished task 89.0 in stage 1.0 (TID 263). 1208 bytes result sent to driver
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 91.0 in stage 1.0 (TID 265, localhost, executor driver, partition 91, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Running task 91.0 in stage 1.0 (TID 265)
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 89.0 in stage 1.0 (TID 263) in 3 ms on localhost (executor driver) (89/175)
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Finished task 90.0 in stage 1.0 (TID 264). 1208 bytes result sent to driver
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 92.0 in stage 1.0 (TID 266, localhost, executor driver, partition 92, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Finished task 91.0 in stage 1.0 (TID 265). 1165 bytes result sent to driver
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 90.0 in stage 1.0 (TID 264) in 2 ms on localhost (executor driver) (90/175)
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Running task 92.0 in stage 1.0 (TID 266)
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 93.0 in stage 1.0 (TID 267, localhost, executor driver, partition 93, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Running task 93.0 in stage 1.0 (TID 267)
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 91.0 in stage 1.0 (TID 265) in 1 ms on localhost (executor driver) (91/175)
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Finished task 92.0 in stage 1.0 (TID 266). 1165 bytes result sent to driver
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 94.0 in stage 1.0 (TID 268, localhost, executor driver, partition 94, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Finished task 93.0 in stage 1.0 (TID 267). 1165 bytes result sent to driver
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Running task 94.0 in stage 1.0 (TID 268)
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 92.0 in stage 1.0 (TID 266) in 0 ms on localhost (executor driver) (92/175)
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 95.0 in stage 1.0 (TID 269, localhost, executor driver, partition 95, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 93.0 in stage 1.0 (TID 267) in 0 ms on localhost (executor driver) (93/175)
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Running task 95.0 in stage 1.0 (TID 269)
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Finished task 94.0 in stage 1.0 (TID 268). 1165 bytes result sent to driver
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 96.0 in stage 1.0 (TID 270, localhost, executor driver, partition 96, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 94.0 in stage 1.0 (TID 268) in 0 ms on localhost (executor driver) (94/175)
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Running task 96.0 in stage 1.0 (TID 270)
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Finished task 95.0 in stage 1.0 (TID 269). 1165 bytes result sent to driver
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 97.0 in stage 1.0 (TID 271, localhost, executor driver, partition 97, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Running task 97.0 in stage 1.0 (TID 271)
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 95.0 in stage 1.0 (TID 269) in 0 ms on localhost (executor driver) (95/175)
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Finished task 96.0 in stage 1.0 (TID 270). 1165 bytes result sent to driver
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 98.0 in stage 1.0 (TID 272, localhost, executor driver, partition 98, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 96.0 in stage 1.0 (TID 270) in 0 ms on localhost (executor driver) (96/175)
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Running task 98.0 in stage 1.0 (TID 272)
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Finished task 97.0 in stage 1.0 (TID 271). 1165 bytes result sent to driver
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 99.0 in stage 1.0 (TID 273, localhost, executor driver, partition 99, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 97.0 in stage 1.0 (TID 271) in 0 ms on localhost (executor driver) (97/175)
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Running task 99.0 in stage 1.0 (TID 273)
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Finished task 98.0 in stage 1.0 (TID 272). 1165 bytes result sent to driver
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 100.0 in stage 1.0 (TID 274, localhost, executor driver, partition 100, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Running task 100.0 in stage 1.0 (TID 274)
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,530 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Finished task 99.0 in stage 1.0 (TID 273). 1165 bytes result sent to driver
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 98.0 in stage 1.0 (TID 272) in 0 ms on localhost (executor driver) (98/175)
2017-11-03 09:43:23,530 INFO[org.apache.spark.executor.Executor:54] - Finished task 100.0 in stage 1.0 (TID 274). 1165 bytes result sent to driver
2017-11-03 09:43:23,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 101.0 in stage 1.0 (TID 275, localhost, executor driver, partition 101, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Running task 101.0 in stage 1.0 (TID 275)
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 102.0 in stage 1.0 (TID 276, localhost, executor driver, partition 102, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Running task 102.0 in stage 1.0 (TID 276)
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 100.0 in stage 1.0 (TID 274) in 16 ms on localhost (executor driver) (99/175)
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Finished task 101.0 in stage 1.0 (TID 275). 1251 bytes result sent to driver
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 99.0 in stage 1.0 (TID 273) in 16 ms on localhost (executor driver) (100/175)
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Finished task 102.0 in stage 1.0 (TID 276). 1165 bytes result sent to driver
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 103.0 in stage 1.0 (TID 277, localhost, executor driver, partition 103, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Running task 103.0 in stage 1.0 (TID 277)
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 104.0 in stage 1.0 (TID 278, localhost, executor driver, partition 104, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 102.0 in stage 1.0 (TID 276) in 0 ms on localhost (executor driver) (101/175)
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Running task 104.0 in stage 1.0 (TID 278)
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 101.0 in stage 1.0 (TID 275) in 16 ms on localhost (executor driver) (102/175)
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Finished task 104.0 in stage 1.0 (TID 278). 1165 bytes result sent to driver
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 105.0 in stage 1.0 (TID 279, localhost, executor driver, partition 105, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Finished task 103.0 in stage 1.0 (TID 277). 1165 bytes result sent to driver
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Running task 105.0 in stage 1.0 (TID 279)
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 104.0 in stage 1.0 (TID 278) in 0 ms on localhost (executor driver) (103/175)
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 106.0 in stage 1.0 (TID 280, localhost, executor driver, partition 106, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 103.0 in stage 1.0 (TID 277) in 0 ms on localhost (executor driver) (104/175)
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Running task 106.0 in stage 1.0 (TID 280)
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Finished task 105.0 in stage 1.0 (TID 279). 1165 bytes result sent to driver
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 107.0 in stage 1.0 (TID 281, localhost, executor driver, partition 107, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Running task 107.0 in stage 1.0 (TID 281)
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Finished task 106.0 in stage 1.0 (TID 280). 1165 bytes result sent to driver
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 105.0 in stage 1.0 (TID 279) in 0 ms on localhost (executor driver) (105/175)
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 108.0 in stage 1.0 (TID 282, localhost, executor driver, partition 108, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 106.0 in stage 1.0 (TID 280) in 0 ms on localhost (executor driver) (106/175)
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Running task 108.0 in stage 1.0 (TID 282)
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Finished task 107.0 in stage 1.0 (TID 281). 1165 bytes result sent to driver
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 109.0 in stage 1.0 (TID 283, localhost, executor driver, partition 109, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Running task 109.0 in stage 1.0 (TID 283)
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 107.0 in stage 1.0 (TID 281) in 0 ms on localhost (executor driver) (107/175)
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Finished task 108.0 in stage 1.0 (TID 282). 1165 bytes result sent to driver
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 110.0 in stage 1.0 (TID 284, localhost, executor driver, partition 110, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Running task 110.0 in stage 1.0 (TID 284)
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 108.0 in stage 1.0 (TID 282) in 0 ms on localhost (executor driver) (108/175)
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Finished task 109.0 in stage 1.0 (TID 283). 1165 bytes result sent to driver
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 111.0 in stage 1.0 (TID 285, localhost, executor driver, partition 111, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Running task 111.0 in stage 1.0 (TID 285)
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 109.0 in stage 1.0 (TID 283) in 0 ms on localhost (executor driver) (109/175)
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Finished task 110.0 in stage 1.0 (TID 284). 1165 bytes result sent to driver
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 112.0 in stage 1.0 (TID 286, localhost, executor driver, partition 112, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 110.0 in stage 1.0 (TID 284) in 0 ms on localhost (executor driver) (110/175)
2017-11-03 09:43:23,546 INFO[org.apache.spark.executor.Executor:54] - Running task 112.0 in stage 1.0 (TID 286)
2017-11-03 09:43:23,546 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 15 ms
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Finished task 111.0 in stage 1.0 (TID 285). 1251 bytes result sent to driver
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 113.0 in stage 1.0 (TID 287, localhost, executor driver, partition 113, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Finished task 112.0 in stage 1.0 (TID 286). 1251 bytes result sent to driver
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 111.0 in stage 1.0 (TID 285) in 15 ms on localhost (executor driver) (111/175)
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Running task 113.0 in stage 1.0 (TID 287)
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 114.0 in stage 1.0 (TID 288, localhost, executor driver, partition 114, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 112.0 in stage 1.0 (TID 286) in 15 ms on localhost (executor driver) (112/175)
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Running task 114.0 in stage 1.0 (TID 288)
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Finished task 113.0 in stage 1.0 (TID 287). 1165 bytes result sent to driver
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Finished task 114.0 in stage 1.0 (TID 288). 1165 bytes result sent to driver
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 115.0 in stage 1.0 (TID 289, localhost, executor driver, partition 115, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 116.0 in stage 1.0 (TID 290, localhost, executor driver, partition 116, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Running task 115.0 in stage 1.0 (TID 289)
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Running task 116.0 in stage 1.0 (TID 290)
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 114.0 in stage 1.0 (TID 288) in 0 ms on localhost (executor driver) (113/175)
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 113.0 in stage 1.0 (TID 287) in 0 ms on localhost (executor driver) (114/175)
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Finished task 115.0 in stage 1.0 (TID 289). 1165 bytes result sent to driver
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Finished task 116.0 in stage 1.0 (TID 290). 1165 bytes result sent to driver
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 117.0 in stage 1.0 (TID 291, localhost, executor driver, partition 117, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Running task 117.0 in stage 1.0 (TID 291)
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 118.0 in stage 1.0 (TID 292, localhost, executor driver, partition 118, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 115.0 in stage 1.0 (TID 289) in 0 ms on localhost (executor driver) (115/175)
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Running task 118.0 in stage 1.0 (TID 292)
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Finished task 117.0 in stage 1.0 (TID 291). 1165 bytes result sent to driver
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Finished task 118.0 in stage 1.0 (TID 292). 1165 bytes result sent to driver
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 116.0 in stage 1.0 (TID 290) in 0 ms on localhost (executor driver) (116/175)
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 119.0 in stage 1.0 (TID 293, localhost, executor driver, partition 119, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Running task 119.0 in stage 1.0 (TID 293)
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 117.0 in stage 1.0 (TID 291) in 0 ms on localhost (executor driver) (117/175)
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,561 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 120.0 in stage 1.0 (TID 294, localhost, executor driver, partition 120, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 118.0 in stage 1.0 (TID 292) in 0 ms on localhost (executor driver) (118/175)
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Running task 120.0 in stage 1.0 (TID 294)
2017-11-03 09:43:23,561 INFO[org.apache.spark.executor.Executor:54] - Finished task 119.0 in stage 1.0 (TID 293). 1165 bytes result sent to driver
2017-11-03 09:43:23,561 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 121.0 in stage 1.0 (TID 295, localhost, executor driver, partition 121, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 119.0 in stage 1.0 (TID 293) in 16 ms on localhost (executor driver) (119/175)
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Finished task 120.0 in stage 1.0 (TID 294). 1208 bytes result sent to driver
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Running task 121.0 in stage 1.0 (TID 295)
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 122.0 in stage 1.0 (TID 296, localhost, executor driver, partition 122, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Running task 122.0 in stage 1.0 (TID 296)
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 120.0 in stage 1.0 (TID 294) in 16 ms on localhost (executor driver) (120/175)
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Finished task 121.0 in stage 1.0 (TID 295). 1165 bytes result sent to driver
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Finished task 122.0 in stage 1.0 (TID 296). 1165 bytes result sent to driver
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 123.0 in stage 1.0 (TID 297, localhost, executor driver, partition 123, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Running task 123.0 in stage 1.0 (TID 297)
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 124.0 in stage 1.0 (TID 298, localhost, executor driver, partition 124, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Running task 124.0 in stage 1.0 (TID 298)
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 121.0 in stage 1.0 (TID 295) in 16 ms on localhost (executor driver) (121/175)
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 122.0 in stage 1.0 (TID 296) in 0 ms on localhost (executor driver) (122/175)
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Finished task 124.0 in stage 1.0 (TID 298). 1165 bytes result sent to driver
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 125.0 in stage 1.0 (TID 299, localhost, executor driver, partition 125, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Finished task 123.0 in stage 1.0 (TID 297). 1165 bytes result sent to driver
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Running task 125.0 in stage 1.0 (TID 299)
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 124.0 in stage 1.0 (TID 298) in 0 ms on localhost (executor driver) (123/175)
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 123.0 in stage 1.0 (TID 297) in 0 ms on localhost (executor driver) (124/175)
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 126.0 in stage 1.0 (TID 300, localhost, executor driver, partition 126, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Running task 126.0 in stage 1.0 (TID 300)
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Finished task 125.0 in stage 1.0 (TID 299). 1165 bytes result sent to driver
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 127.0 in stage 1.0 (TID 301, localhost, executor driver, partition 127, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Running task 127.0 in stage 1.0 (TID 301)
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 125.0 in stage 1.0 (TID 299) in 0 ms on localhost (executor driver) (125/175)
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Finished task 126.0 in stage 1.0 (TID 300). 1165 bytes result sent to driver
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 128.0 in stage 1.0 (TID 302, localhost, executor driver, partition 128, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 126.0 in stage 1.0 (TID 300) in 0 ms on localhost (executor driver) (126/175)
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Running task 128.0 in stage 1.0 (TID 302)
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Finished task 127.0 in stage 1.0 (TID 301). 1165 bytes result sent to driver
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 129.0 in stage 1.0 (TID 303, localhost, executor driver, partition 129, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 127.0 in stage 1.0 (TID 301) in 0 ms on localhost (executor driver) (127/175)
2017-11-03 09:43:23,577 INFO[org.apache.spark.executor.Executor:54] - Running task 129.0 in stage 1.0 (TID 303)
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,577 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,593 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,593 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Finished task 129.0 in stage 1.0 (TID 303). 1251 bytes result sent to driver
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 130.0 in stage 1.0 (TID 304, localhost, executor driver, partition 130, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Finished task 128.0 in stage 1.0 (TID 302). 1208 bytes result sent to driver
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Running task 130.0 in stage 1.0 (TID 304)
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 129.0 in stage 1.0 (TID 303) in 16 ms on localhost (executor driver) (128/175)
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 131.0 in stage 1.0 (TID 305, localhost, executor driver, partition 131, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,593 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,593 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 128.0 in stage 1.0 (TID 302) in 16 ms on localhost (executor driver) (129/175)
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Running task 131.0 in stage 1.0 (TID 305)
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Finished task 130.0 in stage 1.0 (TID 304). 1165 bytes result sent to driver
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 132.0 in stage 1.0 (TID 306, localhost, executor driver, partition 132, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,593 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,593 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 130.0 in stage 1.0 (TID 304) in 0 ms on localhost (executor driver) (130/175)
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Finished task 131.0 in stage 1.0 (TID 305). 1165 bytes result sent to driver
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Running task 132.0 in stage 1.0 (TID 306)
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 133.0 in stage 1.0 (TID 307, localhost, executor driver, partition 133, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 131.0 in stage 1.0 (TID 305) in 0 ms on localhost (executor driver) (131/175)
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Running task 133.0 in stage 1.0 (TID 307)
2017-11-03 09:43:23,593 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,593 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Finished task 132.0 in stage 1.0 (TID 306). 1165 bytes result sent to driver
2017-11-03 09:43:23,593 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,593 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 134.0 in stage 1.0 (TID 308, localhost, executor driver, partition 134, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Finished task 133.0 in stage 1.0 (TID 307). 1165 bytes result sent to driver
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 132.0 in stage 1.0 (TID 306) in 0 ms on localhost (executor driver) (132/175)
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Running task 134.0 in stage 1.0 (TID 308)
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 135.0 in stage 1.0 (TID 309, localhost, executor driver, partition 135, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 133.0 in stage 1.0 (TID 307) in 0 ms on localhost (executor driver) (133/175)
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Running task 135.0 in stage 1.0 (TID 309)
2017-11-03 09:43:23,593 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,593 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Finished task 134.0 in stage 1.0 (TID 308). 1165 bytes result sent to driver
2017-11-03 09:43:23,593 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,593 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 136.0 in stage 1.0 (TID 310, localhost, executor driver, partition 136, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Finished task 135.0 in stage 1.0 (TID 309). 1165 bytes result sent to driver
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 134.0 in stage 1.0 (TID 308) in 0 ms on localhost (executor driver) (134/175)
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Running task 136.0 in stage 1.0 (TID 310)
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 137.0 in stage 1.0 (TID 311, localhost, executor driver, partition 137, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,593 INFO[org.apache.spark.executor.Executor:54] - Running task 137.0 in stage 1.0 (TID 311)
2017-11-03 09:43:23,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 135.0 in stage 1.0 (TID 309) in 0 ms on localhost (executor driver) (135/175)
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Finished task 137.0 in stage 1.0 (TID 311). 1251 bytes result sent to driver
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Finished task 136.0 in stage 1.0 (TID 310). 1251 bytes result sent to driver
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 138.0 in stage 1.0 (TID 312, localhost, executor driver, partition 138, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Running task 138.0 in stage 1.0 (TID 312)
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 139.0 in stage 1.0 (TID 313, localhost, executor driver, partition 139, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 137.0 in stage 1.0 (TID 311) in 15 ms on localhost (executor driver) (136/175)
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Running task 139.0 in stage 1.0 (TID 313)
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 136.0 in stage 1.0 (TID 310) in 15 ms on localhost (executor driver) (137/175)
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Finished task 138.0 in stage 1.0 (TID 312). 1165 bytes result sent to driver
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 140.0 in stage 1.0 (TID 314, localhost, executor driver, partition 140, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 138.0 in stage 1.0 (TID 312) in 0 ms on localhost (executor driver) (138/175)
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Finished task 139.0 in stage 1.0 (TID 313). 1165 bytes result sent to driver
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Running task 140.0 in stage 1.0 (TID 314)
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Finished task 140.0 in stage 1.0 (TID 314). 1165 bytes result sent to driver
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 141.0 in stage 1.0 (TID 315, localhost, executor driver, partition 141, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Running task 141.0 in stage 1.0 (TID 315)
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 142.0 in stage 1.0 (TID 316, localhost, executor driver, partition 142, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 139.0 in stage 1.0 (TID 313) in 0 ms on localhost (executor driver) (139/175)
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Running task 142.0 in stage 1.0 (TID 316)
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Finished task 141.0 in stage 1.0 (TID 315). 1165 bytes result sent to driver
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 140.0 in stage 1.0 (TID 314) in 0 ms on localhost (executor driver) (140/175)
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 143.0 in stage 1.0 (TID 317, localhost, executor driver, partition 143, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Finished task 142.0 in stage 1.0 (TID 316). 1165 bytes result sent to driver
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Running task 143.0 in stage 1.0 (TID 317)
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 141.0 in stage 1.0 (TID 315) in 0 ms on localhost (executor driver) (141/175)
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 144.0 in stage 1.0 (TID 318, localhost, executor driver, partition 144, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 142.0 in stage 1.0 (TID 316) in 0 ms on localhost (executor driver) (142/175)
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Running task 144.0 in stage 1.0 (TID 318)
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Finished task 143.0 in stage 1.0 (TID 317). 1165 bytes result sent to driver
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,608 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 143.0 in stage 1.0 (TID 317) in 0 ms on localhost (executor driver) (143/175)
2017-11-03 09:43:23,608 INFO[org.apache.spark.executor.Executor:54] - Finished task 144.0 in stage 1.0 (TID 318). 1165 bytes result sent to driver
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 145.0 in stage 1.0 (TID 319, localhost, executor driver, partition 145, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 146.0 in stage 1.0 (TID 320, localhost, executor driver, partition 146, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Running task 145.0 in stage 1.0 (TID 319)
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 144.0 in stage 1.0 (TID 318) in 16 ms on localhost (executor driver) (144/175)
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Running task 146.0 in stage 1.0 (TID 320)
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Finished task 146.0 in stage 1.0 (TID 320). 1165 bytes result sent to driver
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 147.0 in stage 1.0 (TID 321, localhost, executor driver, partition 147, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 146.0 in stage 1.0 (TID 320) in 16 ms on localhost (executor driver) (145/175)
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Running task 147.0 in stage 1.0 (TID 321)
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Finished task 145.0 in stage 1.0 (TID 319). 1165 bytes result sent to driver
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 148.0 in stage 1.0 (TID 322, localhost, executor driver, partition 148, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Running task 148.0 in stage 1.0 (TID 322)
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 145.0 in stage 1.0 (TID 319) in 16 ms on localhost (executor driver) (146/175)
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Finished task 147.0 in stage 1.0 (TID 321). 1165 bytes result sent to driver
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 149.0 in stage 1.0 (TID 323, localhost, executor driver, partition 149, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Finished task 148.0 in stage 1.0 (TID 322). 1165 bytes result sent to driver
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Running task 149.0 in stage 1.0 (TID 323)
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 147.0 in stage 1.0 (TID 321) in 0 ms on localhost (executor driver) (147/175)
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 150.0 in stage 1.0 (TID 324, localhost, executor driver, partition 150, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Running task 150.0 in stage 1.0 (TID 324)
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 148.0 in stage 1.0 (TID 322) in 0 ms on localhost (executor driver) (148/175)
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Finished task 149.0 in stage 1.0 (TID 323). 1165 bytes result sent to driver
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Finished task 150.0 in stage 1.0 (TID 324). 1165 bytes result sent to driver
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 151.0 in stage 1.0 (TID 325, localhost, executor driver, partition 151, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Running task 151.0 in stage 1.0 (TID 325)
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 152.0 in stage 1.0 (TID 326, localhost, executor driver, partition 152, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Running task 152.0 in stage 1.0 (TID 326)
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 150.0 in stage 1.0 (TID 324) in 0 ms on localhost (executor driver) (149/175)
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 149.0 in stage 1.0 (TID 323) in 0 ms on localhost (executor driver) (150/175)
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Finished task 151.0 in stage 1.0 (TID 325). 1165 bytes result sent to driver
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Finished task 152.0 in stage 1.0 (TID 326). 1165 bytes result sent to driver
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 153.0 in stage 1.0 (TID 327, localhost, executor driver, partition 153, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Running task 153.0 in stage 1.0 (TID 327)
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 154.0 in stage 1.0 (TID 328, localhost, executor driver, partition 154, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 151.0 in stage 1.0 (TID 325) in 0 ms on localhost (executor driver) (151/175)
2017-11-03 09:43:23,624 INFO[org.apache.spark.executor.Executor:54] - Running task 154.0 in stage 1.0 (TID 328)
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,624 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Finished task 153.0 in stage 1.0 (TID 327). 1251 bytes result sent to driver
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 152.0 in stage 1.0 (TID 326) in 16 ms on localhost (executor driver) (152/175)
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Finished task 154.0 in stage 1.0 (TID 328). 1251 bytes result sent to driver
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 155.0 in stage 1.0 (TID 329, localhost, executor driver, partition 155, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Running task 155.0 in stage 1.0 (TID 329)
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 156.0 in stage 1.0 (TID 330, localhost, executor driver, partition 156, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Running task 156.0 in stage 1.0 (TID 330)
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 154.0 in stage 1.0 (TID 328) in 16 ms on localhost (executor driver) (153/175)
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 153.0 in stage 1.0 (TID 327) in 16 ms on localhost (executor driver) (154/175)
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Finished task 156.0 in stage 1.0 (TID 330). 1165 bytes result sent to driver
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Finished task 155.0 in stage 1.0 (TID 329). 1165 bytes result sent to driver
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 157.0 in stage 1.0 (TID 331, localhost, executor driver, partition 157, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Running task 157.0 in stage 1.0 (TID 331)
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 158.0 in stage 1.0 (TID 332, localhost, executor driver, partition 158, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 156.0 in stage 1.0 (TID 330) in 0 ms on localhost (executor driver) (155/175)
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Running task 158.0 in stage 1.0 (TID 332)
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 155.0 in stage 1.0 (TID 329) in 0 ms on localhost (executor driver) (156/175)
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Finished task 157.0 in stage 1.0 (TID 331). 1165 bytes result sent to driver
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Finished task 158.0 in stage 1.0 (TID 332). 1165 bytes result sent to driver
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 159.0 in stage 1.0 (TID 333, localhost, executor driver, partition 159, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Running task 159.0 in stage 1.0 (TID 333)
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 160.0 in stage 1.0 (TID 334, localhost, executor driver, partition 160, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 158.0 in stage 1.0 (TID 332) in 0 ms on localhost (executor driver) (157/175)
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 157.0 in stage 1.0 (TID 331) in 0 ms on localhost (executor driver) (158/175)
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Running task 160.0 in stage 1.0 (TID 334)
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Finished task 159.0 in stage 1.0 (TID 333). 1165 bytes result sent to driver
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 161.0 in stage 1.0 (TID 335, localhost, executor driver, partition 161, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 159.0 in stage 1.0 (TID 333) in 0 ms on localhost (executor driver) (159/175)
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Finished task 160.0 in stage 1.0 (TID 334). 1165 bytes result sent to driver
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Running task 161.0 in stage 1.0 (TID 335)
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 162.0 in stage 1.0 (TID 336, localhost, executor driver, partition 162, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,640 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 160.0 in stage 1.0 (TID 334) in 0 ms on localhost (executor driver) (160/175)
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Running task 162.0 in stage 1.0 (TID 336)
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,640 INFO[org.apache.spark.executor.Executor:54] - Finished task 161.0 in stage 1.0 (TID 335). 1165 bytes result sent to driver
2017-11-03 09:43:23,640 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 15 ms
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 163.0 in stage 1.0 (TID 337, localhost, executor driver, partition 163, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Running task 163.0 in stage 1.0 (TID 337)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 161.0 in stage 1.0 (TID 335) in 15 ms on localhost (executor driver) (161/175)
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Finished task 162.0 in stage 1.0 (TID 336). 1251 bytes result sent to driver
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 164.0 in stage 1.0 (TID 338, localhost, executor driver, partition 164, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Running task 164.0 in stage 1.0 (TID 338)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 162.0 in stage 1.0 (TID 336) in 15 ms on localhost (executor driver) (162/175)
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Finished task 163.0 in stage 1.0 (TID 337). 1165 bytes result sent to driver
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Finished task 164.0 in stage 1.0 (TID 338). 1165 bytes result sent to driver
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 165.0 in stage 1.0 (TID 339, localhost, executor driver, partition 165, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Running task 165.0 in stage 1.0 (TID 339)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 166.0 in stage 1.0 (TID 340, localhost, executor driver, partition 166, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Running task 166.0 in stage 1.0 (TID 340)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 163.0 in stage 1.0 (TID 337) in 15 ms on localhost (executor driver) (163/175)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 164.0 in stage 1.0 (TID 338) in 0 ms on localhost (executor driver) (164/175)
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Finished task 165.0 in stage 1.0 (TID 339). 1165 bytes result sent to driver
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Finished task 166.0 in stage 1.0 (TID 340). 1165 bytes result sent to driver
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 167.0 in stage 1.0 (TID 341, localhost, executor driver, partition 167, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Running task 167.0 in stage 1.0 (TID 341)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 168.0 in stage 1.0 (TID 342, localhost, executor driver, partition 168, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 165.0 in stage 1.0 (TID 339) in 0 ms on localhost (executor driver) (165/175)
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Running task 168.0 in stage 1.0 (TID 342)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 166.0 in stage 1.0 (TID 340) in 0 ms on localhost (executor driver) (166/175)
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Finished task 167.0 in stage 1.0 (TID 341). 1165 bytes result sent to driver
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Finished task 168.0 in stage 1.0 (TID 342). 1165 bytes result sent to driver
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 169.0 in stage 1.0 (TID 343, localhost, executor driver, partition 169, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Running task 169.0 in stage 1.0 (TID 343)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 170.0 in stage 1.0 (TID 344, localhost, executor driver, partition 170, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 168.0 in stage 1.0 (TID 342) in 0 ms on localhost (executor driver) (167/175)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 167.0 in stage 1.0 (TID 341) in 0 ms on localhost (executor driver) (168/175)
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Running task 170.0 in stage 1.0 (TID 344)
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Finished task 169.0 in stage 1.0 (TID 343). 1165 bytes result sent to driver
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 171.0 in stage 1.0 (TID 345, localhost, executor driver, partition 171, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Finished task 170.0 in stage 1.0 (TID 344). 1165 bytes result sent to driver
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Running task 171.0 in stage 1.0 (TID 345)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 169.0 in stage 1.0 (TID 343) in 0 ms on localhost (executor driver) (169/175)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 172.0 in stage 1.0 (TID 346, localhost, executor driver, partition 172, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Running task 172.0 in stage 1.0 (TID 346)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 170.0 in stage 1.0 (TID 344) in 0 ms on localhost (executor driver) (170/175)
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Finished task 171.0 in stage 1.0 (TID 345). 1165 bytes result sent to driver
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 173.0 in stage 1.0 (TID 347, localhost, executor driver, partition 173, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Finished task 172.0 in stage 1.0 (TID 346). 1165 bytes result sent to driver
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Running task 173.0 in stage 1.0 (TID 347)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 171.0 in stage 1.0 (TID 345) in 0 ms on localhost (executor driver) (171/175)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 174.0 in stage 1.0 (TID 348, localhost, executor driver, partition 174, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:23,655 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 172.0 in stage 1.0 (TID 346) in 0 ms on localhost (executor driver) (172/175)
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Running task 174.0 in stage 1.0 (TID 348)
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,655 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,655 INFO[org.apache.spark.executor.Executor:54] - Finished task 173.0 in stage 1.0 (TID 347). 1165 bytes result sent to driver
2017-11-03 09:43:23,671 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,671 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,671 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 52.0 in stage 1.0 (TID 349, localhost, executor driver, partition 52, ANY, 4621 bytes)
2017-11-03 09:43:23,671 INFO[org.apache.spark.executor.Executor:54] - Running task 52.0 in stage 1.0 (TID 349)
2017-11-03 09:43:23,671 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 173.0 in stage 1.0 (TID 347) in 16 ms on localhost (executor driver) (173/175)
2017-11-03 09:43:23,671 INFO[org.apache.spark.executor.Executor:54] - Finished task 174.0 in stage 1.0 (TID 348). 1251 bytes result sent to driver
2017-11-03 09:43:23,671 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 174.0 in stage 1.0 (TID 348) in 16 ms on localhost (executor driver) (174/175)
2017-11-03 09:43:23,671 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 175 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,671 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,785 INFO[org.apache.spark.executor.Executor:54] - Finished task 52.0 in stage 1.0 (TID 349). 1258 bytes result sent to driver
2017-11-03 09:43:23,786 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 52.0 in stage 1.0 (TID 349) in 115 ms on localhost (executor driver) (175/175)
2017-11-03 09:43:23,786 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-11-03 09:43:23,787 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (sortByKey at SparkNginxLog.java:47) finished in 0.579 s
2017-11-03 09:43:23,789 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: sortByKey at SparkNginxLog.java:47, took 27.892442 s
2017-11-03 09:43:23,814 INFO[org.apache.spark.SparkContext:54] - Starting job: collect at SparkNginxLog.java:49
2017-11-03 09:43:23,814 INFO[org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 0 is 450 bytes
2017-11-03 09:43:23,814 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (reduceByKey at SparkNginxLog.java:41)
2017-11-03 09:43:23,814 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (collect at SparkNginxLog.java:49) with 2 output partitions
2017-11-03 09:43:23,814 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 4 (collect at SparkNginxLog.java:49)
2017-11-03 09:43:23,814 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 3)
2017-11-03 09:43:23,814 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 3)
2017-11-03 09:43:23,814 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 3 (ShuffledRDD[3] at reduceByKey at SparkNginxLog.java:41), which has no missing parents
2017-11-03 09:43:23,830 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 3.8 KB, free 631.4 MB)
2017-11-03 09:43:23,830 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 2.2 KB, free 631.4 MB)
2017-11-03 09:43:23,830 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:57421 (size: 2.2 KB, free: 631.8 MB)
2017-11-03 09:43:23,830 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2017-11-03 09:43:23,830 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 175 missing tasks from ShuffleMapStage 3 (ShuffledRDD[3] at reduceByKey at SparkNginxLog.java:41) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2017-11-03 09:43:23,830 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 175 tasks
2017-11-03 09:43:23,830 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 350, localhost, executor driver, partition 0, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,830 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 3.0 (TID 351, localhost, executor driver, partition 1, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,830 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 350)
2017-11-03 09:43:23,830 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 3.0 (TID 351)
2017-11-03 09:43:23,830 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,830 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,830 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,830 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,845 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 350). 983 bytes result sent to driver
2017-11-03 09:43:23,845 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 3.0 (TID 352, localhost, executor driver, partition 2, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,845 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 3.0 (TID 351). 1026 bytes result sent to driver
2017-11-03 09:43:23,845 INFO[org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 3.0 (TID 352)
2017-11-03 09:43:23,845 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 350) in 15 ms on localhost (executor driver) (1/175)
2017-11-03 09:43:23,845 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 3.0 (TID 353, localhost, executor driver, partition 3, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,845 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 3.0 (TID 351) in 15 ms on localhost (executor driver) (2/175)
2017-11-03 09:43:23,845 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,845 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,845 INFO[org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 3.0 (TID 353)
2017-11-03 09:43:23,845 INFO[org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 3.0 (TID 352). 983 bytes result sent to driver
2017-11-03 09:43:23,845 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,845 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,845 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 3.0 (TID 354, localhost, executor driver, partition 4, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,845 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 3.0 (TID 352) in 0 ms on localhost (executor driver) (3/175)
2017-11-03 09:43:23,845 INFO[org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 3.0 (TID 354)
2017-11-03 09:43:23,845 INFO[org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 3.0 (TID 353). 983 bytes result sent to driver
2017-11-03 09:43:23,845 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,845 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,845 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 3.0 (TID 355, localhost, executor driver, partition 5, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,845 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 3.0 (TID 353) in 0 ms on localhost (executor driver) (4/175)
2017-11-03 09:43:23,845 INFO[org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 3.0 (TID 355)
2017-11-03 09:43:23,845 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,845 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,861 INFO[org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 3.0 (TID 354). 1069 bytes result sent to driver
2017-11-03 09:43:23,861 INFO[org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 3.0 (TID 355). 1069 bytes result sent to driver
2017-11-03 09:43:23,861 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 3.0 (TID 356, localhost, executor driver, partition 6, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,861 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 3.0 (TID 357, localhost, executor driver, partition 7, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,861 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 3.0 (TID 354) in 16 ms on localhost (executor driver) (5/175)
2017-11-03 09:43:23,861 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 3.0 (TID 355) in 16 ms on localhost (executor driver) (6/175)
2017-11-03 09:43:23,861 INFO[org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 3.0 (TID 357)
2017-11-03 09:43:23,861 INFO[org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 3.0 (TID 356)
2017-11-03 09:43:23,861 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,861 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,861 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,861 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,861 INFO[org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 3.0 (TID 357). 983 bytes result sent to driver
2017-11-03 09:43:23,861 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 3.0 (TID 358, localhost, executor driver, partition 8, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,861 INFO[org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 3.0 (TID 356). 983 bytes result sent to driver
2017-11-03 09:43:23,861 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 3.0 (TID 357) in 0 ms on localhost (executor driver) (7/175)
2017-11-03 09:43:23,861 INFO[org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 3.0 (TID 358)
2017-11-03 09:43:23,861 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 3.0 (TID 359, localhost, executor driver, partition 9, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,861 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 3.0 (TID 356) in 0 ms on localhost (executor driver) (8/175)
2017-11-03 09:43:23,861 INFO[org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 3.0 (TID 359)
2017-11-03 09:43:23,877 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,877 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,877 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,877 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 16 ms
2017-11-03 09:43:23,877 INFO[org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 3.0 (TID 358). 1026 bytes result sent to driver
2017-11-03 09:43:23,877 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 3.0 (TID 360, localhost, executor driver, partition 10, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,877 INFO[org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 3.0 (TID 359). 1069 bytes result sent to driver
2017-11-03 09:43:23,877 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 3.0 (TID 358) in 16 ms on localhost (executor driver) (9/175)
2017-11-03 09:43:23,877 INFO[org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 3.0 (TID 360)
2017-11-03 09:43:23,877 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 3.0 (TID 361, localhost, executor driver, partition 11, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,877 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 3.0 (TID 359) in 16 ms on localhost (executor driver) (10/175)
2017-11-03 09:43:23,877 INFO[org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 3.0 (TID 361)
2017-11-03 09:43:23,877 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,877 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,877 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,877 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,877 INFO[org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 3.0 (TID 360). 983 bytes result sent to driver
2017-11-03 09:43:23,877 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 3.0 (TID 362, localhost, executor driver, partition 12, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,877 INFO[org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 3.0 (TID 361). 983 bytes result sent to driver
2017-11-03 09:43:23,892 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 3.0 (TID 360) in 15 ms on localhost (executor driver) (11/175)
2017-11-03 09:43:23,892 INFO[org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 3.0 (TID 362)
2017-11-03 09:43:23,892 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 3.0 (TID 363, localhost, executor driver, partition 13, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,892 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 3.0 (TID 361) in 15 ms on localhost (executor driver) (12/175)
2017-11-03 09:43:23,892 INFO[org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 3.0 (TID 363)
2017-11-03 09:43:23,892 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,892 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,892 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,892 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,892 INFO[org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 3.0 (TID 362). 983 bytes result sent to driver
2017-11-03 09:43:23,892 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 3.0 (TID 364, localhost, executor driver, partition 14, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,892 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 3.0 (TID 362) in 15 ms on localhost (executor driver) (13/175)
2017-11-03 09:43:23,892 INFO[org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 3.0 (TID 364)
2017-11-03 09:43:23,892 INFO[org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 3.0 (TID 363). 983 bytes result sent to driver
2017-11-03 09:43:23,892 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 3.0 (TID 365, localhost, executor driver, partition 15, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,892 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 3.0 (TID 363) in 0 ms on localhost (executor driver) (14/175)
2017-11-03 09:43:23,892 INFO[org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 3.0 (TID 365)
2017-11-03 09:43:23,892 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,892 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,892 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,892 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,892 INFO[org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 3.0 (TID 364). 983 bytes result sent to driver
2017-11-03 09:43:23,892 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 16.0 in stage 3.0 (TID 366, localhost, executor driver, partition 16, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,892 INFO[org.apache.spark.executor.Executor:54] - Running task 16.0 in stage 3.0 (TID 366)
2017-11-03 09:43:23,892 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 3.0 (TID 364) in 0 ms on localhost (executor driver) (15/175)
2017-11-03 09:43:23,892 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,892 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,908 INFO[org.apache.spark.executor.Executor:54] - Finished task 16.0 in stage 3.0 (TID 366). 1069 bytes result sent to driver
2017-11-03 09:43:23,908 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 17.0 in stage 3.0 (TID 367, localhost, executor driver, partition 17, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,908 INFO[org.apache.spark.executor.Executor:54] - Running task 17.0 in stage 3.0 (TID 367)
2017-11-03 09:43:23,908 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 16.0 in stage 3.0 (TID 366) in 16 ms on localhost (executor driver) (16/175)
2017-11-03 09:43:23,908 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,908 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,908 INFO[org.apache.spark.executor.Executor:54] - Finished task 17.0 in stage 3.0 (TID 367). 983 bytes result sent to driver
2017-11-03 09:43:23,908 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 18.0 in stage 3.0 (TID 368, localhost, executor driver, partition 18, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,908 INFO[org.apache.spark.executor.Executor:54] - Running task 18.0 in stage 3.0 (TID 368)
2017-11-03 09:43:23,908 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 17.0 in stage 3.0 (TID 367) in 0 ms on localhost (executor driver) (17/175)
2017-11-03 09:43:23,908 INFO[org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 3.0 (TID 365). 1069 bytes result sent to driver
2017-11-03 09:43:23,908 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,908 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,908 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 19.0 in stage 3.0 (TID 369, localhost, executor driver, partition 19, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,908 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 3.0 (TID 365) in 16 ms on localhost (executor driver) (18/175)
2017-11-03 09:43:23,908 INFO[org.apache.spark.executor.Executor:54] - Running task 19.0 in stage 3.0 (TID 369)
2017-11-03 09:43:23,908 INFO[org.apache.spark.executor.Executor:54] - Finished task 18.0 in stage 3.0 (TID 368). 983 bytes result sent to driver
2017-11-03 09:43:23,908 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 20.0 in stage 3.0 (TID 370, localhost, executor driver, partition 20, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,908 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 18.0 in stage 3.0 (TID 368) in 0 ms on localhost (executor driver) (19/175)
2017-11-03 09:43:23,908 INFO[org.apache.spark.executor.Executor:54] - Running task 20.0 in stage 3.0 (TID 370)
2017-11-03 09:43:23,908 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,908 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,908 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,923 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 15 ms
2017-11-03 09:43:23,923 INFO[org.apache.spark.executor.Executor:54] - Finished task 19.0 in stage 3.0 (TID 369). 1069 bytes result sent to driver
2017-11-03 09:43:23,923 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 21.0 in stage 3.0 (TID 371, localhost, executor driver, partition 21, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,923 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 19.0 in stage 3.0 (TID 369) in 15 ms on localhost (executor driver) (20/175)
2017-11-03 09:43:23,923 INFO[org.apache.spark.executor.Executor:54] - Finished task 20.0 in stage 3.0 (TID 370). 1069 bytes result sent to driver
2017-11-03 09:43:23,923 INFO[org.apache.spark.executor.Executor:54] - Running task 21.0 in stage 3.0 (TID 371)
2017-11-03 09:43:23,923 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 22.0 in stage 3.0 (TID 372, localhost, executor driver, partition 22, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,923 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 20.0 in stage 3.0 (TID 370) in 15 ms on localhost (executor driver) (21/175)
2017-11-03 09:43:23,923 INFO[org.apache.spark.executor.Executor:54] - Running task 22.0 in stage 3.0 (TID 372)
2017-11-03 09:43:23,923 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,923 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,923 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,923 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,923 INFO[org.apache.spark.executor.Executor:54] - Finished task 21.0 in stage 3.0 (TID 371). 983 bytes result sent to driver
2017-11-03 09:43:23,923 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 23.0 in stage 3.0 (TID 373, localhost, executor driver, partition 23, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,923 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 21.0 in stage 3.0 (TID 371) in 0 ms on localhost (executor driver) (22/175)
2017-11-03 09:43:23,923 INFO[org.apache.spark.executor.Executor:54] - Running task 23.0 in stage 3.0 (TID 373)
2017-11-03 09:43:23,923 INFO[org.apache.spark.executor.Executor:54] - Finished task 22.0 in stage 3.0 (TID 372). 983 bytes result sent to driver
2017-11-03 09:43:23,923 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,923 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,923 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 24.0 in stage 3.0 (TID 374, localhost, executor driver, partition 24, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,923 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 22.0 in stage 3.0 (TID 372) in 0 ms on localhost (executor driver) (23/175)
2017-11-03 09:43:23,923 INFO[org.apache.spark.executor.Executor:54] - Running task 24.0 in stage 3.0 (TID 374)
2017-11-03 09:43:23,923 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,923 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,923 INFO[org.apache.spark.executor.Executor:54] - Finished task 23.0 in stage 3.0 (TID 373). 983 bytes result sent to driver
2017-11-03 09:43:23,939 INFO[org.apache.spark.executor.Executor:54] - Finished task 24.0 in stage 3.0 (TID 374). 1069 bytes result sent to driver
2017-11-03 09:43:23,939 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 25.0 in stage 3.0 (TID 375, localhost, executor driver, partition 25, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,939 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 26.0 in stage 3.0 (TID 376, localhost, executor driver, partition 26, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,939 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 23.0 in stage 3.0 (TID 373) in 16 ms on localhost (executor driver) (24/175)
2017-11-03 09:43:23,939 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 24.0 in stage 3.0 (TID 374) in 16 ms on localhost (executor driver) (25/175)
2017-11-03 09:43:23,939 INFO[org.apache.spark.executor.Executor:54] - Running task 25.0 in stage 3.0 (TID 375)
2017-11-03 09:43:23,939 INFO[org.apache.spark.executor.Executor:54] - Running task 26.0 in stage 3.0 (TID 376)
2017-11-03 09:43:23,939 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,939 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,939 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,939 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,939 INFO[org.apache.spark.executor.Executor:54] - Finished task 25.0 in stage 3.0 (TID 375). 983 bytes result sent to driver
2017-11-03 09:43:23,939 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 27.0 in stage 3.0 (TID 377, localhost, executor driver, partition 27, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,939 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 25.0 in stage 3.0 (TID 375) in 0 ms on localhost (executor driver) (26/175)
2017-11-03 09:43:23,939 INFO[org.apache.spark.executor.Executor:54] - Finished task 26.0 in stage 3.0 (TID 376). 983 bytes result sent to driver
2017-11-03 09:43:23,939 INFO[org.apache.spark.executor.Executor:54] - Running task 27.0 in stage 3.0 (TID 377)
2017-11-03 09:43:23,939 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 28.0 in stage 3.0 (TID 378, localhost, executor driver, partition 28, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,939 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 26.0 in stage 3.0 (TID 376) in 0 ms on localhost (executor driver) (27/175)
2017-11-03 09:43:23,939 INFO[org.apache.spark.executor.Executor:54] - Running task 28.0 in stage 3.0 (TID 378)
2017-11-03 09:43:23,939 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,939 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,939 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,939 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,955 INFO[org.apache.spark.executor.Executor:54] - Finished task 27.0 in stage 3.0 (TID 377). 1069 bytes result sent to driver
2017-11-03 09:43:23,955 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 29.0 in stage 3.0 (TID 379, localhost, executor driver, partition 29, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,955 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 27.0 in stage 3.0 (TID 377) in 16 ms on localhost (executor driver) (28/175)
2017-11-03 09:43:23,955 INFO[org.apache.spark.executor.Executor:54] - Running task 29.0 in stage 3.0 (TID 379)
2017-11-03 09:43:23,955 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,955 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,970 INFO[org.apache.spark.executor.Executor:54] - Finished task 29.0 in stage 3.0 (TID 379). 1026 bytes result sent to driver
2017-11-03 09:43:23,970 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 30.0 in stage 3.0 (TID 380, localhost, executor driver, partition 30, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,970 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 29.0 in stage 3.0 (TID 379) in 15 ms on localhost (executor driver) (29/175)
2017-11-03 09:43:23,970 INFO[org.apache.spark.executor.Executor:54] - Running task 30.0 in stage 3.0 (TID 380)
2017-11-03 09:43:23,970 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,970 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:23,986 INFO[org.apache.spark.executor.Executor:54] - Finished task 30.0 in stage 3.0 (TID 380). 1026 bytes result sent to driver
2017-11-03 09:43:23,986 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 31.0 in stage 3.0 (TID 381, localhost, executor driver, partition 31, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:23,986 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 30.0 in stage 3.0 (TID 380) in 16 ms on localhost (executor driver) (30/175)
2017-11-03 09:43:23,986 INFO[org.apache.spark.executor.Executor:54] - Running task 31.0 in stage 3.0 (TID 381)
2017-11-03 09:43:23,986 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:23,986 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,048 INFO[org.apache.spark.executor.Executor:54] - Finished task 31.0 in stage 3.0 (TID 381). 1026 bytes result sent to driver
2017-11-03 09:43:24,048 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 32.0 in stage 3.0 (TID 382, localhost, executor driver, partition 32, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,048 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 31.0 in stage 3.0 (TID 381) in 62 ms on localhost (executor driver) (31/175)
2017-11-03 09:43:24,048 INFO[org.apache.spark.executor.Executor:54] - Running task 32.0 in stage 3.0 (TID 382)
2017-11-03 09:43:24,048 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,048 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,048 INFO[org.apache.spark.executor.Executor:54] - Finished task 32.0 in stage 3.0 (TID 382). 983 bytes result sent to driver
2017-11-03 09:43:24,048 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 33.0 in stage 3.0 (TID 383, localhost, executor driver, partition 33, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,048 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 32.0 in stage 3.0 (TID 382) in 0 ms on localhost (executor driver) (32/175)
2017-11-03 09:43:24,064 INFO[org.apache.spark.executor.Executor:54] - Running task 33.0 in stage 3.0 (TID 383)
2017-11-03 09:43:24,064 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,064 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,064 INFO[org.apache.spark.executor.Executor:54] - Finished task 33.0 in stage 3.0 (TID 383). 983 bytes result sent to driver
2017-11-03 09:43:24,064 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 34.0 in stage 3.0 (TID 384, localhost, executor driver, partition 34, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,064 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 33.0 in stage 3.0 (TID 383) in 16 ms on localhost (executor driver) (33/175)
2017-11-03 09:43:24,064 INFO[org.apache.spark.executor.Executor:54] - Running task 34.0 in stage 3.0 (TID 384)
2017-11-03 09:43:24,064 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,064 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,080 INFO[org.apache.spark.executor.Executor:54] - Finished task 34.0 in stage 3.0 (TID 384). 983 bytes result sent to driver
2017-11-03 09:43:24,080 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 35.0 in stage 3.0 (TID 385, localhost, executor driver, partition 35, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,080 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 34.0 in stage 3.0 (TID 384) in 16 ms on localhost (executor driver) (34/175)
2017-11-03 09:43:24,080 INFO[org.apache.spark.executor.Executor:54] - Running task 35.0 in stage 3.0 (TID 385)
2017-11-03 09:43:24,080 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,080 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,080 INFO[org.apache.spark.executor.Executor:54] - Finished task 35.0 in stage 3.0 (TID 385). 983 bytes result sent to driver
2017-11-03 09:43:24,080 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 36.0 in stage 3.0 (TID 386, localhost, executor driver, partition 36, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,080 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 35.0 in stage 3.0 (TID 385) in 0 ms on localhost (executor driver) (35/175)
2017-11-03 09:43:24,080 INFO[org.apache.spark.executor.Executor:54] - Running task 36.0 in stage 3.0 (TID 386)
2017-11-03 09:43:24,080 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,080 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,080 INFO[org.apache.spark.executor.Executor:54] - Finished task 28.0 in stage 3.0 (TID 378). 1026 bytes result sent to driver
2017-11-03 09:43:24,080 INFO[org.apache.spark.executor.Executor:54] - Finished task 36.0 in stage 3.0 (TID 386). 983 bytes result sent to driver
2017-11-03 09:43:24,095 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 37.0 in stage 3.0 (TID 387, localhost, executor driver, partition 37, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,095 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 38.0 in stage 3.0 (TID 388, localhost, executor driver, partition 38, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,095 INFO[org.apache.spark.executor.Executor:54] - Running task 37.0 in stage 3.0 (TID 387)
2017-11-03 09:43:24,095 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 36.0 in stage 3.0 (TID 386) in 15 ms on localhost (executor driver) (36/175)
2017-11-03 09:43:24,095 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 28.0 in stage 3.0 (TID 378) in 156 ms on localhost (executor driver) (37/175)
2017-11-03 09:43:24,095 INFO[org.apache.spark.executor.Executor:54] - Running task 38.0 in stage 3.0 (TID 388)
2017-11-03 09:43:24,095 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,095 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,095 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,095 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,095 INFO[org.apache.spark.executor.Executor:54] - Finished task 37.0 in stage 3.0 (TID 387). 983 bytes result sent to driver
2017-11-03 09:43:24,095 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 39.0 in stage 3.0 (TID 389, localhost, executor driver, partition 39, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,095 INFO[org.apache.spark.executor.Executor:54] - Finished task 38.0 in stage 3.0 (TID 388). 983 bytes result sent to driver
2017-11-03 09:43:24,095 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 37.0 in stage 3.0 (TID 387) in 15 ms on localhost (executor driver) (38/175)
2017-11-03 09:43:24,095 INFO[org.apache.spark.executor.Executor:54] - Running task 39.0 in stage 3.0 (TID 389)
2017-11-03 09:43:24,095 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,095 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,095 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 40.0 in stage 3.0 (TID 390, localhost, executor driver, partition 40, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,095 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 38.0 in stage 3.0 (TID 388) in 0 ms on localhost (executor driver) (39/175)
2017-11-03 09:43:24,095 INFO[org.apache.spark.executor.Executor:54] - Running task 40.0 in stage 3.0 (TID 390)
2017-11-03 09:43:24,095 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,095 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,095 INFO[org.apache.spark.executor.Executor:54] - Finished task 39.0 in stage 3.0 (TID 389). 983 bytes result sent to driver
2017-11-03 09:43:24,095 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 41.0 in stage 3.0 (TID 391, localhost, executor driver, partition 41, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,095 INFO[org.apache.spark.executor.Executor:54] - Finished task 40.0 in stage 3.0 (TID 390). 983 bytes result sent to driver
2017-11-03 09:43:24,095 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 39.0 in stage 3.0 (TID 389) in 0 ms on localhost (executor driver) (40/175)
2017-11-03 09:43:24,095 INFO[org.apache.spark.executor.Executor:54] - Running task 41.0 in stage 3.0 (TID 391)
2017-11-03 09:43:24,095 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 42.0 in stage 3.0 (TID 392, localhost, executor driver, partition 42, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,111 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 40.0 in stage 3.0 (TID 390) in 16 ms on localhost (executor driver) (41/175)
2017-11-03 09:43:24,111 INFO[org.apache.spark.executor.Executor:54] - Running task 42.0 in stage 3.0 (TID 392)
2017-11-03 09:43:24,111 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,111 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,111 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,111 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,111 INFO[org.apache.spark.executor.Executor:54] - Finished task 41.0 in stage 3.0 (TID 391). 1069 bytes result sent to driver
2017-11-03 09:43:24,111 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 43.0 in stage 3.0 (TID 393, localhost, executor driver, partition 43, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,111 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 41.0 in stage 3.0 (TID 391) in 16 ms on localhost (executor driver) (42/175)
2017-11-03 09:43:24,111 INFO[org.apache.spark.executor.Executor:54] - Finished task 42.0 in stage 3.0 (TID 392). 983 bytes result sent to driver
2017-11-03 09:43:24,111 INFO[org.apache.spark.executor.Executor:54] - Running task 43.0 in stage 3.0 (TID 393)
2017-11-03 09:43:24,111 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 44.0 in stage 3.0 (TID 394, localhost, executor driver, partition 44, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,111 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,111 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,111 INFO[org.apache.spark.executor.Executor:54] - Finished task 43.0 in stage 3.0 (TID 393). 983 bytes result sent to driver
2017-11-03 09:43:24,111 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 45.0 in stage 3.0 (TID 395, localhost, executor driver, partition 45, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,111 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 43.0 in stage 3.0 (TID 393) in 0 ms on localhost (executor driver) (43/175)
2017-11-03 09:43:24,111 INFO[org.apache.spark.executor.Executor:54] - Running task 44.0 in stage 3.0 (TID 394)
2017-11-03 09:43:24,111 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 42.0 in stage 3.0 (TID 392) in 16 ms on localhost (executor driver) (44/175)
2017-11-03 09:43:24,111 INFO[org.apache.spark.executor.Executor:54] - Running task 45.0 in stage 3.0 (TID 395)
2017-11-03 09:43:24,111 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,111 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,111 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,111 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,127 INFO[org.apache.spark.executor.Executor:54] - Finished task 45.0 in stage 3.0 (TID 395). 1069 bytes result sent to driver
2017-11-03 09:43:24,127 INFO[org.apache.spark.executor.Executor:54] - Finished task 44.0 in stage 3.0 (TID 394). 1026 bytes result sent to driver
2017-11-03 09:43:24,127 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 46.0 in stage 3.0 (TID 396, localhost, executor driver, partition 46, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,127 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 45.0 in stage 3.0 (TID 395) in 16 ms on localhost (executor driver) (45/175)
2017-11-03 09:43:24,127 INFO[org.apache.spark.executor.Executor:54] - Running task 46.0 in stage 3.0 (TID 396)
2017-11-03 09:43:24,127 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,127 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,127 INFO[org.apache.spark.executor.Executor:54] - Finished task 46.0 in stage 3.0 (TID 396). 983 bytes result sent to driver
2017-11-03 09:43:24,127 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 47.0 in stage 3.0 (TID 397, localhost, executor driver, partition 47, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,127 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 46.0 in stage 3.0 (TID 396) in 0 ms on localhost (executor driver) (46/175)
2017-11-03 09:43:24,127 INFO[org.apache.spark.executor.Executor:54] - Running task 47.0 in stage 3.0 (TID 397)
2017-11-03 09:43:24,142 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,142 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,142 INFO[org.apache.spark.executor.Executor:54] - Finished task 47.0 in stage 3.0 (TID 397). 1069 bytes result sent to driver
2017-11-03 09:43:24,142 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 48.0 in stage 3.0 (TID 398, localhost, executor driver, partition 48, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,142 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 47.0 in stage 3.0 (TID 397) in 15 ms on localhost (executor driver) (47/175)
2017-11-03 09:43:24,142 INFO[org.apache.spark.executor.Executor:54] - Running task 48.0 in stage 3.0 (TID 398)
2017-11-03 09:43:24,142 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,142 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,142 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 49.0 in stage 3.0 (TID 399, localhost, executor driver, partition 49, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,142 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 44.0 in stage 3.0 (TID 394) in 31 ms on localhost (executor driver) (48/175)
2017-11-03 09:43:24,142 INFO[org.apache.spark.executor.Executor:54] - Running task 49.0 in stage 3.0 (TID 399)
2017-11-03 09:43:24,142 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,142 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,158 INFO[org.apache.spark.executor.Executor:54] - Finished task 49.0 in stage 3.0 (TID 399). 1069 bytes result sent to driver
2017-11-03 09:43:24,158 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 50.0 in stage 3.0 (TID 400, localhost, executor driver, partition 50, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,158 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 49.0 in stage 3.0 (TID 399) in 16 ms on localhost (executor driver) (49/175)
2017-11-03 09:43:24,158 INFO[org.apache.spark.executor.Executor:54] - Running task 50.0 in stage 3.0 (TID 400)
2017-11-03 09:43:24,158 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,158 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,158 INFO[org.apache.spark.executor.Executor:54] - Finished task 50.0 in stage 3.0 (TID 400). 983 bytes result sent to driver
2017-11-03 09:43:24,158 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 51.0 in stage 3.0 (TID 401, localhost, executor driver, partition 51, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,158 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 50.0 in stage 3.0 (TID 400) in 0 ms on localhost (executor driver) (50/175)
2017-11-03 09:43:24,158 INFO[org.apache.spark.executor.Executor:54] - Running task 51.0 in stage 3.0 (TID 401)
2017-11-03 09:43:24,173 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,173 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,173 INFO[org.apache.spark.executor.Executor:54] - Finished task 51.0 in stage 3.0 (TID 401). 1069 bytes result sent to driver
2017-11-03 09:43:24,173 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 53.0 in stage 3.0 (TID 402, localhost, executor driver, partition 53, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,173 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 51.0 in stage 3.0 (TID 401) in 15 ms on localhost (executor driver) (51/175)
2017-11-03 09:43:24,173 INFO[org.apache.spark.executor.Executor:54] - Running task 53.0 in stage 3.0 (TID 402)
2017-11-03 09:43:24,173 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,173 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,173 INFO[org.apache.spark.executor.Executor:54] - Finished task 53.0 in stage 3.0 (TID 402). 983 bytes result sent to driver
2017-11-03 09:43:24,173 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 54.0 in stage 3.0 (TID 403, localhost, executor driver, partition 54, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,189 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 53.0 in stage 3.0 (TID 402) in 16 ms on localhost (executor driver) (52/175)
2017-11-03 09:43:24,189 INFO[org.apache.spark.executor.Executor:54] - Running task 54.0 in stage 3.0 (TID 403)
2017-11-03 09:43:24,189 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,189 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,189 INFO[org.apache.spark.executor.Executor:54] - Finished task 54.0 in stage 3.0 (TID 403). 983 bytes result sent to driver
2017-11-03 09:43:24,189 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 55.0 in stage 3.0 (TID 404, localhost, executor driver, partition 55, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,189 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 54.0 in stage 3.0 (TID 403) in 16 ms on localhost (executor driver) (53/175)
2017-11-03 09:43:24,189 INFO[org.apache.spark.executor.Executor:54] - Running task 55.0 in stage 3.0 (TID 404)
2017-11-03 09:43:24,189 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,189 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,252 INFO[org.apache.spark.executor.Executor:54] - Finished task 55.0 in stage 3.0 (TID 404). 1069 bytes result sent to driver
2017-11-03 09:43:24,252 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 56.0 in stage 3.0 (TID 405, localhost, executor driver, partition 56, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,252 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 55.0 in stage 3.0 (TID 404) in 63 ms on localhost (executor driver) (54/175)
2017-11-03 09:43:24,252 INFO[org.apache.spark.executor.Executor:54] - Running task 56.0 in stage 3.0 (TID 405)
2017-11-03 09:43:24,252 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,252 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,267 INFO[org.apache.spark.executor.Executor:54] - Finished task 56.0 in stage 3.0 (TID 405). 1069 bytes result sent to driver
2017-11-03 09:43:24,267 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 57.0 in stage 3.0 (TID 406, localhost, executor driver, partition 57, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,267 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 56.0 in stage 3.0 (TID 405) in 15 ms on localhost (executor driver) (55/175)
2017-11-03 09:43:24,267 INFO[org.apache.spark.executor.Executor:54] - Running task 57.0 in stage 3.0 (TID 406)
2017-11-03 09:43:24,267 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,267 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,267 INFO[org.apache.spark.executor.Executor:54] - Finished task 57.0 in stage 3.0 (TID 406). 983 bytes result sent to driver
2017-11-03 09:43:24,267 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 58.0 in stage 3.0 (TID 407, localhost, executor driver, partition 58, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,267 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 57.0 in stage 3.0 (TID 406) in 0 ms on localhost (executor driver) (56/175)
2017-11-03 09:43:24,267 INFO[org.apache.spark.executor.Executor:54] - Running task 58.0 in stage 3.0 (TID 407)
2017-11-03 09:43:24,283 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,283 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,283 INFO[org.apache.spark.executor.Executor:54] - Finished task 58.0 in stage 3.0 (TID 407). 1069 bytes result sent to driver
2017-11-03 09:43:24,283 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 59.0 in stage 3.0 (TID 408, localhost, executor driver, partition 59, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,283 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 58.0 in stage 3.0 (TID 407) in 16 ms on localhost (executor driver) (57/175)
2017-11-03 09:43:24,283 INFO[org.apache.spark.executor.Executor:54] - Running task 59.0 in stage 3.0 (TID 408)
2017-11-03 09:43:24,283 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,283 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,283 INFO[org.apache.spark.executor.Executor:54] - Finished task 59.0 in stage 3.0 (TID 408). 983 bytes result sent to driver
2017-11-03 09:43:24,283 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 60.0 in stage 3.0 (TID 409, localhost, executor driver, partition 60, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,298 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 59.0 in stage 3.0 (TID 408) in 15 ms on localhost (executor driver) (58/175)
2017-11-03 09:43:24,298 INFO[org.apache.spark.executor.Executor:54] - Running task 60.0 in stage 3.0 (TID 409)
2017-11-03 09:43:24,298 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,298 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,298 INFO[org.apache.spark.executor.Executor:54] - Finished task 60.0 in stage 3.0 (TID 409). 983 bytes result sent to driver
2017-11-03 09:43:24,298 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 61.0 in stage 3.0 (TID 410, localhost, executor driver, partition 61, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,298 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 60.0 in stage 3.0 (TID 409) in 15 ms on localhost (executor driver) (59/175)
2017-11-03 09:43:24,298 INFO[org.apache.spark.executor.Executor:54] - Running task 61.0 in stage 3.0 (TID 410)
2017-11-03 09:43:24,298 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,298 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,314 INFO[org.apache.spark.executor.Executor:54] - Finished task 61.0 in stage 3.0 (TID 410). 1069 bytes result sent to driver
2017-11-03 09:43:24,314 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 62.0 in stage 3.0 (TID 411, localhost, executor driver, partition 62, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,314 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 61.0 in stage 3.0 (TID 410) in 16 ms on localhost (executor driver) (60/175)
2017-11-03 09:43:24,314 INFO[org.apache.spark.executor.Executor:54] - Running task 62.0 in stage 3.0 (TID 411)
2017-11-03 09:43:24,314 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,314 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,314 INFO[org.apache.spark.executor.Executor:54] - Finished task 62.0 in stage 3.0 (TID 411). 983 bytes result sent to driver
2017-11-03 09:43:24,314 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 63.0 in stage 3.0 (TID 412, localhost, executor driver, partition 63, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,314 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 62.0 in stage 3.0 (TID 411) in 0 ms on localhost (executor driver) (61/175)
2017-11-03 09:43:24,314 INFO[org.apache.spark.executor.Executor:54] - Running task 63.0 in stage 3.0 (TID 412)
2017-11-03 09:43:24,314 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,314 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,330 INFO[org.apache.spark.executor.Executor:54] - Finished task 63.0 in stage 3.0 (TID 412). 1026 bytes result sent to driver
2017-11-03 09:43:24,330 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 64.0 in stage 3.0 (TID 413, localhost, executor driver, partition 64, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,330 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 63.0 in stage 3.0 (TID 412) in 16 ms on localhost (executor driver) (62/175)
2017-11-03 09:43:24,330 INFO[org.apache.spark.executor.Executor:54] - Running task 64.0 in stage 3.0 (TID 413)
2017-11-03 09:43:24,345 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,345 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,348 INFO[org.apache.spark.executor.Executor:54] - Finished task 64.0 in stage 3.0 (TID 413). 1069 bytes result sent to driver
2017-11-03 09:43:24,348 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 65.0 in stage 3.0 (TID 414, localhost, executor driver, partition 65, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,348 INFO[org.apache.spark.executor.Executor:54] - Running task 65.0 in stage 3.0 (TID 414)
2017-11-03 09:43:24,348 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 64.0 in stage 3.0 (TID 413) in 18 ms on localhost (executor driver) (63/175)
2017-11-03 09:43:24,348 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,348 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,348 INFO[org.apache.spark.executor.Executor:54] - Finished task 65.0 in stage 3.0 (TID 414). 983 bytes result sent to driver
2017-11-03 09:43:24,348 INFO[org.apache.spark.executor.Executor:54] - Finished task 48.0 in stage 3.0 (TID 398). 1026 bytes result sent to driver
2017-11-03 09:43:24,348 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 66.0 in stage 3.0 (TID 415, localhost, executor driver, partition 66, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,348 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 67.0 in stage 3.0 (TID 416, localhost, executor driver, partition 67, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,348 INFO[org.apache.spark.executor.Executor:54] - Running task 66.0 in stage 3.0 (TID 415)
2017-11-03 09:43:24,348 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 65.0 in stage 3.0 (TID 414) in 0 ms on localhost (executor driver) (64/175)
2017-11-03 09:43:24,348 INFO[org.apache.spark.executor.Executor:54] - Running task 67.0 in stage 3.0 (TID 416)
2017-11-03 09:43:24,348 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,348 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,348 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,348 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,348 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 48.0 in stage 3.0 (TID 398) in 206 ms on localhost (executor driver) (65/175)
2017-11-03 09:43:24,366 INFO[org.apache.spark.executor.Executor:54] - Finished task 67.0 in stage 3.0 (TID 416). 1069 bytes result sent to driver
2017-11-03 09:43:24,367 INFO[org.apache.spark.executor.Executor:54] - Finished task 66.0 in stage 3.0 (TID 415). 1069 bytes result sent to driver
2017-11-03 09:43:24,371 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 68.0 in stage 3.0 (TID 417, localhost, executor driver, partition 68, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,371 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 69.0 in stage 3.0 (TID 418, localhost, executor driver, partition 69, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,372 INFO[org.apache.spark.executor.Executor:54] - Running task 68.0 in stage 3.0 (TID 417)
2017-11-03 09:43:24,372 INFO[org.apache.spark.executor.Executor:54] - Running task 69.0 in stage 3.0 (TID 418)
2017-11-03 09:43:24,372 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 67.0 in stage 3.0 (TID 416) in 24 ms on localhost (executor driver) (66/175)
2017-11-03 09:43:24,373 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,374 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2017-11-03 09:43:24,374 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,375 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2017-11-03 09:43:24,376 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 66.0 in stage 3.0 (TID 415) in 28 ms on localhost (executor driver) (67/175)
2017-11-03 09:43:24,377 INFO[org.apache.spark.executor.Executor:54] - Finished task 68.0 in stage 3.0 (TID 417). 1069 bytes result sent to driver
2017-11-03 09:43:24,377 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 70.0 in stage 3.0 (TID 419, localhost, executor driver, partition 70, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,379 INFO[org.apache.spark.executor.Executor:54] - Finished task 69.0 in stage 3.0 (TID 418). 1112 bytes result sent to driver
2017-11-03 09:43:24,380 INFO[org.apache.spark.executor.Executor:54] - Running task 70.0 in stage 3.0 (TID 419)
2017-11-03 09:43:24,380 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 68.0 in stage 3.0 (TID 417) in 10 ms on localhost (executor driver) (68/175)
2017-11-03 09:43:24,380 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 71.0 in stage 3.0 (TID 420, localhost, executor driver, partition 71, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,381 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 69.0 in stage 3.0 (TID 418) in 10 ms on localhost (executor driver) (69/175)
2017-11-03 09:43:24,381 INFO[org.apache.spark.executor.Executor:54] - Running task 71.0 in stage 3.0 (TID 420)
2017-11-03 09:43:24,381 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,381 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,382 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,383 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2017-11-03 09:43:24,385 INFO[org.apache.spark.executor.Executor:54] - Finished task 70.0 in stage 3.0 (TID 419). 1069 bytes result sent to driver
2017-11-03 09:43:24,386 INFO[org.apache.spark.executor.Executor:54] - Finished task 71.0 in stage 3.0 (TID 420). 1069 bytes result sent to driver
2017-11-03 09:43:24,386 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 72.0 in stage 3.0 (TID 421, localhost, executor driver, partition 72, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,387 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 73.0 in stage 3.0 (TID 422, localhost, executor driver, partition 73, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,387 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 70.0 in stage 3.0 (TID 419) in 10 ms on localhost (executor driver) (70/175)
2017-11-03 09:43:24,388 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 71.0 in stage 3.0 (TID 420) in 8 ms on localhost (executor driver) (71/175)
2017-11-03 09:43:24,388 INFO[org.apache.spark.executor.Executor:54] - Running task 72.0 in stage 3.0 (TID 421)
2017-11-03 09:43:24,391 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,392 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2017-11-03 09:43:24,393 INFO[org.apache.spark.executor.Executor:54] - Running task 73.0 in stage 3.0 (TID 422)
2017-11-03 09:43:24,394 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,394 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,394 INFO[org.apache.spark.executor.Executor:54] - Finished task 73.0 in stage 3.0 (TID 422). 1026 bytes result sent to driver
2017-11-03 09:43:24,394 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 74.0 in stage 3.0 (TID 423, localhost, executor driver, partition 74, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,394 INFO[org.apache.spark.executor.Executor:54] - Finished task 72.0 in stage 3.0 (TID 421). 1112 bytes result sent to driver
2017-11-03 09:43:24,394 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 73.0 in stage 3.0 (TID 422) in 7 ms on localhost (executor driver) (72/175)
2017-11-03 09:43:24,394 INFO[org.apache.spark.executor.Executor:54] - Running task 74.0 in stage 3.0 (TID 423)
2017-11-03 09:43:24,394 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,394 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,394 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 75.0 in stage 3.0 (TID 424, localhost, executor driver, partition 75, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,394 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 72.0 in stage 3.0 (TID 421) in 8 ms on localhost (executor driver) (73/175)
2017-11-03 09:43:24,394 INFO[org.apache.spark.executor.Executor:54] - Running task 75.0 in stage 3.0 (TID 424)
2017-11-03 09:43:24,410 INFO[org.apache.spark.executor.Executor:54] - Finished task 74.0 in stage 3.0 (TID 423). 1069 bytes result sent to driver
2017-11-03 09:43:24,426 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,426 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,426 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 76.0 in stage 3.0 (TID 425, localhost, executor driver, partition 76, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,426 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 74.0 in stage 3.0 (TID 423) in 32 ms on localhost (executor driver) (74/175)
2017-11-03 09:43:24,426 INFO[org.apache.spark.executor.Executor:54] - Running task 76.0 in stage 3.0 (TID 425)
2017-11-03 09:43:24,426 INFO[org.apache.spark.executor.Executor:54] - Finished task 75.0 in stage 3.0 (TID 424). 1069 bytes result sent to driver
2017-11-03 09:43:24,426 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 77.0 in stage 3.0 (TID 426, localhost, executor driver, partition 77, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,426 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,426 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,426 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 75.0 in stage 3.0 (TID 424) in 32 ms on localhost (executor driver) (75/175)
2017-11-03 09:43:24,426 INFO[org.apache.spark.executor.Executor:54] - Running task 77.0 in stage 3.0 (TID 426)
2017-11-03 09:43:24,426 INFO[org.apache.spark.executor.Executor:54] - Finished task 76.0 in stage 3.0 (TID 425). 983 bytes result sent to driver
2017-11-03 09:43:24,426 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,426 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,426 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 78.0 in stage 3.0 (TID 427, localhost, executor driver, partition 78, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,426 INFO[org.apache.spark.executor.Executor:54] - Running task 78.0 in stage 3.0 (TID 427)
2017-11-03 09:43:24,426 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 76.0 in stage 3.0 (TID 425) in 0 ms on localhost (executor driver) (76/175)
2017-11-03 09:43:24,442 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,443 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2017-11-03 09:43:24,445 INFO[org.apache.spark.executor.Executor:54] - Finished task 77.0 in stage 3.0 (TID 426). 1026 bytes result sent to driver
2017-11-03 09:43:24,445 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 79.0 in stage 3.0 (TID 428, localhost, executor driver, partition 79, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,446 INFO[org.apache.spark.executor.Executor:54] - Finished task 78.0 in stage 3.0 (TID 427). 1069 bytes result sent to driver
2017-11-03 09:43:24,446 INFO[org.apache.spark.executor.Executor:54] - Running task 79.0 in stage 3.0 (TID 428)
2017-11-03 09:43:24,446 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 77.0 in stage 3.0 (TID 426) in 20 ms on localhost (executor driver) (77/175)
2017-11-03 09:43:24,446 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 80.0 in stage 3.0 (TID 429, localhost, executor driver, partition 80, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,447 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 78.0 in stage 3.0 (TID 427) in 21 ms on localhost (executor driver) (78/175)
2017-11-03 09:43:24,447 INFO[org.apache.spark.executor.Executor:54] - Running task 80.0 in stage 3.0 (TID 429)
2017-11-03 09:43:24,447 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,447 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,448 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,448 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,450 INFO[org.apache.spark.executor.Executor:54] - Finished task 79.0 in stage 3.0 (TID 428). 1069 bytes result sent to driver
2017-11-03 09:43:24,450 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 81.0 in stage 3.0 (TID 430, localhost, executor driver, partition 81, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,451 INFO[org.apache.spark.executor.Executor:54] - Running task 81.0 in stage 3.0 (TID 430)
2017-11-03 09:43:24,451 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 79.0 in stage 3.0 (TID 428) in 6 ms on localhost (executor driver) (79/175)
2017-11-03 09:43:24,451 INFO[org.apache.spark.executor.Executor:54] - Finished task 80.0 in stage 3.0 (TID 429). 1069 bytes result sent to driver
2017-11-03 09:43:24,452 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 82.0 in stage 3.0 (TID 431, localhost, executor driver, partition 82, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,453 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,453 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,453 INFO[org.apache.spark.executor.Executor:54] - Running task 82.0 in stage 3.0 (TID 431)
2017-11-03 09:43:24,453 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 80.0 in stage 3.0 (TID 429) in 7 ms on localhost (executor driver) (80/175)
2017-11-03 09:43:24,454 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,454 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,459 INFO[org.apache.spark.executor.Executor:54] - Finished task 82.0 in stage 3.0 (TID 431). 1069 bytes result sent to driver
2017-11-03 09:43:24,460 INFO[org.apache.spark.executor.Executor:54] - Finished task 81.0 in stage 3.0 (TID 430). 1112 bytes result sent to driver
2017-11-03 09:43:24,461 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 83.0 in stage 3.0 (TID 432, localhost, executor driver, partition 83, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,462 INFO[org.apache.spark.executor.Executor:54] - Running task 83.0 in stage 3.0 (TID 432)
2017-11-03 09:43:24,462 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 84.0 in stage 3.0 (TID 433, localhost, executor driver, partition 84, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,462 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 82.0 in stage 3.0 (TID 431) in 10 ms on localhost (executor driver) (81/175)
2017-11-03 09:43:24,462 INFO[org.apache.spark.executor.Executor:54] - Running task 84.0 in stage 3.0 (TID 433)
2017-11-03 09:43:24,463 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,463 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,463 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,464 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2017-11-03 09:43:24,465 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 81.0 in stage 3.0 (TID 430) in 15 ms on localhost (executor driver) (82/175)
2017-11-03 09:43:24,466 INFO[org.apache.spark.executor.Executor:54] - Finished task 83.0 in stage 3.0 (TID 432). 1069 bytes result sent to driver
2017-11-03 09:43:24,467 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 85.0 in stage 3.0 (TID 434, localhost, executor driver, partition 85, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,467 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 83.0 in stage 3.0 (TID 432) in 6 ms on localhost (executor driver) (83/175)
2017-11-03 09:43:24,468 INFO[org.apache.spark.executor.Executor:54] - Running task 85.0 in stage 3.0 (TID 434)
2017-11-03 09:43:24,468 INFO[org.apache.spark.executor.Executor:54] - Finished task 84.0 in stage 3.0 (TID 433). 1069 bytes result sent to driver
2017-11-03 09:43:24,469 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 86.0 in stage 3.0 (TID 435, localhost, executor driver, partition 86, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,469 INFO[org.apache.spark.executor.Executor:54] - Running task 86.0 in stage 3.0 (TID 435)
2017-11-03 09:43:24,469 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 84.0 in stage 3.0 (TID 433) in 7 ms on localhost (executor driver) (84/175)
2017-11-03 09:43:24,469 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,470 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2017-11-03 09:43:24,470 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,470 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,472 INFO[org.apache.spark.executor.Executor:54] - Finished task 85.0 in stage 3.0 (TID 434). 1112 bytes result sent to driver
2017-11-03 09:43:24,472 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 87.0 in stage 3.0 (TID 436, localhost, executor driver, partition 87, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,473 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 85.0 in stage 3.0 (TID 434) in 7 ms on localhost (executor driver) (85/175)
2017-11-03 09:43:24,473 INFO[org.apache.spark.executor.Executor:54] - Running task 87.0 in stage 3.0 (TID 436)
2017-11-03 09:43:24,473 INFO[org.apache.spark.executor.Executor:54] - Finished task 86.0 in stage 3.0 (TID 435). 1069 bytes result sent to driver
2017-11-03 09:43:24,474 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,474 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,474 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 88.0 in stage 3.0 (TID 437, localhost, executor driver, partition 88, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,475 INFO[org.apache.spark.executor.Executor:54] - Running task 88.0 in stage 3.0 (TID 437)
2017-11-03 09:43:24,475 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 86.0 in stage 3.0 (TID 435) in 6 ms on localhost (executor driver) (86/175)
2017-11-03 09:43:24,476 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,476 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,477 INFO[org.apache.spark.executor.Executor:54] - Finished task 87.0 in stage 3.0 (TID 436). 1069 bytes result sent to driver
2017-11-03 09:43:24,477 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 89.0 in stage 3.0 (TID 438, localhost, executor driver, partition 89, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,478 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 87.0 in stage 3.0 (TID 436) in 6 ms on localhost (executor driver) (87/175)
2017-11-03 09:43:24,478 INFO[org.apache.spark.executor.Executor:54] - Running task 89.0 in stage 3.0 (TID 438)
2017-11-03 09:43:24,479 INFO[org.apache.spark.executor.Executor:54] - Finished task 88.0 in stage 3.0 (TID 437). 1069 bytes result sent to driver
2017-11-03 09:43:24,480 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 90.0 in stage 3.0 (TID 439, localhost, executor driver, partition 90, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,480 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,480 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,480 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 88.0 in stage 3.0 (TID 437) in 6 ms on localhost (executor driver) (88/175)
2017-11-03 09:43:24,480 INFO[org.apache.spark.executor.Executor:54] - Running task 90.0 in stage 3.0 (TID 439)
2017-11-03 09:43:24,481 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,481 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,481 INFO[org.apache.spark.executor.Executor:54] - Finished task 89.0 in stage 3.0 (TID 438). 1069 bytes result sent to driver
2017-11-03 09:43:24,481 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 91.0 in stage 3.0 (TID 440, localhost, executor driver, partition 91, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,481 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 89.0 in stage 3.0 (TID 438) in 4 ms on localhost (executor driver) (89/175)
2017-11-03 09:43:24,481 INFO[org.apache.spark.executor.Executor:54] - Running task 91.0 in stage 3.0 (TID 440)
2017-11-03 09:43:24,481 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,481 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,481 INFO[org.apache.spark.executor.Executor:54] - Finished task 90.0 in stage 3.0 (TID 439). 1026 bytes result sent to driver
2017-11-03 09:43:24,481 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 92.0 in stage 3.0 (TID 441, localhost, executor driver, partition 92, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,481 INFO[org.apache.spark.executor.Executor:54] - Running task 92.0 in stage 3.0 (TID 441)
2017-11-03 09:43:24,481 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 90.0 in stage 3.0 (TID 439) in 1 ms on localhost (executor driver) (90/175)
2017-11-03 09:43:24,481 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,481 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,481 INFO[org.apache.spark.executor.Executor:54] - Finished task 91.0 in stage 3.0 (TID 440). 983 bytes result sent to driver
2017-11-03 09:43:24,481 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 93.0 in stage 3.0 (TID 442, localhost, executor driver, partition 93, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,481 INFO[org.apache.spark.executor.Executor:54] - Finished task 92.0 in stage 3.0 (TID 441). 983 bytes result sent to driver
2017-11-03 09:43:24,481 INFO[org.apache.spark.executor.Executor:54] - Running task 93.0 in stage 3.0 (TID 442)
2017-11-03 09:43:24,481 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,481 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 91.0 in stage 3.0 (TID 440) in 0 ms on localhost (executor driver) (91/175)
2017-11-03 09:43:24,481 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 94.0 in stage 3.0 (TID 443, localhost, executor driver, partition 94, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,481 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 92.0 in stage 3.0 (TID 441) in 0 ms on localhost (executor driver) (92/175)
2017-11-03 09:43:24,481 INFO[org.apache.spark.executor.Executor:54] - Running task 94.0 in stage 3.0 (TID 443)
2017-11-03 09:43:24,481 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,481 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,481 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,481 INFO[org.apache.spark.executor.Executor:54] - Finished task 94.0 in stage 3.0 (TID 443). 983 bytes result sent to driver
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 95.0 in stage 3.0 (TID 444, localhost, executor driver, partition 95, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,496 INFO[org.apache.spark.executor.Executor:54] - Finished task 93.0 in stage 3.0 (TID 442). 1069 bytes result sent to driver
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 94.0 in stage 3.0 (TID 443) in 15 ms on localhost (executor driver) (93/175)
2017-11-03 09:43:24,496 INFO[org.apache.spark.executor.Executor:54] - Running task 95.0 in stage 3.0 (TID 444)
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 96.0 in stage 3.0 (TID 445, localhost, executor driver, partition 96, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 93.0 in stage 3.0 (TID 442) in 15 ms on localhost (executor driver) (94/175)
2017-11-03 09:43:24,496 INFO[org.apache.spark.executor.Executor:54] - Running task 96.0 in stage 3.0 (TID 445)
2017-11-03 09:43:24,496 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,496 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,496 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,496 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,496 INFO[org.apache.spark.executor.Executor:54] - Finished task 95.0 in stage 3.0 (TID 444). 983 bytes result sent to driver
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 97.0 in stage 3.0 (TID 446, localhost, executor driver, partition 97, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 95.0 in stage 3.0 (TID 444) in 0 ms on localhost (executor driver) (95/175)
2017-11-03 09:43:24,496 INFO[org.apache.spark.executor.Executor:54] - Running task 97.0 in stage 3.0 (TID 446)
2017-11-03 09:43:24,496 INFO[org.apache.spark.executor.Executor:54] - Finished task 96.0 in stage 3.0 (TID 445). 983 bytes result sent to driver
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 98.0 in stage 3.0 (TID 447, localhost, executor driver, partition 98, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,496 INFO[org.apache.spark.executor.Executor:54] - Running task 98.0 in stage 3.0 (TID 447)
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 96.0 in stage 3.0 (TID 445) in 0 ms on localhost (executor driver) (96/175)
2017-11-03 09:43:24,496 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,496 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,496 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,496 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,496 INFO[org.apache.spark.executor.Executor:54] - Finished task 97.0 in stage 3.0 (TID 446). 983 bytes result sent to driver
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 99.0 in stage 3.0 (TID 448, localhost, executor driver, partition 99, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,496 INFO[org.apache.spark.executor.Executor:54] - Finished task 98.0 in stage 3.0 (TID 447). 983 bytes result sent to driver
2017-11-03 09:43:24,496 INFO[org.apache.spark.executor.Executor:54] - Running task 99.0 in stage 3.0 (TID 448)
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 97.0 in stage 3.0 (TID 446) in 0 ms on localhost (executor driver) (97/175)
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 100.0 in stage 3.0 (TID 449, localhost, executor driver, partition 100, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 98.0 in stage 3.0 (TID 447) in 0 ms on localhost (executor driver) (98/175)
2017-11-03 09:43:24,496 INFO[org.apache.spark.executor.Executor:54] - Running task 100.0 in stage 3.0 (TID 449)
2017-11-03 09:43:24,496 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,496 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,496 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,496 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,496 INFO[org.apache.spark.executor.Executor:54] - Finished task 99.0 in stage 3.0 (TID 448). 983 bytes result sent to driver
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 101.0 in stage 3.0 (TID 450, localhost, executor driver, partition 101, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,496 INFO[org.apache.spark.executor.Executor:54] - Running task 101.0 in stage 3.0 (TID 450)
2017-11-03 09:43:24,496 INFO[org.apache.spark.executor.Executor:54] - Finished task 100.0 in stage 3.0 (TID 449). 983 bytes result sent to driver
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 99.0 in stage 3.0 (TID 448) in 0 ms on localhost (executor driver) (99/175)
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 102.0 in stage 3.0 (TID 451, localhost, executor driver, partition 102, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,496 INFO[org.apache.spark.executor.Executor:54] - Running task 102.0 in stage 3.0 (TID 451)
2017-11-03 09:43:24,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 100.0 in stage 3.0 (TID 449) in 0 ms on localhost (executor driver) (100/175)
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,512 INFO[org.apache.spark.executor.Executor:54] - Finished task 101.0 in stage 3.0 (TID 450). 1069 bytes result sent to driver
2017-11-03 09:43:24,512 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 103.0 in stage 3.0 (TID 452, localhost, executor driver, partition 103, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,512 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 101.0 in stage 3.0 (TID 450) in 16 ms on localhost (executor driver) (101/175)
2017-11-03 09:43:24,512 INFO[org.apache.spark.executor.Executor:54] - Running task 103.0 in stage 3.0 (TID 452)
2017-11-03 09:43:24,512 INFO[org.apache.spark.executor.Executor:54] - Finished task 102.0 in stage 3.0 (TID 451). 1069 bytes result sent to driver
2017-11-03 09:43:24,512 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 104.0 in stage 3.0 (TID 453, localhost, executor driver, partition 104, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,512 INFO[org.apache.spark.executor.Executor:54] - Running task 104.0 in stage 3.0 (TID 453)
2017-11-03 09:43:24,512 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 102.0 in stage 3.0 (TID 451) in 16 ms on localhost (executor driver) (102/175)
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,512 INFO[org.apache.spark.executor.Executor:54] - Finished task 103.0 in stage 3.0 (TID 452). 983 bytes result sent to driver
2017-11-03 09:43:24,512 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 105.0 in stage 3.0 (TID 454, localhost, executor driver, partition 105, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,512 INFO[org.apache.spark.executor.Executor:54] - Running task 105.0 in stage 3.0 (TID 454)
2017-11-03 09:43:24,512 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 103.0 in stage 3.0 (TID 452) in 0 ms on localhost (executor driver) (103/175)
2017-11-03 09:43:24,512 INFO[org.apache.spark.executor.Executor:54] - Finished task 104.0 in stage 3.0 (TID 453). 983 bytes result sent to driver
2017-11-03 09:43:24,512 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 106.0 in stage 3.0 (TID 455, localhost, executor driver, partition 106, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,512 INFO[org.apache.spark.executor.Executor:54] - Running task 106.0 in stage 3.0 (TID 455)
2017-11-03 09:43:24,512 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 104.0 in stage 3.0 (TID 453) in 0 ms on localhost (executor driver) (104/175)
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,512 INFO[org.apache.spark.executor.Executor:54] - Finished task 105.0 in stage 3.0 (TID 454). 983 bytes result sent to driver
2017-11-03 09:43:24,512 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 107.0 in stage 3.0 (TID 456, localhost, executor driver, partition 107, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,512 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 105.0 in stage 3.0 (TID 454) in 0 ms on localhost (executor driver) (105/175)
2017-11-03 09:43:24,512 INFO[org.apache.spark.executor.Executor:54] - Running task 107.0 in stage 3.0 (TID 456)
2017-11-03 09:43:24,512 INFO[org.apache.spark.executor.Executor:54] - Finished task 106.0 in stage 3.0 (TID 455). 983 bytes result sent to driver
2017-11-03 09:43:24,512 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 108.0 in stage 3.0 (TID 457, localhost, executor driver, partition 108, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,512 INFO[org.apache.spark.executor.Executor:54] - Running task 108.0 in stage 3.0 (TID 457)
2017-11-03 09:43:24,512 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 106.0 in stage 3.0 (TID 455) in 0 ms on localhost (executor driver) (106/175)
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,512 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,512 INFO[org.apache.spark.executor.Executor:54] - Finished task 107.0 in stage 3.0 (TID 456). 983 bytes result sent to driver
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 109.0 in stage 3.0 (TID 458, localhost, executor driver, partition 109, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,528 INFO[org.apache.spark.executor.Executor:54] - Running task 109.0 in stage 3.0 (TID 458)
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 107.0 in stage 3.0 (TID 456) in 16 ms on localhost (executor driver) (107/175)
2017-11-03 09:43:24,528 INFO[org.apache.spark.executor.Executor:54] - Finished task 108.0 in stage 3.0 (TID 457). 1069 bytes result sent to driver
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 110.0 in stage 3.0 (TID 459, localhost, executor driver, partition 110, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,528 INFO[org.apache.spark.executor.Executor:54] - Running task 110.0 in stage 3.0 (TID 459)
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 108.0 in stage 3.0 (TID 457) in 16 ms on localhost (executor driver) (108/175)
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,528 INFO[org.apache.spark.executor.Executor:54] - Finished task 109.0 in stage 3.0 (TID 458). 983 bytes result sent to driver
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 111.0 in stage 3.0 (TID 460, localhost, executor driver, partition 111, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,528 INFO[org.apache.spark.executor.Executor:54] - Running task 111.0 in stage 3.0 (TID 460)
2017-11-03 09:43:24,528 INFO[org.apache.spark.executor.Executor:54] - Finished task 110.0 in stage 3.0 (TID 459). 983 bytes result sent to driver
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 109.0 in stage 3.0 (TID 458) in 16 ms on localhost (executor driver) (109/175)
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 112.0 in stage 3.0 (TID 461, localhost, executor driver, partition 112, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,528 INFO[org.apache.spark.executor.Executor:54] - Running task 112.0 in stage 3.0 (TID 461)
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 110.0 in stage 3.0 (TID 459) in 0 ms on localhost (executor driver) (110/175)
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,528 INFO[org.apache.spark.executor.Executor:54] - Finished task 111.0 in stage 3.0 (TID 460). 983 bytes result sent to driver
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 113.0 in stage 3.0 (TID 462, localhost, executor driver, partition 113, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,528 INFO[org.apache.spark.executor.Executor:54] - Running task 113.0 in stage 3.0 (TID 462)
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 111.0 in stage 3.0 (TID 460) in 0 ms on localhost (executor driver) (111/175)
2017-11-03 09:43:24,528 INFO[org.apache.spark.executor.Executor:54] - Finished task 112.0 in stage 3.0 (TID 461). 983 bytes result sent to driver
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 114.0 in stage 3.0 (TID 463, localhost, executor driver, partition 114, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,528 INFO[org.apache.spark.executor.Executor:54] - Running task 114.0 in stage 3.0 (TID 463)
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 112.0 in stage 3.0 (TID 461) in 0 ms on localhost (executor driver) (112/175)
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,528 INFO[org.apache.spark.executor.Executor:54] - Finished task 113.0 in stage 3.0 (TID 462). 983 bytes result sent to driver
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 115.0 in stage 3.0 (TID 464, localhost, executor driver, partition 115, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,528 INFO[org.apache.spark.executor.Executor:54] - Running task 115.0 in stage 3.0 (TID 464)
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 113.0 in stage 3.0 (TID 462) in 0 ms on localhost (executor driver) (113/175)
2017-11-03 09:43:24,528 INFO[org.apache.spark.executor.Executor:54] - Finished task 114.0 in stage 3.0 (TID 463). 983 bytes result sent to driver
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 116.0 in stage 3.0 (TID 465, localhost, executor driver, partition 116, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,528 INFO[org.apache.spark.executor.Executor:54] - Running task 116.0 in stage 3.0 (TID 465)
2017-11-03 09:43:24,528 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 114.0 in stage 3.0 (TID 463) in 0 ms on localhost (executor driver) (114/175)
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,543 INFO[org.apache.spark.executor.Executor:54] - Finished task 115.0 in stage 3.0 (TID 464). 1069 bytes result sent to driver
2017-11-03 09:43:24,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 117.0 in stage 3.0 (TID 466, localhost, executor driver, partition 117, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,543 INFO[org.apache.spark.executor.Executor:54] - Running task 117.0 in stage 3.0 (TID 466)
2017-11-03 09:43:24,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 115.0 in stage 3.0 (TID 464) in 15 ms on localhost (executor driver) (115/175)
2017-11-03 09:43:24,543 INFO[org.apache.spark.executor.Executor:54] - Finished task 116.0 in stage 3.0 (TID 465). 1069 bytes result sent to driver
2017-11-03 09:43:24,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 118.0 in stage 3.0 (TID 467, localhost, executor driver, partition 118, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,543 INFO[org.apache.spark.executor.Executor:54] - Running task 118.0 in stage 3.0 (TID 467)
2017-11-03 09:43:24,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 116.0 in stage 3.0 (TID 465) in 15 ms on localhost (executor driver) (116/175)
2017-11-03 09:43:24,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,543 INFO[org.apache.spark.executor.Executor:54] - Finished task 117.0 in stage 3.0 (TID 466). 983 bytes result sent to driver
2017-11-03 09:43:24,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 119.0 in stage 3.0 (TID 468, localhost, executor driver, partition 119, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,543 INFO[org.apache.spark.executor.Executor:54] - Running task 119.0 in stage 3.0 (TID 468)
2017-11-03 09:43:24,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 117.0 in stage 3.0 (TID 466) in 0 ms on localhost (executor driver) (117/175)
2017-11-03 09:43:24,543 INFO[org.apache.spark.executor.Executor:54] - Finished task 118.0 in stage 3.0 (TID 467). 983 bytes result sent to driver
2017-11-03 09:43:24,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 120.0 in stage 3.0 (TID 469, localhost, executor driver, partition 120, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,543 INFO[org.apache.spark.executor.Executor:54] - Running task 120.0 in stage 3.0 (TID 469)
2017-11-03 09:43:24,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 118.0 in stage 3.0 (TID 467) in 0 ms on localhost (executor driver) (118/175)
2017-11-03 09:43:24,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,543 INFO[org.apache.spark.executor.Executor:54] - Finished task 119.0 in stage 3.0 (TID 468). 983 bytes result sent to driver
2017-11-03 09:43:24,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 121.0 in stage 3.0 (TID 470, localhost, executor driver, partition 121, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 119.0 in stage 3.0 (TID 468) in 0 ms on localhost (executor driver) (119/175)
2017-11-03 09:43:24,543 INFO[org.apache.spark.executor.Executor:54] - Running task 121.0 in stage 3.0 (TID 470)
2017-11-03 09:43:24,543 INFO[org.apache.spark.executor.Executor:54] - Finished task 120.0 in stage 3.0 (TID 469). 983 bytes result sent to driver
2017-11-03 09:43:24,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 122.0 in stage 3.0 (TID 471, localhost, executor driver, partition 122, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,543 INFO[org.apache.spark.executor.Executor:54] - Running task 122.0 in stage 3.0 (TID 471)
2017-11-03 09:43:24,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 120.0 in stage 3.0 (TID 469) in 0 ms on localhost (executor driver) (120/175)
2017-11-03 09:43:24,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,543 INFO[org.apache.spark.executor.Executor:54] - Finished task 122.0 in stage 3.0 (TID 471). 983 bytes result sent to driver
2017-11-03 09:43:24,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 123.0 in stage 3.0 (TID 472, localhost, executor driver, partition 123, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,559 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 122.0 in stage 3.0 (TID 471) in 16 ms on localhost (executor driver) (121/175)
2017-11-03 09:43:24,559 INFO[org.apache.spark.executor.Executor:54] - Finished task 121.0 in stage 3.0 (TID 470). 1069 bytes result sent to driver
2017-11-03 09:43:24,559 INFO[org.apache.spark.executor.Executor:54] - Running task 123.0 in stage 3.0 (TID 472)
2017-11-03 09:43:24,559 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 124.0 in stage 3.0 (TID 473, localhost, executor driver, partition 124, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,559 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 121.0 in stage 3.0 (TID 470) in 16 ms on localhost (executor driver) (122/175)
2017-11-03 09:43:24,559 INFO[org.apache.spark.executor.Executor:54] - Running task 124.0 in stage 3.0 (TID 473)
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,559 INFO[org.apache.spark.executor.Executor:54] - Finished task 123.0 in stage 3.0 (TID 472). 983 bytes result sent to driver
2017-11-03 09:43:24,559 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 125.0 in stage 3.0 (TID 474, localhost, executor driver, partition 125, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,559 INFO[org.apache.spark.executor.Executor:54] - Running task 125.0 in stage 3.0 (TID 474)
2017-11-03 09:43:24,559 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 123.0 in stage 3.0 (TID 472) in 16 ms on localhost (executor driver) (123/175)
2017-11-03 09:43:24,559 INFO[org.apache.spark.executor.Executor:54] - Finished task 124.0 in stage 3.0 (TID 473). 983 bytes result sent to driver
2017-11-03 09:43:24,559 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 126.0 in stage 3.0 (TID 475, localhost, executor driver, partition 126, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,559 INFO[org.apache.spark.executor.Executor:54] - Running task 126.0 in stage 3.0 (TID 475)
2017-11-03 09:43:24,559 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 124.0 in stage 3.0 (TID 473) in 0 ms on localhost (executor driver) (124/175)
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,559 INFO[org.apache.spark.executor.Executor:54] - Finished task 125.0 in stage 3.0 (TID 474). 983 bytes result sent to driver
2017-11-03 09:43:24,559 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 127.0 in stage 3.0 (TID 476, localhost, executor driver, partition 127, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,559 INFO[org.apache.spark.executor.Executor:54] - Running task 127.0 in stage 3.0 (TID 476)
2017-11-03 09:43:24,559 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 125.0 in stage 3.0 (TID 474) in 0 ms on localhost (executor driver) (125/175)
2017-11-03 09:43:24,559 INFO[org.apache.spark.executor.Executor:54] - Finished task 126.0 in stage 3.0 (TID 475). 983 bytes result sent to driver
2017-11-03 09:43:24,559 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 128.0 in stage 3.0 (TID 477, localhost, executor driver, partition 128, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,559 INFO[org.apache.spark.executor.Executor:54] - Running task 128.0 in stage 3.0 (TID 477)
2017-11-03 09:43:24,559 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 126.0 in stage 3.0 (TID 475) in 0 ms on localhost (executor driver) (126/175)
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,559 INFO[org.apache.spark.executor.Executor:54] - Finished task 127.0 in stage 3.0 (TID 476). 983 bytes result sent to driver
2017-11-03 09:43:24,559 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 129.0 in stage 3.0 (TID 478, localhost, executor driver, partition 129, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,559 INFO[org.apache.spark.executor.Executor:54] - Running task 129.0 in stage 3.0 (TID 478)
2017-11-03 09:43:24,559 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 127.0 in stage 3.0 (TID 476) in 0 ms on localhost (executor driver) (127/175)
2017-11-03 09:43:24,559 INFO[org.apache.spark.executor.Executor:54] - Finished task 128.0 in stage 3.0 (TID 477). 983 bytes result sent to driver
2017-11-03 09:43:24,559 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 130.0 in stage 3.0 (TID 479, localhost, executor driver, partition 130, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,559 INFO[org.apache.spark.executor.Executor:54] - Running task 130.0 in stage 3.0 (TID 479)
2017-11-03 09:43:24,559 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 128.0 in stage 3.0 (TID 477) in 0 ms on localhost (executor driver) (128/175)
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,559 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,575 INFO[org.apache.spark.executor.Executor:54] - Finished task 129.0 in stage 3.0 (TID 478). 1069 bytes result sent to driver
2017-11-03 09:43:24,575 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 131.0 in stage 3.0 (TID 480, localhost, executor driver, partition 131, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,575 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 129.0 in stage 3.0 (TID 478) in 16 ms on localhost (executor driver) (129/175)
2017-11-03 09:43:24,575 INFO[org.apache.spark.executor.Executor:54] - Running task 131.0 in stage 3.0 (TID 480)
2017-11-03 09:43:24,575 INFO[org.apache.spark.executor.Executor:54] - Finished task 130.0 in stage 3.0 (TID 479). 1026 bytes result sent to driver
2017-11-03 09:43:24,575 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 132.0 in stage 3.0 (TID 481, localhost, executor driver, partition 132, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,575 INFO[org.apache.spark.executor.Executor:54] - Running task 132.0 in stage 3.0 (TID 481)
2017-11-03 09:43:24,575 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 130.0 in stage 3.0 (TID 479) in 16 ms on localhost (executor driver) (130/175)
2017-11-03 09:43:24,575 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,575 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,575 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,575 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,575 INFO[org.apache.spark.executor.Executor:54] - Finished task 131.0 in stage 3.0 (TID 480). 983 bytes result sent to driver
2017-11-03 09:43:24,575 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 133.0 in stage 3.0 (TID 482, localhost, executor driver, partition 133, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,575 INFO[org.apache.spark.executor.Executor:54] - Running task 133.0 in stage 3.0 (TID 482)
2017-11-03 09:43:24,575 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 131.0 in stage 3.0 (TID 480) in 0 ms on localhost (executor driver) (131/175)
2017-11-03 09:43:24,575 INFO[org.apache.spark.executor.Executor:54] - Finished task 132.0 in stage 3.0 (TID 481). 983 bytes result sent to driver
2017-11-03 09:43:24,575 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 134.0 in stage 3.0 (TID 483, localhost, executor driver, partition 134, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,575 INFO[org.apache.spark.executor.Executor:54] - Running task 134.0 in stage 3.0 (TID 483)
2017-11-03 09:43:24,575 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,575 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 132.0 in stage 3.0 (TID 481) in 0 ms on localhost (executor driver) (132/175)
2017-11-03 09:43:24,575 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,575 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,575 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,575 INFO[org.apache.spark.executor.Executor:54] - Finished task 133.0 in stage 3.0 (TID 482). 983 bytes result sent to driver
2017-11-03 09:43:24,575 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 135.0 in stage 3.0 (TID 484, localhost, executor driver, partition 135, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,575 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 133.0 in stage 3.0 (TID 482) in 0 ms on localhost (executor driver) (133/175)
2017-11-03 09:43:24,575 INFO[org.apache.spark.executor.Executor:54] - Running task 135.0 in stage 3.0 (TID 484)
2017-11-03 09:43:24,575 INFO[org.apache.spark.executor.Executor:54] - Finished task 134.0 in stage 3.0 (TID 483). 983 bytes result sent to driver
2017-11-03 09:43:24,575 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 136.0 in stage 3.0 (TID 485, localhost, executor driver, partition 136, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,575 INFO[org.apache.spark.executor.Executor:54] - Running task 136.0 in stage 3.0 (TID 485)
2017-11-03 09:43:24,575 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,575 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,575 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,575 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,575 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 134.0 in stage 3.0 (TID 483) in 0 ms on localhost (executor driver) (134/175)
2017-11-03 09:43:24,590 INFO[org.apache.spark.executor.Executor:54] - Finished task 135.0 in stage 3.0 (TID 484). 1069 bytes result sent to driver
2017-11-03 09:43:24,590 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 137.0 in stage 3.0 (TID 486, localhost, executor driver, partition 137, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,590 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 135.0 in stage 3.0 (TID 484) in 15 ms on localhost (executor driver) (135/175)
2017-11-03 09:43:24,590 INFO[org.apache.spark.executor.Executor:54] - Finished task 136.0 in stage 3.0 (TID 485). 1026 bytes result sent to driver
2017-11-03 09:43:24,590 INFO[org.apache.spark.executor.Executor:54] - Running task 137.0 in stage 3.0 (TID 486)
2017-11-03 09:43:24,590 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 138.0 in stage 3.0 (TID 487, localhost, executor driver, partition 138, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,590 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 136.0 in stage 3.0 (TID 485) in 15 ms on localhost (executor driver) (136/175)
2017-11-03 09:43:24,590 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,590 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,590 INFO[org.apache.spark.executor.Executor:54] - Running task 138.0 in stage 3.0 (TID 487)
2017-11-03 09:43:24,590 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,590 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,590 INFO[org.apache.spark.executor.Executor:54] - Finished task 137.0 in stage 3.0 (TID 486). 983 bytes result sent to driver
2017-11-03 09:43:24,590 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 139.0 in stage 3.0 (TID 488, localhost, executor driver, partition 139, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,590 INFO[org.apache.spark.executor.Executor:54] - Finished task 138.0 in stage 3.0 (TID 487). 983 bytes result sent to driver
2017-11-03 09:43:24,590 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 137.0 in stage 3.0 (TID 486) in 0 ms on localhost (executor driver) (137/175)
2017-11-03 09:43:24,590 INFO[org.apache.spark.executor.Executor:54] - Running task 139.0 in stage 3.0 (TID 488)
2017-11-03 09:43:24,590 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 140.0 in stage 3.0 (TID 489, localhost, executor driver, partition 140, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,590 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 138.0 in stage 3.0 (TID 487) in 0 ms on localhost (executor driver) (138/175)
2017-11-03 09:43:24,590 INFO[org.apache.spark.executor.Executor:54] - Running task 140.0 in stage 3.0 (TID 489)
2017-11-03 09:43:24,590 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,590 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,590 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,590 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,590 INFO[org.apache.spark.executor.Executor:54] - Finished task 139.0 in stage 3.0 (TID 488). 983 bytes result sent to driver
2017-11-03 09:43:24,590 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 141.0 in stage 3.0 (TID 490, localhost, executor driver, partition 141, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,590 INFO[org.apache.spark.executor.Executor:54] - Finished task 140.0 in stage 3.0 (TID 489). 983 bytes result sent to driver
2017-11-03 09:43:24,590 INFO[org.apache.spark.executor.Executor:54] - Running task 141.0 in stage 3.0 (TID 490)
2017-11-03 09:43:24,590 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 139.0 in stage 3.0 (TID 488) in 0 ms on localhost (executor driver) (139/175)
2017-11-03 09:43:24,590 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 142.0 in stage 3.0 (TID 491, localhost, executor driver, partition 142, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,590 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 140.0 in stage 3.0 (TID 489) in 0 ms on localhost (executor driver) (140/175)
2017-11-03 09:43:24,590 INFO[org.apache.spark.executor.Executor:54] - Running task 142.0 in stage 3.0 (TID 491)
2017-11-03 09:43:24,590 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,590 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,606 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,606 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,606 INFO[org.apache.spark.executor.Executor:54] - Finished task 141.0 in stage 3.0 (TID 490). 1069 bytes result sent to driver
2017-11-03 09:43:24,606 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 143.0 in stage 3.0 (TID 492, localhost, executor driver, partition 143, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,606 INFO[org.apache.spark.executor.Executor:54] - Running task 143.0 in stage 3.0 (TID 492)
2017-11-03 09:43:24,606 INFO[org.apache.spark.executor.Executor:54] - Finished task 142.0 in stage 3.0 (TID 491). 1069 bytes result sent to driver
2017-11-03 09:43:24,606 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 141.0 in stage 3.0 (TID 490) in 16 ms on localhost (executor driver) (141/175)
2017-11-03 09:43:24,606 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 144.0 in stage 3.0 (TID 493, localhost, executor driver, partition 144, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,606 INFO[org.apache.spark.executor.Executor:54] - Running task 144.0 in stage 3.0 (TID 493)
2017-11-03 09:43:24,606 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 142.0 in stage 3.0 (TID 491) in 16 ms on localhost (executor driver) (142/175)
2017-11-03 09:43:24,606 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,606 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,606 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,606 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,606 INFO[org.apache.spark.executor.Executor:54] - Finished task 143.0 in stage 3.0 (TID 492). 983 bytes result sent to driver
2017-11-03 09:43:24,606 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 145.0 in stage 3.0 (TID 494, localhost, executor driver, partition 145, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,606 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 143.0 in stage 3.0 (TID 492) in 0 ms on localhost (executor driver) (143/175)
2017-11-03 09:43:24,606 INFO[org.apache.spark.executor.Executor:54] - Running task 145.0 in stage 3.0 (TID 494)
2017-11-03 09:43:24,606 INFO[org.apache.spark.executor.Executor:54] - Finished task 144.0 in stage 3.0 (TID 493). 983 bytes result sent to driver
2017-11-03 09:43:24,606 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 146.0 in stage 3.0 (TID 495, localhost, executor driver, partition 146, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,606 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 144.0 in stage 3.0 (TID 493) in 0 ms on localhost (executor driver) (144/175)
2017-11-03 09:43:24,606 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,606 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,606 INFO[org.apache.spark.executor.Executor:54] - Running task 146.0 in stage 3.0 (TID 495)
2017-11-03 09:43:24,606 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,606 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,606 INFO[org.apache.spark.executor.Executor:54] - Finished task 145.0 in stage 3.0 (TID 494). 983 bytes result sent to driver
2017-11-03 09:43:24,606 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 147.0 in stage 3.0 (TID 496, localhost, executor driver, partition 147, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,606 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 145.0 in stage 3.0 (TID 494) in 0 ms on localhost (executor driver) (145/175)
2017-11-03 09:43:24,621 INFO[org.apache.spark.executor.Executor:54] - Finished task 146.0 in stage 3.0 (TID 495). 1069 bytes result sent to driver
2017-11-03 09:43:24,621 INFO[org.apache.spark.executor.Executor:54] - Running task 147.0 in stage 3.0 (TID 496)
2017-11-03 09:43:24,621 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,621 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,621 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 148.0 in stage 3.0 (TID 497, localhost, executor driver, partition 148, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,621 INFO[org.apache.spark.executor.Executor:54] - Finished task 147.0 in stage 3.0 (TID 496). 983 bytes result sent to driver
2017-11-03 09:43:24,621 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 149.0 in stage 3.0 (TID 498, localhost, executor driver, partition 149, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,621 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 146.0 in stage 3.0 (TID 495) in 15 ms on localhost (executor driver) (146/175)
2017-11-03 09:43:24,621 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 147.0 in stage 3.0 (TID 496) in 15 ms on localhost (executor driver) (147/175)
2017-11-03 09:43:24,621 INFO[org.apache.spark.executor.Executor:54] - Running task 148.0 in stage 3.0 (TID 497)
2017-11-03 09:43:24,621 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,621 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,621 INFO[org.apache.spark.executor.Executor:54] - Finished task 148.0 in stage 3.0 (TID 497). 983 bytes result sent to driver
2017-11-03 09:43:24,621 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 150.0 in stage 3.0 (TID 499, localhost, executor driver, partition 150, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,621 INFO[org.apache.spark.executor.Executor:54] - Running task 149.0 in stage 3.0 (TID 498)
2017-11-03 09:43:24,621 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 148.0 in stage 3.0 (TID 497) in 0 ms on localhost (executor driver) (148/175)
2017-11-03 09:43:24,637 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,637 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,637 INFO[org.apache.spark.executor.Executor:54] - Running task 150.0 in stage 3.0 (TID 499)
2017-11-03 09:43:24,637 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,637 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,637 INFO[org.apache.spark.executor.Executor:54] - Finished task 149.0 in stage 3.0 (TID 498). 1026 bytes result sent to driver
2017-11-03 09:43:24,637 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 151.0 in stage 3.0 (TID 500, localhost, executor driver, partition 151, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,637 INFO[org.apache.spark.executor.Executor:54] - Running task 151.0 in stage 3.0 (TID 500)
2017-11-03 09:43:24,637 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 149.0 in stage 3.0 (TID 498) in 16 ms on localhost (executor driver) (149/175)
2017-11-03 09:43:24,637 INFO[org.apache.spark.executor.Executor:54] - Finished task 150.0 in stage 3.0 (TID 499). 983 bytes result sent to driver
2017-11-03 09:43:24,637 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,637 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 152.0 in stage 3.0 (TID 501, localhost, executor driver, partition 152, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,637 INFO[org.apache.spark.executor.Executor:54] - Running task 152.0 in stage 3.0 (TID 501)
2017-11-03 09:43:24,637 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,637 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 150.0 in stage 3.0 (TID 499) in 16 ms on localhost (executor driver) (150/175)
2017-11-03 09:43:24,637 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,637 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,637 INFO[org.apache.spark.executor.Executor:54] - Finished task 151.0 in stage 3.0 (TID 500). 983 bytes result sent to driver
2017-11-03 09:43:24,637 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 153.0 in stage 3.0 (TID 502, localhost, executor driver, partition 153, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,637 INFO[org.apache.spark.executor.Executor:54] - Finished task 152.0 in stage 3.0 (TID 501). 983 bytes result sent to driver
2017-11-03 09:43:24,637 INFO[org.apache.spark.executor.Executor:54] - Running task 153.0 in stage 3.0 (TID 502)
2017-11-03 09:43:24,637 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 151.0 in stage 3.0 (TID 500) in 0 ms on localhost (executor driver) (151/175)
2017-11-03 09:43:24,637 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 154.0 in stage 3.0 (TID 503, localhost, executor driver, partition 154, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,637 INFO[org.apache.spark.executor.Executor:54] - Running task 154.0 in stage 3.0 (TID 503)
2017-11-03 09:43:24,637 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 152.0 in stage 3.0 (TID 501) in 0 ms on localhost (executor driver) (152/175)
2017-11-03 09:43:24,637 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,637 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,653 INFO[org.apache.spark.executor.Executor:54] - Finished task 153.0 in stage 3.0 (TID 502). 1069 bytes result sent to driver
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,653 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 155.0 in stage 3.0 (TID 504, localhost, executor driver, partition 155, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,653 INFO[org.apache.spark.executor.Executor:54] - Running task 155.0 in stage 3.0 (TID 504)
2017-11-03 09:43:24,653 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 153.0 in stage 3.0 (TID 502) in 16 ms on localhost (executor driver) (153/175)
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,653 INFO[org.apache.spark.executor.Executor:54] - Finished task 154.0 in stage 3.0 (TID 503). 1026 bytes result sent to driver
2017-11-03 09:43:24,653 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 156.0 in stage 3.0 (TID 505, localhost, executor driver, partition 156, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,653 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 154.0 in stage 3.0 (TID 503) in 16 ms on localhost (executor driver) (154/175)
2017-11-03 09:43:24,653 INFO[org.apache.spark.executor.Executor:54] - Running task 156.0 in stage 3.0 (TID 505)
2017-11-03 09:43:24,653 INFO[org.apache.spark.executor.Executor:54] - Finished task 155.0 in stage 3.0 (TID 504). 983 bytes result sent to driver
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,653 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 157.0 in stage 3.0 (TID 506, localhost, executor driver, partition 157, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,653 INFO[org.apache.spark.executor.Executor:54] - Running task 157.0 in stage 3.0 (TID 506)
2017-11-03 09:43:24,653 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 155.0 in stage 3.0 (TID 504) in 0 ms on localhost (executor driver) (155/175)
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,653 INFO[org.apache.spark.executor.Executor:54] - Finished task 156.0 in stage 3.0 (TID 505). 983 bytes result sent to driver
2017-11-03 09:43:24,653 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 158.0 in stage 3.0 (TID 507, localhost, executor driver, partition 158, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,653 INFO[org.apache.spark.executor.Executor:54] - Running task 158.0 in stage 3.0 (TID 507)
2017-11-03 09:43:24,653 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 156.0 in stage 3.0 (TID 505) in 0 ms on localhost (executor driver) (156/175)
2017-11-03 09:43:24,653 INFO[org.apache.spark.executor.Executor:54] - Finished task 157.0 in stage 3.0 (TID 506). 983 bytes result sent to driver
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,653 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 159.0 in stage 3.0 (TID 508, localhost, executor driver, partition 159, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,653 INFO[org.apache.spark.executor.Executor:54] - Running task 159.0 in stage 3.0 (TID 508)
2017-11-03 09:43:24,653 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 157.0 in stage 3.0 (TID 506) in 0 ms on localhost (executor driver) (157/175)
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,653 INFO[org.apache.spark.executor.Executor:54] - Finished task 158.0 in stage 3.0 (TID 507). 983 bytes result sent to driver
2017-11-03 09:43:24,653 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 160.0 in stage 3.0 (TID 509, localhost, executor driver, partition 160, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,653 INFO[org.apache.spark.executor.Executor:54] - Running task 160.0 in stage 3.0 (TID 509)
2017-11-03 09:43:24,653 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 158.0 in stage 3.0 (TID 507) in 0 ms on localhost (executor driver) (158/175)
2017-11-03 09:43:24,653 INFO[org.apache.spark.executor.Executor:54] - Finished task 159.0 in stage 3.0 (TID 508). 983 bytes result sent to driver
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,653 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 161.0 in stage 3.0 (TID 510, localhost, executor driver, partition 161, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,653 INFO[org.apache.spark.executor.Executor:54] - Running task 161.0 in stage 3.0 (TID 510)
2017-11-03 09:43:24,653 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 159.0 in stage 3.0 (TID 508) in 0 ms on localhost (executor driver) (159/175)
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,653 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,653 INFO[org.apache.spark.executor.Executor:54] - Finished task 160.0 in stage 3.0 (TID 509). 983 bytes result sent to driver
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 162.0 in stage 3.0 (TID 511, localhost, executor driver, partition 162, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 160.0 in stage 3.0 (TID 509) in 15 ms on localhost (executor driver) (160/175)
2017-11-03 09:43:24,668 INFO[org.apache.spark.executor.Executor:54] - Finished task 161.0 in stage 3.0 (TID 510). 1069 bytes result sent to driver
2017-11-03 09:43:24,668 INFO[org.apache.spark.executor.Executor:54] - Running task 162.0 in stage 3.0 (TID 511)
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 163.0 in stage 3.0 (TID 512, localhost, executor driver, partition 163, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,668 INFO[org.apache.spark.executor.Executor:54] - Running task 163.0 in stage 3.0 (TID 512)
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 161.0 in stage 3.0 (TID 510) in 15 ms on localhost (executor driver) (161/175)
2017-11-03 09:43:24,668 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,668 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,668 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,668 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,668 INFO[org.apache.spark.executor.Executor:54] - Finished task 162.0 in stage 3.0 (TID 511). 983 bytes result sent to driver
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 164.0 in stage 3.0 (TID 513, localhost, executor driver, partition 164, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,668 INFO[org.apache.spark.executor.Executor:54] - Finished task 163.0 in stage 3.0 (TID 512). 983 bytes result sent to driver
2017-11-03 09:43:24,668 INFO[org.apache.spark.executor.Executor:54] - Running task 164.0 in stage 3.0 (TID 513)
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 162.0 in stage 3.0 (TID 511) in 15 ms on localhost (executor driver) (162/175)
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 165.0 in stage 3.0 (TID 514, localhost, executor driver, partition 165, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,668 INFO[org.apache.spark.executor.Executor:54] - Running task 165.0 in stage 3.0 (TID 514)
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 163.0 in stage 3.0 (TID 512) in 0 ms on localhost (executor driver) (163/175)
2017-11-03 09:43:24,668 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,668 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,668 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,668 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,668 INFO[org.apache.spark.executor.Executor:54] - Finished task 164.0 in stage 3.0 (TID 513). 983 bytes result sent to driver
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 166.0 in stage 3.0 (TID 515, localhost, executor driver, partition 166, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 164.0 in stage 3.0 (TID 513) in 0 ms on localhost (executor driver) (164/175)
2017-11-03 09:43:24,668 INFO[org.apache.spark.executor.Executor:54] - Running task 166.0 in stage 3.0 (TID 515)
2017-11-03 09:43:24,668 INFO[org.apache.spark.executor.Executor:54] - Finished task 165.0 in stage 3.0 (TID 514). 983 bytes result sent to driver
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 167.0 in stage 3.0 (TID 516, localhost, executor driver, partition 167, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,668 INFO[org.apache.spark.executor.Executor:54] - Running task 167.0 in stage 3.0 (TID 516)
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 165.0 in stage 3.0 (TID 514) in 0 ms on localhost (executor driver) (165/175)
2017-11-03 09:43:24,668 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,668 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,668 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,668 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,668 INFO[org.apache.spark.executor.Executor:54] - Finished task 166.0 in stage 3.0 (TID 515). 983 bytes result sent to driver
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 168.0 in stage 3.0 (TID 517, localhost, executor driver, partition 168, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,668 INFO[org.apache.spark.executor.Executor:54] - Running task 168.0 in stage 3.0 (TID 517)
2017-11-03 09:43:24,668 INFO[org.apache.spark.executor.Executor:54] - Finished task 167.0 in stage 3.0 (TID 516). 983 bytes result sent to driver
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 166.0 in stage 3.0 (TID 515) in 0 ms on localhost (executor driver) (166/175)
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 169.0 in stage 3.0 (TID 518, localhost, executor driver, partition 169, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,668 INFO[org.apache.spark.executor.Executor:54] - Running task 169.0 in stage 3.0 (TID 518)
2017-11-03 09:43:24,668 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 167.0 in stage 3.0 (TID 516) in 0 ms on localhost (executor driver) (167/175)
2017-11-03 09:43:24,668 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,668 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,684 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,684 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,686 INFO[org.apache.spark.executor.Executor:54] - Finished task 168.0 in stage 3.0 (TID 517). 1069 bytes result sent to driver
2017-11-03 09:43:24,687 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 170.0 in stage 3.0 (TID 519, localhost, executor driver, partition 170, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,687 INFO[org.apache.spark.executor.Executor:54] - Running task 170.0 in stage 3.0 (TID 519)
2017-11-03 09:43:24,687 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 168.0 in stage 3.0 (TID 517) in 19 ms on localhost (executor driver) (168/175)
2017-11-03 09:43:24,688 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,688 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,688 INFO[org.apache.spark.executor.Executor:54] - Finished task 169.0 in stage 3.0 (TID 518). 1069 bytes result sent to driver
2017-11-03 09:43:24,689 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 171.0 in stage 3.0 (TID 520, localhost, executor driver, partition 171, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,689 INFO[org.apache.spark.executor.Executor:54] - Running task 171.0 in stage 3.0 (TID 520)
2017-11-03 09:43:24,689 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 169.0 in stage 3.0 (TID 518) in 21 ms on localhost (executor driver) (169/175)
2017-11-03 09:43:24,690 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,690 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,691 INFO[org.apache.spark.executor.Executor:54] - Finished task 170.0 in stage 3.0 (TID 519). 1112 bytes result sent to driver
2017-11-03 09:43:24,691 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 172.0 in stage 3.0 (TID 521, localhost, executor driver, partition 172, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,692 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 170.0 in stage 3.0 (TID 519) in 6 ms on localhost (executor driver) (170/175)
2017-11-03 09:43:24,692 INFO[org.apache.spark.executor.Executor:54] - Running task 172.0 in stage 3.0 (TID 521)
2017-11-03 09:43:24,693 INFO[org.apache.spark.executor.Executor:54] - Finished task 171.0 in stage 3.0 (TID 520). 1112 bytes result sent to driver
2017-11-03 09:43:24,693 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 173.0 in stage 3.0 (TID 522, localhost, executor driver, partition 173, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,693 INFO[org.apache.spark.executor.Executor:54] - Running task 173.0 in stage 3.0 (TID 522)
2017-11-03 09:43:24,694 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 171.0 in stage 3.0 (TID 520) in 5 ms on localhost (executor driver) (171/175)
2017-11-03 09:43:24,694 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,694 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,695 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,695 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,701 INFO[org.apache.spark.executor.Executor:54] - Finished task 172.0 in stage 3.0 (TID 521). 1069 bytes result sent to driver
2017-11-03 09:43:24,702 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 174.0 in stage 3.0 (TID 523, localhost, executor driver, partition 174, PROCESS_LOCAL, 4610 bytes)
2017-11-03 09:43:24,702 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 172.0 in stage 3.0 (TID 521) in 11 ms on localhost (executor driver) (172/175)
2017-11-03 09:43:24,702 INFO[org.apache.spark.executor.Executor:54] - Finished task 173.0 in stage 3.0 (TID 522). 1069 bytes result sent to driver
2017-11-03 09:43:24,702 INFO[org.apache.spark.executor.Executor:54] - Running task 174.0 in stage 3.0 (TID 523)
2017-11-03 09:43:24,703 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 52.0 in stage 3.0 (TID 524, localhost, executor driver, partition 52, ANY, 4610 bytes)
2017-11-03 09:43:24,703 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 173.0 in stage 3.0 (TID 522) in 10 ms on localhost (executor driver) (173/175)
2017-11-03 09:43:24,704 INFO[org.apache.spark.executor.Executor:54] - Running task 52.0 in stage 3.0 (TID 524)
2017-11-03 09:43:24,704 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,704 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,705 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 175 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,705 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,707 INFO[org.apache.spark.executor.Executor:54] - Finished task 174.0 in stage 3.0 (TID 523). 1155 bytes result sent to driver
2017-11-03 09:43:24,708 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 174.0 in stage 3.0 (TID 523) in 6 ms on localhost (executor driver) (174/175)
2017-11-03 09:43:24,741 INFO[org.apache.spark.executor.Executor:54] - Finished task 52.0 in stage 3.0 (TID 524). 1241 bytes result sent to driver
2017-11-03 09:43:24,741 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 52.0 in stage 3.0 (TID 524) in 38 ms on localhost (executor driver) (175/175)
2017-11-03 09:43:24,741 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2017-11-03 09:43:24,741 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 3 (reduceByKey at SparkNginxLog.java:41) finished in 0.911 s
2017-11-03 09:43:24,741 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2017-11-03 09:43:24,741 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2017-11-03 09:43:24,741 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 4)
2017-11-03 09:43:24,741 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2017-11-03 09:43:24,741 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 4 (ShuffledRDD[6] at sortByKey at SparkNginxLog.java:47), which has no missing parents
2017-11-03 09:43:24,741 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 3.4 KB, free 631.4 MB)
2017-11-03 09:43:24,741 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1998.0 B, free 631.4 MB)
2017-11-03 09:43:24,741 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.11.26:57421 (size: 1998.0 B, free: 631.8 MB)
2017-11-03 09:43:24,741 INFO[org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2017-11-03 09:43:24,741 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 4 (ShuffledRDD[6] at sortByKey at SparkNginxLog.java:47) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 09:43:24,741 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 4.0 with 2 tasks
2017-11-03 09:43:24,741 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 4.0 (TID 525, localhost, executor driver, partition 1, PROCESS_LOCAL, 4621 bytes)
2017-11-03 09:43:24,741 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 4.0 (TID 526, localhost, executor driver, partition 0, ANY, 4621 bytes)
2017-11-03 09:43:24,741 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 4.0 (TID 526)
2017-11-03 09:43:24,741 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 4.0 (TID 525)
2017-11-03 09:43:24,741 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,741 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 175 blocks
2017-11-03 09:43:24,741 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,741 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 09:43:24,756 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 4.0 (TID 526). 1154 bytes result sent to driver
2017-11-03 09:43:24,756 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 4.0 (TID 525). 962 bytes result sent to driver
2017-11-03 09:43:24,756 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 4.0 (TID 526) in 15 ms on localhost (executor driver) (1/2)
2017-11-03 09:43:24,756 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 4.0 (TID 525) in 15 ms on localhost (executor driver) (2/2)
2017-11-03 09:43:24,756 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2017-11-03 09:43:24,756 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 4 (collect at SparkNginxLog.java:49) finished in 0.015 s
2017-11-03 09:43:24,756 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: collect at SparkNginxLog.java:49, took 0.948119 s
2017-11-03 09:43:24,756 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@58860997{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}
2017-11-03 09:43:24,756 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4041
2017-11-03 09:43:24,772 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2017-11-03 09:43:25,209 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2017-11-03 09:43:25,209 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2017-11-03 09:43:25,209 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2017-11-03 09:43:25,209 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2017-11-03 09:43:25,209 WARN[org.apache.spark.SparkEnv:87] - Exception while deleting Spark temp dir: C:\Users\Administrator\AppData\Local\Temp\spark-4db18d66-fa7a-4e41-a0f8-25991631cd5a\userFiles-d7f2205a-6288-4f36-8ea8-6623f4f5d9b4
java.io.IOException: Failed to delete: C:\Users\Administrator\AppData\Local\Temp\spark-4db18d66-fa7a-4e41-a0f8-25991631cd5a\userFiles-d7f2205a-6288-4f36-8ea8-6623f4f5d9b4
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1937)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1936)
	at org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:654)
	at org.apache.spark.api.java.JavaSparkContext.close(JavaSparkContext.scala:657)
	at com.lovecws.mumu.spark.rdd.nginxlog.SparkNginxLog.accessCount(SparkNginxLog.java:52)
	at com.lovecws.mumu.spark.rdd.nginxlog.SparkNginxLogTest.yearAccessCount(SparkNginxLogTest.java:28)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:25,225 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2017-11-03 09:43:25,256 INFO[org.apache.spark.SparkContext:54] - SparkContext already stopped.
2017-11-03 09:43:25,272 WARN[org.apache.spark.sql.SparkSession$Builder:66] - Using an existing SparkSession; some configuration may not take effect.
2017-11-03 09:43:26,723 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 159.059907 ms
2017-11-03 09:43:26,755 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 20.41071 ms
2017-11-03 09:43:26,770 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: person
2017-11-03 09:43:26,880 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select *from person
2017-11-03 09:43:26,973 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select max(age) from person
2017-11-03 09:43:27,145 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 14.561666 ms
2017-11-03 09:43:27,145 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 8.565403 ms
2017-11-03 09:43:27,161 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 5.520603 ms
2017-11-03 09:43:27,177 WARN[org.apache.spark.sql.SparkSession$Builder:66] - Using an existing SparkSession; some configuration may not take effect.
2017-11-03 09:43:27,381 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 11.901436 ms
2017-11-03 09:43:27,413 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 6.295836 ms
2017-11-03 09:43:27,444 INFO[org.apache.spark.SparkContext:54] - Starting job: json at SchemaRDDSparkSQL.java:86
2017-11-03 09:43:27,444 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at SchemaRDDSparkSQL.java:86) with 1 output partitions
2017-11-03 09:43:27,444 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at SchemaRDDSparkSQL.java:86)
2017-11-03 09:43:27,444 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 09:43:27,444 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 09:43:27,444 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[6] at json at SchemaRDDSparkSQL.java:86), which has no missing parents
2017-11-03 09:43:27,475 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Cancelling stage 0
2017-11-03 09:43:27,475 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at SchemaRDDSparkSQL.java:86) failed in Unknown s due to Job aborted due to stage failure: Task serialization failed: java.lang.NullPointerException
java.lang.NullPointerException
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:83)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:56)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1488)
	at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1006)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:930)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:874)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1677)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

2017-11-03 09:43:27,475 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 failed: json at SchemaRDDSparkSQL.java:86, took 0.022115 s
2017-11-03 09:43:27,475 WARN[org.apache.spark.sql.SparkSession$Builder:66] - Using an existing SparkSession; some configuration may not take effect.
2017-11-03 09:43:27,522 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 15.529986 ms
2017-11-03 09:43:27,553 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 7.472316 ms
2017-11-03 09:43:27,569 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 9.333259 ms
2017-11-03 09:43:27,584 INFO[org.apache.spark.streaming.dstream.FileInputDStream:54] - Duration for remembering RDDs set to 60000 ms for org.apache.spark.streaming.dstream.FileInputDStream@58ad0586
2017-11-03 09:43:27,647 INFO[org.apache.spark.streaming.dstream.FileInputDStream:54] - Duration for remembering RDDs set to 60000 ms for org.apache.spark.streaming.dstream.FileInputDStream@7f02b2d4
2017-11-03 09:43:27,678 INFO[org.apache.spark.streaming.CheckpointReader:54] - Checkpoint files found: file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000.bk
2017-11-03 09:43:27,678 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000
2017-11-03 09:43:27,707 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,724 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:43:27,724 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000.bk
2017-11-03 09:43:27,724 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,724 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:43:27,724 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000
2017-11-03 09:43:27,724 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,724 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:43:27,739 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000.bk
2017-11-03 09:43:27,739 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,739 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:43:27,739 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000
2017-11-03 09:43:27,739 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,739 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:43:27,739 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000.bk
2017-11-03 09:43:27,755 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,755 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:43:27,755 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000
2017-11-03 09:43:27,755 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,755 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:43:27,755 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000.bk
2017-11-03 09:43:27,755 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,771 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:43:27,771 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000
2017-11-03 09:43:27,771 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,771 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:43:27,771 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000.bk
2017-11-03 09:43:27,771 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,771 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.checkpoint(SocketSparkStreamingTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:43:27,786 INFO[org.apache.spark.streaming.CheckpointReader:54] - Checkpoint files found: file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000.bk,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000,file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000.bk
2017-11-03 09:43:27,786 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000
2017-11-03 09:43:27,786 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,786 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 58 more
2017-11-03 09:43:27,786 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000.bk
2017-11-03 09:43:27,786 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,802 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429450000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 58 more
2017-11-03 09:43:27,802 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000
2017-11-03 09:43:27,809 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,810 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 58 more
2017-11-03 09:43:27,811 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000.bk
2017-11-03 09:43:27,830 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,845 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429440000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 58 more
2017-11-03 09:43:27,846 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000
2017-11-03 09:43:27,854 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,875 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 58 more
2017-11-03 09:43:27,888 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000.bk
2017-11-03 09:43:27,894 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,918 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429430000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 58 more
2017-11-03 09:43:27,921 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000
2017-11-03 09:43:27,926 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,928 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:43:27,930 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000.bk
2017-11-03 09:43:27,945 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,946 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429420000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:43:27,947 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000
2017-11-03 09:43:27,962 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,963 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:43:27,964 INFO[org.apache.spark.streaming.CheckpointReader:54] - Attempting to load checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000.bk
2017-11-03 09:43:27,970 ERROR[org.apache.spark.util.Utils:91] - Exception encountered
java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
2017-11-03 09:43:27,972 WARN[org.apache.spark.streaming.CheckpointReader:87] - Error reading checkpoint from file file:/D:/data/sparkstreaming/checkpoint/socket/checkpoint-1509429410000.bk
java.io.IOException: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)
	at org.apache.spark.streaming.DStreamGraph.readObject(DStreamGraph.scala:194)
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2136)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:168)
	at org.apache.spark.streaming.Checkpoint$$anonfun$deserialize$2.apply(Checkpoint.scala:158)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)
	at org.apache.spark.streaming.Checkpoint$.deserialize(Checkpoint.scala:171)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:353)
	at org.apache.spark.streaming.CheckpointReader$$anonfun$read$2.apply(Checkpoint.scala:349)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.streaming.CheckpointReader$.read(Checkpoint.scala:349)
	at org.apache.spark.streaming.StreamingContext$.getOrCreate(StreamingContext.scala:827)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$.getOrCreate(JavaStreamingContext.scala:626)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.getOrCreate(JavaStreamingContext.scala)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.checkpoint(SocketSparkStreaming.java:87)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreaming.streaming(SocketSparkStreaming.java:37)
	at com.lovecws.mumu.spark.streaming.SocketSparkStreamingTest.streaming(SocketSparkStreamingTest.java:19)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)
Caused by: java.lang.ClassNotFoundException: com.lovecws.mumu.spark.streaming.basic.SocketSparkStreaming$2
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677)
	at org.apache.spark.streaming.ObjectInputStreamWithLoader.resolveClass(Checkpoint.scala:385)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1933)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1529)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245)
	at java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:552)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply$mcV$sp(DStreamGraph.scala:198)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$readObject$1.apply(DStreamGraph.scala:194)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)
	... 57 more
2017-11-03 09:43:27,990 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2017-11-03 09:43:27,990 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@6a66a204{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 09:43:27,990 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2017-11-03 09:43:27,990 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2017-11-03 09:43:27,990 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2017-11-03 09:43:27,990 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2017-11-03 09:43:27,990 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2017-11-03 09:43:27,990 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2017-11-03 09:43:27,990 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2017-11-03 09:43:27,990 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2017-11-03 09:43:27,990 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-4db18d66-fa7a-4e41-a0f8-25991631cd5a\userFiles-d7f2205a-6288-4f36-8ea8-6623f4f5d9b4
2017-11-03 09:43:28,005 ERROR[org.apache.spark.util.ShutdownHookManager:91] - Exception while deleting Spark temp dir: C:\Users\Administrator\AppData\Local\Temp\spark-4db18d66-fa7a-4e41-a0f8-25991631cd5a\userFiles-d7f2205a-6288-4f36-8ea8-6623f4f5d9b4
java.io.IOException: Failed to delete: C:\Users\Administrator\AppData\Local\Temp\spark-4db18d66-fa7a-4e41-a0f8-25991631cd5a\userFiles-d7f2205a-6288-4f36-8ea8-6623f4f5d9b4
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
2017-11-03 09:43:28,005 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-4db18d66-fa7a-4e41-a0f8-25991631cd5a
2017-11-03 09:43:28,005 ERROR[org.apache.spark.util.ShutdownHookManager:91] - Exception while deleting Spark temp dir: C:\Users\Administrator\AppData\Local\Temp\spark-4db18d66-fa7a-4e41-a0f8-25991631cd5a
java.io.IOException: Failed to delete: C:\Users\Administrator\AppData\Local\Temp\spark-4db18d66-fa7a-4e41-a0f8-25991631cd5a
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
2017-11-03 10:02:56,637 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2017-11-03 10:02:57,168 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2017-11-03 10:02:57,203 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2017-11-03 10:02:57,204 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2017-11-03 10:02:57,205 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2017-11-03 10:02:57,206 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2017-11-03 10:02:57,208 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2017-11-03 10:02:57,685 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 58540.
2017-11-03 10:02:57,703 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2017-11-03 10:02:57,750 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2017-11-03 10:02:57,753 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2017-11-03 10:02:57,753 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2017-11-03 10:02:57,762 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-2965c496-d336-4092-82d0-39a626e03409
2017-11-03 10:02:57,782 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2017-11-03 10:02:57,821 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2017-11-03 10:02:57,888 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2581ms
2017-11-03 10:02:57,959 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2017-11-03 10:02:57,973 INFO[org.spark_project.jetty.server.Server:403] - Started @2667ms
2017-11-03 10:02:57,992 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@76f4f4ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:02:57,992 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2017-11-03 10:02:58,015 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@72445aba{/jobs,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,016 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74cadd41{/jobs/json,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,017 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55e8ec2f{/jobs/job,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,018 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs/job/json,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,019 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@878452d{/stages,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,020 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c51bb7{/stages/json,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,021 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42a9e5d1{/stages/stage,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,022 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@665e9289{/stages/stage/json,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,023 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f603e89{/stages/pool,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,023 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@350ec41e{/stages/pool/json,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,024 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71984c3{/storage,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,025 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5536379e{/storage/json,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,025 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6c8bca63{/storage/rdd,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,026 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/storage/rdd/json,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,027 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/environment,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,028 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f1c5af9{/environment/json,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,029 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/executors,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,030 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/executors/json,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,032 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/executors/threadDump,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,032 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,039 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/static,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,040 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,041 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/api,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,041 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/jobs/job/kill,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,042 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/stages/stage/kill,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,044 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2017-11-03 10:02:58,115 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2017-11-03 10:02:58,137 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58553.
2017-11-03 10:02:58,138 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:58553
2017-11-03 10:02:58,139 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2017-11-03 10:02:58,140 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 58553, None)
2017-11-03 10:02:58,142 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:58553 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 58553, None)
2017-11-03 10:02:58,145 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 58553, None)
2017-11-03 10:02:58,145 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 58553, None)
2017-11-03 10:02:58,352 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a175569{/metrics/json,null,AVAILABLE,@Spark}
2017-11-03 10:02:58,374 INFO[org.apache.spark.SparkContext:54] - Added JAR hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar at hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1509674578374
2017-11-03 10:02:58,746 INFO[org.apache.spark.rdd.SequenceFileRDDFunctions:54] - Saving as sequence file of type (NullWritable,BytesWritable)
2017-11-03 10:02:59,131 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2017-11-03 10:02:59,138 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@76f4f4ec{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:02:59,139 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2017-11-03 10:02:59,146 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2017-11-03 10:02:59,153 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2017-11-03 10:02:59,153 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2017-11-03 10:02:59,157 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2017-11-03 10:02:59,160 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2017-11-03 10:02:59,162 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2017-11-03 10:02:59,163 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2017-11-03 10:02:59,163 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-4abe972e-d03a-4b49-b389-a845c2779caa
2017-11-03 10:03:43,154 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2017-11-03 10:03:43,902 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2017-11-03 10:03:43,927 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2017-11-03 10:03:43,927 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2017-11-03 10:03:43,928 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2017-11-03 10:03:43,931 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2017-11-03 10:03:43,932 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2017-11-03 10:03:44,405 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 58587.
2017-11-03 10:03:44,430 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2017-11-03 10:03:44,481 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2017-11-03 10:03:44,484 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2017-11-03 10:03:44,484 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2017-11-03 10:03:44,493 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-3b122847-5477-457e-a3e7-33cc3e0a86e9
2017-11-03 10:03:44,513 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2017-11-03 10:03:44,553 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2017-11-03 10:03:44,625 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2385ms
2017-11-03 10:03:44,686 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2017-11-03 10:03:44,700 INFO[org.spark_project.jetty.server.Server:403] - Started @2462ms
2017-11-03 10:03:44,718 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@211fbc66{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:03:44,719 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2017-11-03 10:03:44,739 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@72445aba{/jobs,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74cadd41{/jobs/json,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55e8ec2f{/jobs/job,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,741 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs/job/json,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,741 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@878452d{/stages,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,742 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c51bb7{/stages/json,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,743 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42a9e5d1{/stages/stage,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,744 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@665e9289{/stages/stage/json,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,744 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f603e89{/stages/pool,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,745 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@350ec41e{/stages/pool/json,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,745 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71984c3{/storage,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,746 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5536379e{/storage/json,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,747 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6c8bca63{/storage/rdd,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,748 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/storage/rdd/json,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,749 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/environment,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,750 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f1c5af9{/environment/json,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,751 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/executors,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,752 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/executors/json,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,753 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/executors/threadDump,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,754 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,759 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/static,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,759 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,760 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/api,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,761 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/jobs/job/kill,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,762 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/stages/stage/kill,null,AVAILABLE,@Spark}
2017-11-03 10:03:44,764 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2017-11-03 10:03:44,836 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2017-11-03 10:03:44,858 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58600.
2017-11-03 10:03:44,859 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:58600
2017-11-03 10:03:44,860 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2017-11-03 10:03:44,863 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 58600, None)
2017-11-03 10:03:44,868 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:58600 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 58600, None)
2017-11-03 10:03:44,870 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 58600, None)
2017-11-03 10:03:44,871 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 58600, None)
2017-11-03 10:03:45,053 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a175569{/metrics/json,null,AVAILABLE,@Spark}
2017-11-03 10:03:45,080 INFO[org.apache.spark.SparkContext:54] - Added JAR hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar at hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1509674625080
2017-11-03 10:03:45,427 INFO[org.apache.spark.rdd.SequenceFileRDDFunctions:54] - Saving as sequence file of type (NullWritable,BytesWritable)
2017-11-03 10:03:45,767 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2017-11-03 10:03:45,767 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2017-11-03 10:03:45,813 INFO[org.apache.spark.SparkContext:54] - Starting job: saveAsObjectFile at SparkRDDOperation.java:101
2017-11-03 10:03:45,823 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (saveAsObjectFile at SparkRDDOperation.java:101) with 2 output partitions
2017-11-03 10:03:45,824 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (saveAsObjectFile at SparkRDDOperation.java:101)
2017-11-03 10:03:45,824 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 10:03:45,825 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:03:45,832 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at saveAsObjectFile at SparkRDDOperation.java:101), which has no missing parents
2017-11-03 10:03:45,976 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 81.6 KB, free 631.7 MB)
2017-11-03 10:03:46,011 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.2 KB, free 631.7 MB)
2017-11-03 10:03:46,014 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:58600 (size: 29.2 KB, free: 631.8 MB)
2017-11-03 10:03:46,016 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:03:46,031 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at saveAsObjectFile at SparkRDDOperation.java:101) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:03:46,033 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 2 tasks
2017-11-03 10:03:46,075 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4843 bytes)
2017-11-03 10:03:46,076 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4840 bytes)
2017-11-03 10:03:46,081 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2017-11-03 10:03:46,081 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 0.0 (TID 1)
2017-11-03 10:03:46,085 INFO[org.apache.spark.executor.Executor:54] - Fetching hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1509674625080
2017-11-03 10:03:53,556 INFO[org.apache.spark.util.Utils:54] - Fetching hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar to C:\Users\Administrator\AppData\Local\Temp\spark-2ab7c301-ea02-4a46-af03-a00d6489bb8c\userFiles-2e021576-2b1f-41a7-8db5-ed222211d4c7\fetchFileTemp6717267494907673692.tmp
2017-11-03 10:03:53,828 INFO[org.apache.spark.executor.Executor:54] - Adding file:/C:/Users/Administrator/AppData/Local/Temp/spark-2ab7c301-ea02-4a46-af03-a00d6489bb8c/userFiles-2e021576-2b1f-41a7-8db5-ed222211d4c7/mumu-spark.jar to class loader
2017-11-03 10:03:53,899 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2017-11-03 10:03:53,899 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2017-11-03 10:03:53,899 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2017-11-03 10:03:53,899 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2017-11-03 10:03:53,951 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:582] - Saved output of task 'attempt_20171103100345_0000_m_000001_1' to file:/D:/data/spark/object/_temporary/0/task_20171103100345_0000_m_000001
2017-11-03 10:03:53,952 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:582] - Saved output of task 'attempt_20171103100345_0000_m_000000_0' to file:/D:/data/spark/object/_temporary/0/task_20171103100345_0000_m_000000
2017-11-03 10:03:53,952 INFO[org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20171103100345_0000_m_000001_1: Committed
2017-11-03 10:03:53,952 INFO[org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20171103100345_0000_m_000000_0: Committed
2017-11-03 10:03:53,964 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2017-11-03 10:03:53,964 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 0.0 (TID 1). 837 bytes result sent to driver
2017-11-03 10:03:53,971 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 7911 ms on localhost (executor driver) (1/2)
2017-11-03 10:03:53,972 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 7896 ms on localhost (executor driver) (2/2)
2017-11-03 10:03:53,972 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-11-03 10:03:53,976 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (saveAsObjectFile at SparkRDDOperation.java:101) finished in 7.928 s
2017-11-03 10:03:53,982 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: saveAsObjectFile at SparkRDDOperation.java:101, took 8.169174 s
2017-11-03 10:03:54,003 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@211fbc66{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:03:54,005 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2017-11-03 10:03:54,012 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2017-11-03 10:03:54,018 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2017-11-03 10:03:54,019 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2017-11-03 10:03:54,023 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2017-11-03 10:03:54,025 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2017-11-03 10:03:54,028 WARN[org.apache.spark.SparkEnv:87] - Exception while deleting Spark temp dir: C:\Users\Administrator\AppData\Local\Temp\spark-2ab7c301-ea02-4a46-af03-a00d6489bb8c\userFiles-2e021576-2b1f-41a7-8db5-ed222211d4c7
java.io.IOException: Failed to delete: C:\Users\Administrator\AppData\Local\Temp\spark-2ab7c301-ea02-4a46-af03-a00d6489bb8c\userFiles-2e021576-2b1f-41a7-8db5-ed222211d4c7
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1937)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1936)
	at org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:654)
	at org.apache.spark.api.java.JavaSparkContext.close(JavaSparkContext.scala:657)
	at com.lovecws.mumu.spark.rdd.SparkRDDOperation.saveAsObjectFile(SparkRDDOperation.java:102)
	at com.lovecws.mumu.spark.rdd.SparkRDDOperationTest.saveAsObjectFile(SparkRDDOperationTest.java:71)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
2017-11-03 10:03:54,032 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2017-11-03 10:03:54,038 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2017-11-03 10:03:54,039 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-2ab7c301-ea02-4a46-af03-a00d6489bb8c
2017-11-03 10:03:54,045 ERROR[org.apache.spark.util.ShutdownHookManager:91] - Exception while deleting Spark temp dir: C:\Users\Administrator\AppData\Local\Temp\spark-2ab7c301-ea02-4a46-af03-a00d6489bb8c
java.io.IOException: Failed to delete: C:\Users\Administrator\AppData\Local\Temp\spark-2ab7c301-ea02-4a46-af03-a00d6489bb8c
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
2017-11-03 10:03:54,045 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-2ab7c301-ea02-4a46-af03-a00d6489bb8c\userFiles-2e021576-2b1f-41a7-8db5-ed222211d4c7
2017-11-03 10:03:54,047 ERROR[org.apache.spark.util.ShutdownHookManager:91] - Exception while deleting Spark temp dir: C:\Users\Administrator\AppData\Local\Temp\spark-2ab7c301-ea02-4a46-af03-a00d6489bb8c\userFiles-2e021576-2b1f-41a7-8db5-ed222211d4c7
java.io.IOException: Failed to delete: C:\Users\Administrator\AppData\Local\Temp\spark-2ab7c301-ea02-4a46-af03-a00d6489bb8c\userFiles-2e021576-2b1f-41a7-8db5-ed222211d4c7
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
2017-11-03 10:05:24,785 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2017-11-03 10:05:25,477 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2017-11-03 10:05:25,504 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2017-11-03 10:05:25,505 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2017-11-03 10:05:25,505 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2017-11-03 10:05:25,506 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2017-11-03 10:05:25,507 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2017-11-03 10:05:25,978 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 58651.
2017-11-03 10:05:25,995 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2017-11-03 10:05:26,041 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2017-11-03 10:05:26,045 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2017-11-03 10:05:26,045 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2017-11-03 10:05:26,054 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-d55fea2f-97bd-42c9-bcc6-354b7b7edfd4
2017-11-03 10:05:26,074 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2017-11-03 10:05:26,111 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2017-11-03 10:05:26,181 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2345ms
2017-11-03 10:05:26,243 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2017-11-03 10:05:26,259 INFO[org.spark_project.jetty.server.Server:403] - Started @2423ms
2017-11-03 10:05:26,278 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@12257d4b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:05:26,279 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2017-11-03 10:05:26,300 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@61bcd567{/jobs,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,301 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2e54db99{/jobs/json,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,302 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4f2613d1{/jobs/job,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,303 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@773f7880{/jobs/job/json,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,303 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/stages,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,304 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@83298d7{/stages/json,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,305 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5b080f3a{/stages/stage,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,306 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7d3430a7{/stages/stage/json,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,307 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2756c0a7{/stages/pool,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,307 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@69637b10{/stages/pool/json,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,308 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@165b2f7f{/storage,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,308 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@277f7dd3{/storage/json,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,309 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2364305a{/storage/rdd,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,310 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bc715b8{/storage/rdd/json,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,311 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/environment,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,311 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/environment/json,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,312 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/executors,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,313 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/executors/json,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,314 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/executors/threadDump,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,315 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/executors/threadDump/json,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,321 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/static,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,322 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,323 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/api,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,324 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5860f3d7{/jobs/job/kill,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,324 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/stages/stage/kill,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,326 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2017-11-03 10:05:26,394 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2017-11-03 10:05:26,416 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58664.
2017-11-03 10:05:26,416 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:58664
2017-11-03 10:05:26,418 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2017-11-03 10:05:26,419 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 58664, None)
2017-11-03 10:05:26,423 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:58664 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 58664, None)
2017-11-03 10:05:26,425 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 58664, None)
2017-11-03 10:05:26,426 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 58664, None)
2017-11-03 10:05:26,596 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@11963225{/metrics/json,null,AVAILABLE,@Spark}
2017-11-03 10:05:26,616 INFO[org.apache.spark.SparkContext:54] - Added JAR hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar at hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1509674726616
2017-11-03 10:05:27,280 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2017-11-03 10:05:27,280 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2017-11-03 10:05:27,326 INFO[org.apache.spark.SparkContext:54] - Starting job: saveAsTextFile at SparkRDDOperation.java:108
2017-11-03 10:05:27,337 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (saveAsTextFile at SparkRDDOperation.java:108) with 2 output partitions
2017-11-03 10:05:27,337 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (saveAsTextFile at SparkRDDOperation.java:108)
2017-11-03 10:05:27,337 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 10:05:27,338 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:05:27,345 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[1] at saveAsTextFile at SparkRDDOperation.java:108), which has no missing parents
2017-11-03 10:05:27,494 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 81.2 KB, free 631.7 MB)
2017-11-03 10:05:27,526 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.0 KB, free 631.7 MB)
2017-11-03 10:05:27,529 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:58664 (size: 29.0 KB, free: 631.8 MB)
2017-11-03 10:05:27,531 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:05:27,542 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at saveAsTextFile at SparkRDDOperation.java:108) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:05:27,543 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 2 tasks
2017-11-03 10:05:27,583 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4843 bytes)
2017-11-03 10:05:27,586 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4840 bytes)
2017-11-03 10:05:27,590 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 0.0 (TID 1)
2017-11-03 10:05:27,590 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2017-11-03 10:05:27,594 INFO[org.apache.spark.executor.Executor:54] - Fetching hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1509674726616
2017-11-03 10:05:35,073 INFO[org.apache.spark.util.Utils:54] - Fetching hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar to C:\Users\Administrator\AppData\Local\Temp\spark-769be43e-6f9a-4307-aff9-7c7a88770c3e\userFiles-95d130c2-41e6-4341-b347-e40dabae9392\fetchFileTemp6159200156076755035.tmp
2017-11-03 10:05:35,365 INFO[org.apache.spark.executor.Executor:54] - Adding file:/C:/Users/Administrator/AppData/Local/Temp/spark-769be43e-6f9a-4307-aff9-7c7a88770c3e/userFiles-95d130c2-41e6-4341-b347-e40dabae9392/mumu-spark.jar to class loader
2017-11-03 10:05:35,441 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2017-11-03 10:05:35,441 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2017-11-03 10:05:35,441 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2017-11-03 10:05:35,441 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2017-11-03 10:05:35,459 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:582] - Saved output of task 'attempt_20171103100527_0000_m_000000_0' to file:/D:/data/spark/text/_temporary/0/task_20171103100527_0000_m_000000
2017-11-03 10:05:35,460 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:582] - Saved output of task 'attempt_20171103100527_0000_m_000001_1' to file:/D:/data/spark/text/_temporary/0/task_20171103100527_0000_m_000001
2017-11-03 10:05:35,460 INFO[org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20171103100527_0000_m_000000_0: Committed
2017-11-03 10:05:35,460 INFO[org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20171103100527_0000_m_000001_1: Committed
2017-11-03 10:05:35,471 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2017-11-03 10:05:35,471 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 0.0 (TID 1). 837 bytes result sent to driver
2017-11-03 10:05:35,476 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 7909 ms on localhost (executor driver) (1/2)
2017-11-03 10:05:35,477 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 7892 ms on localhost (executor driver) (2/2)
2017-11-03 10:05:35,478 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-11-03 10:05:35,481 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (saveAsTextFile at SparkRDDOperation.java:108) finished in 7.924 s
2017-11-03 10:05:35,488 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: saveAsTextFile at SparkRDDOperation.java:108, took 8.160389 s
2017-11-03 10:05:35,544 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2017-11-03 10:05:35,549 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@12257d4b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:05:35,550 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2017-11-03 10:05:35,558 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2017-11-03 10:05:35,566 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2017-11-03 10:05:35,566 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2017-11-03 10:05:35,570 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2017-11-03 10:05:35,572 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2017-11-03 10:05:35,576 WARN[org.apache.spark.SparkEnv:87] - Exception while deleting Spark temp dir: C:\Users\Administrator\AppData\Local\Temp\spark-769be43e-6f9a-4307-aff9-7c7a88770c3e\userFiles-95d130c2-41e6-4341-b347-e40dabae9392
java.io.IOException: Failed to delete: C:\Users\Administrator\AppData\Local\Temp\spark-769be43e-6f9a-4307-aff9-7c7a88770c3e\userFiles-95d130c2-41e6-4341-b347-e40dabae9392
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1937)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1936)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
2017-11-03 10:05:35,578 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2017-11-03 10:05:35,579 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2017-11-03 10:05:35,580 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-769be43e-6f9a-4307-aff9-7c7a88770c3e\userFiles-95d130c2-41e6-4341-b347-e40dabae9392
2017-11-03 10:05:35,583 ERROR[org.apache.spark.util.ShutdownHookManager:91] - Exception while deleting Spark temp dir: C:\Users\Administrator\AppData\Local\Temp\spark-769be43e-6f9a-4307-aff9-7c7a88770c3e\userFiles-95d130c2-41e6-4341-b347-e40dabae9392
java.io.IOException: Failed to delete: C:\Users\Administrator\AppData\Local\Temp\spark-769be43e-6f9a-4307-aff9-7c7a88770c3e\userFiles-95d130c2-41e6-4341-b347-e40dabae9392
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
2017-11-03 10:05:35,584 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-769be43e-6f9a-4307-aff9-7c7a88770c3e
2017-11-03 10:05:35,588 ERROR[org.apache.spark.util.ShutdownHookManager:91] - Exception while deleting Spark temp dir: C:\Users\Administrator\AppData\Local\Temp\spark-769be43e-6f9a-4307-aff9-7c7a88770c3e
java.io.IOException: Failed to delete: C:\Users\Administrator\AppData\Local\Temp\spark-769be43e-6f9a-4307-aff9-7c7a88770c3e
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
2017-11-03 10:06:21,956 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2017-11-03 10:06:22,834 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2017-11-03 10:06:22,878 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2017-11-03 10:06:22,879 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2017-11-03 10:06:22,880 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2017-11-03 10:06:22,881 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2017-11-03 10:06:22,882 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2017-11-03 10:06:23,386 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 58706.
2017-11-03 10:06:23,404 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2017-11-03 10:06:23,450 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2017-11-03 10:06:23,453 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2017-11-03 10:06:23,453 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2017-11-03 10:06:23,461 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-046683f5-7f27-4647-9ce5-091308cdfbee
2017-11-03 10:06:23,482 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2017-11-03 10:06:23,519 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2017-11-03 10:06:23,589 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2539ms
2017-11-03 10:06:23,655 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2017-11-03 10:06:23,671 INFO[org.spark_project.jetty.server.Server:403] - Started @2623ms
2017-11-03 10:06:23,690 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@3a39f5f9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:06:23,691 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2017-11-03 10:06:23,713 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@72445aba{/jobs,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,713 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74cadd41{/jobs/json,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,714 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55e8ec2f{/jobs/job,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,715 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs/job/json,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,716 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@878452d{/stages,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,716 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c51bb7{/stages/json,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,717 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42a9e5d1{/stages/stage,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,719 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@665e9289{/stages/stage/json,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,719 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f603e89{/stages/pool,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,720 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@350ec41e{/stages/pool/json,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,721 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71984c3{/storage,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,722 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5536379e{/storage/json,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,722 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6c8bca63{/storage/rdd,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,723 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/storage/rdd/json,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,724 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/environment,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,725 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f1c5af9{/environment/json,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,726 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/executors,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,726 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/executors/json,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,727 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/executors/threadDump,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,729 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,735 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/static,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,736 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,737 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/api,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,738 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/jobs/job/kill,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,738 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/stages/stage/kill,null,AVAILABLE,@Spark}
2017-11-03 10:06:23,740 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2017-11-03 10:06:23,806 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2017-11-03 10:06:23,826 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58719.
2017-11-03 10:06:23,826 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:58719
2017-11-03 10:06:23,828 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2017-11-03 10:06:23,829 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 58719, None)
2017-11-03 10:06:23,833 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:58719 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 58719, None)
2017-11-03 10:06:23,835 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 58719, None)
2017-11-03 10:06:23,836 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 58719, None)
2017-11-03 10:06:24,026 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a175569{/metrics/json,null,AVAILABLE,@Spark}
2017-11-03 10:06:24,044 INFO[org.apache.spark.SparkContext:54] - Added JAR hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar at hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1509674784044
2017-11-03 10:06:24,696 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2017-11-03 10:06:24,696 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2017-11-03 10:06:24,749 INFO[org.apache.spark.SparkContext:54] - Starting job: saveAsTextFile at SparkRDDOperation.java:108
2017-11-03 10:06:24,759 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (saveAsTextFile at SparkRDDOperation.java:108) with 2 output partitions
2017-11-03 10:06:24,760 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (saveAsTextFile at SparkRDDOperation.java:108)
2017-11-03 10:06:24,760 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 10:06:24,761 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:06:24,768 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[1] at saveAsTextFile at SparkRDDOperation.java:108), which has no missing parents
2017-11-03 10:06:24,926 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 81.2 KB, free 631.7 MB)
2017-11-03 10:06:24,959 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 29.0 KB, free 631.7 MB)
2017-11-03 10:06:24,962 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:58719 (size: 29.0 KB, free: 631.8 MB)
2017-11-03 10:06:24,965 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:06:24,977 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at saveAsTextFile at SparkRDDOperation.java:108) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:06:24,978 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 2 tasks
2017-11-03 10:06:25,022 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4843 bytes)
2017-11-03 10:06:25,024 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 4840 bytes)
2017-11-03 10:06:25,028 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 0.0 (TID 1)
2017-11-03 10:06:25,028 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2017-11-03 10:06:25,032 INFO[org.apache.spark.executor.Executor:54] - Fetching hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1509674784044
2017-11-03 10:06:32,537 INFO[org.apache.spark.util.Utils:54] - Fetching hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar to C:\Users\Administrator\AppData\Local\Temp\spark-57963c3d-388e-417d-af83-0f2a0914561a\userFiles-0d014ed0-6aeb-431d-916f-e4667985c7fc\fetchFileTemp7331145018862914243.tmp
2017-11-03 10:06:32,806 INFO[org.apache.spark.executor.Executor:54] - Adding file:/C:/Users/Administrator/AppData/Local/Temp/spark-57963c3d-388e-417d-af83-0f2a0914561a/userFiles-0d014ed0-6aeb-431d-916f-e4667985c7fc/mumu-spark.jar to class loader
2017-11-03 10:06:32,879 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2017-11-03 10:06:32,879 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2017-11-03 10:06:32,880 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2017-11-03 10:06:32,880 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2017-11-03 10:06:32,898 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:582] - Saved output of task 'attempt_20171103100624_0000_m_000000_0' to file:/D:/data/spark/text/_temporary/0/task_20171103100624_0000_m_000000
2017-11-03 10:06:32,898 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:582] - Saved output of task 'attempt_20171103100624_0000_m_000001_1' to file:/D:/data/spark/text/_temporary/0/task_20171103100624_0000_m_000001
2017-11-03 10:06:32,899 INFO[org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20171103100624_0000_m_000001_1: Committed
2017-11-03 10:06:32,899 INFO[org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20171103100624_0000_m_000000_0: Committed
2017-11-03 10:06:32,910 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 0.0 (TID 1). 880 bytes result sent to driver
2017-11-03 10:06:32,910 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 837 bytes result sent to driver
2017-11-03 10:06:32,916 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 7908 ms on localhost (executor driver) (1/2)
2017-11-03 10:06:32,917 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 7894 ms on localhost (executor driver) (2/2)
2017-11-03 10:06:32,918 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-11-03 10:06:32,921 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (saveAsTextFile at SparkRDDOperation.java:108) finished in 7.929 s
2017-11-03 10:06:32,927 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: saveAsTextFile at SparkRDDOperation.java:108, took 8.177291 s
2017-11-03 10:06:32,949 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@3a39f5f9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:06:32,951 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2017-11-03 10:06:32,958 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2017-11-03 10:06:32,965 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2017-11-03 10:06:32,965 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2017-11-03 10:06:32,969 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2017-11-03 10:06:32,971 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2017-11-03 10:06:32,975 WARN[org.apache.spark.SparkEnv:87] - Exception while deleting Spark temp dir: C:\Users\Administrator\AppData\Local\Temp\spark-57963c3d-388e-417d-af83-0f2a0914561a\userFiles-0d014ed0-6aeb-431d-916f-e4667985c7fc
java.io.IOException: Failed to delete: C:\Users\Administrator\AppData\Local\Temp\spark-57963c3d-388e-417d-af83-0f2a0914561a\userFiles-0d014ed0-6aeb-431d-916f-e4667985c7fc
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1937)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1936)
	at org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:654)
	at org.apache.spark.api.java.JavaSparkContext.close(JavaSparkContext.scala:657)
	at com.lovecws.mumu.spark.rdd.SparkRDDOperation.saveAsTextFile(SparkRDDOperation.java:109)
	at com.lovecws.mumu.spark.rdd.SparkRDDOperationTest.saveAsTextFile(SparkRDDOperationTest.java:82)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70)
2017-11-03 10:06:32,977 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2017-11-03 10:06:32,980 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2017-11-03 10:06:32,981 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-57963c3d-388e-417d-af83-0f2a0914561a
2017-11-03 10:06:32,984 ERROR[org.apache.spark.util.ShutdownHookManager:91] - Exception while deleting Spark temp dir: C:\Users\Administrator\AppData\Local\Temp\spark-57963c3d-388e-417d-af83-0f2a0914561a
java.io.IOException: Failed to delete: C:\Users\Administrator\AppData\Local\Temp\spark-57963c3d-388e-417d-af83-0f2a0914561a
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
2017-11-03 10:06:32,985 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-57963c3d-388e-417d-af83-0f2a0914561a\userFiles-0d014ed0-6aeb-431d-916f-e4667985c7fc
2017-11-03 10:06:32,987 ERROR[org.apache.spark.util.ShutdownHookManager:91] - Exception while deleting Spark temp dir: C:\Users\Administrator\AppData\Local\Temp\spark-57963c3d-388e-417d-af83-0f2a0914561a\userFiles-0d014ed0-6aeb-431d-916f-e4667985c7fc
java.io.IOException: Failed to delete: C:\Users\Administrator\AppData\Local\Temp\spark-57963c3d-388e-417d-af83-0f2a0914561a\userFiles-0d014ed0-6aeb-431d-916f-e4667985c7fc
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
2017-11-03 10:31:45,267 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2017-11-03 10:31:45,849 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2017-11-03 10:31:45,881 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2017-11-03 10:31:45,881 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2017-11-03 10:31:45,881 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2017-11-03 10:31:45,881 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2017-11-03 10:31:45,881 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2017-11-03 10:31:46,248 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 59776.
2017-11-03 10:31:46,263 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2017-11-03 10:31:46,279 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2017-11-03 10:31:46,279 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2017-11-03 10:31:46,279 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2017-11-03 10:31:46,279 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-04f3fe56-f1f3-4bfc-bf99-74f60f308e99
2017-11-03 10:31:46,342 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2017-11-03 10:31:46,373 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2017-11-03 10:31:46,451 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2280ms
2017-11-03 10:31:46,498 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2017-11-03 10:31:46,513 INFO[org.spark_project.jetty.server.Server:403] - Started @2346ms
2017-11-03 10:31:46,529 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@7a6fb710{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:31:46,529 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2017-11-03 10:31:46,545 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@30bcf3c1{/jobs,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,545 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ee37ca3{/jobs/json,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,545 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53812a9b{/jobs/job,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,545 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@422c3c7a{/jobs/job/json,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,545 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@d8305c2{/stages,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,545 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@75d0911a{/stages/json,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,545 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@60d1a32f{/stages/stage,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,545 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@726e5805{/stages/stage/json,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,545 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4b672daa{/stages/pool,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,545 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e077866{/stages/pool/json,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,545 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c2b6087{/storage,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,545 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a8e6492{/storage/json,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,560 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3b77a04f{/storage/rdd,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,560 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2e11485{/storage/rdd/json,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,560 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@662f5666{/environment,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,560 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5974109{/environment/json,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,560 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ef3efa8{/executors,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,560 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f8f9349{/executors/json,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,560 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7446d8d5{/executors/threadDump,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,560 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4fbda97b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,560 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@37b70343{/static,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,560 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51cd7ffc{/,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,576 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4cc6fa2a{/api,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,576 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6b9ce1bf{/jobs/job/kill,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,579 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@75ed9710{/stages/stage/kill,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,581 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2017-11-03 10:31:46,655 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2017-11-03 10:31:46,698 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59789.
2017-11-03 10:31:46,699 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:59789
2017-11-03 10:31:46,700 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2017-11-03 10:31:46,701 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 59789, None)
2017-11-03 10:31:46,703 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:59789 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 59789, None)
2017-11-03 10:31:46,709 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 59789, None)
2017-11-03 10:31:46,712 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 59789, None)
2017-11-03 10:31:46,868 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63fd4873{/metrics/json,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,924 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2017-11-03 10:31:46,925 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2017-11-03 10:31:46,932 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e9c413e{/SQL,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,932 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5af5def9{/SQL/json,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,933 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@27a0a5a2{/SQL/execution,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,933 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@33aa93c{/SQL/execution/json,null,AVAILABLE,@Spark}
2017-11-03 10:31:46,935 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7dd712e8{/static/sql,null,AVAILABLE,@Spark}
2017-11-03 10:31:47,850 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2017-11-03 10:31:49,459 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 182.705937 ms
2017-11-03 10:31:49,630 INFO[org.apache.spark.SparkContext:54] - Starting job: first at RowMatrix.scala:61
2017-11-03 10:31:49,646 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (first at RowMatrix.scala:61) with 1 output partitions
2017-11-03 10:31:49,646 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (first at RowMatrix.scala:61)
2017-11-03 10:31:49,646 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 10:31:49,646 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:31:49,646 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[4] at map at Correlation.scala:70), which has no missing parents
2017-11-03 10:31:49,725 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 8.1 KB, free 631.8 MB)
2017-11-03 10:31:49,756 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 631.8 MB)
2017-11-03 10:31:49,756 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:59789 (size: 4.2 KB, free: 631.8 MB)
2017-11-03 10:31:49,756 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:31:49,881 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at Correlation.scala:70) (first 15 tasks are for partitions Vector(0))
2017-11-03 10:31:49,881 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2017-11-03 10:31:49,913 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:31:49,913 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2017-11-03 10:31:49,992 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 11.785969 ms
2017-11-03 10:31:50,024 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1159 bytes result sent to driver
2017-11-03 10:31:50,024 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 127 ms on localhost (executor driver) (1/1)
2017-11-03 10:31:50,024 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-11-03 10:31:50,024 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (first at RowMatrix.scala:61) finished in 0.127 s
2017-11-03 10:31:50,039 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: first at RowMatrix.scala:61, took 0.407220 s
2017-11-03 10:31:50,074 INFO[org.apache.spark.SparkContext:54] - Starting job: treeAggregate at RowMatrix.scala:419
2017-11-03 10:31:50,074 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (treeAggregate at RowMatrix.scala:419) with 2 output partitions
2017-11-03 10:31:50,074 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (treeAggregate at RowMatrix.scala:419)
2017-11-03 10:31:50,074 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 10:31:50,074 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:31:50,074 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[5] at treeAggregate at RowMatrix.scala:419), which has no missing parents
2017-11-03 10:31:50,074 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.0 KB, free 631.8 MB)
2017-11-03 10:31:50,074 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 631.8 MB)
2017-11-03 10:31:50,074 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:59789 (size: 4.6 KB, free: 631.8 MB)
2017-11-03 10:31:50,074 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:31:50,074 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at treeAggregate at RowMatrix.scala:419) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:31:50,074 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 2 tasks
2017-11-03 10:31:50,074 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:31:50,074 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:31:50,074 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2017-11-03 10:31:50,090 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 1.0 (TID 2)
2017-11-03 10:31:50,090 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 1.0 (TID 2). 1528 bytes result sent to driver
2017-11-03 10:31:50,090 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1614 bytes result sent to driver
2017-11-03 10:31:50,109 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 2) in 35 ms on localhost (executor driver) (1/2)
2017-11-03 10:31:50,109 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 35 ms on localhost (executor driver) (2/2)
2017-11-03 10:31:50,109 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-11-03 10:31:50,110 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (treeAggregate at RowMatrix.scala:419) finished in 0.036 s
2017-11-03 10:31:50,111 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: treeAggregate at RowMatrix.scala:419, took 0.033932 s
2017-11-03 10:31:50,663 INFO[org.apache.spark.SparkContext:54] - Starting job: treeAggregate at RowMatrix.scala:122
2017-11-03 10:31:50,663 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 2 (treeAggregate at RowMatrix.scala:122) with 2 output partitions
2017-11-03 10:31:50,663 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (treeAggregate at RowMatrix.scala:122)
2017-11-03 10:31:50,663 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 10:31:50,663 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:31:50,663 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[6] at treeAggregate at RowMatrix.scala:122), which has no missing parents
2017-11-03 10:31:50,663 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 8.9 KB, free 631.8 MB)
2017-11-03 10:31:50,663 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.5 KB, free 631.8 MB)
2017-11-03 10:31:50,679 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:59789 (size: 4.5 KB, free: 631.8 MB)
2017-11-03 10:31:50,683 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:31:50,684 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at treeAggregate at RowMatrix.scala:122) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:31:50,684 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 2 tasks
2017-11-03 10:31:50,685 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:31:50,686 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:31:50,686 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 2.0 (TID 4)
2017-11-03 10:31:50,686 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 3)
2017-11-03 10:31:50,704 WARN[com.github.fommil.netlib.BLAS:61] - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
2017-11-03 10:31:50,704 WARN[com.github.fommil.netlib.BLAS:61] - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
2017-11-03 10:31:50,709 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 2.0 (TID 4). 1223 bytes result sent to driver
2017-11-03 10:31:50,709 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 3). 1180 bytes result sent to driver
2017-11-03 10:31:50,710 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 3) in 25 ms on localhost (executor driver) (1/2)
2017-11-03 10:31:50,713 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 4) in 27 ms on localhost (executor driver) (2/2)
2017-11-03 10:31:50,714 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-11-03 10:31:50,714 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (treeAggregate at RowMatrix.scala:122) finished in 0.029 s
2017-11-03 10:31:50,715 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 2 finished: treeAggregate at RowMatrix.scala:122, took 0.044527 s
2017-11-03 10:31:51,045 WARN[org.apache.spark.mllib.stat.correlation.PearsonCorrelation:66] - Pearson correlation matrix contains NaN values.
2017-11-03 10:31:51,109 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 20.933518 ms
2017-11-03 10:31:51,125 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 8.601967 ms
2017-11-03 10:31:51,156 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_2_piece0 on 192.168.11.26:59789 in memory (size: 4.5 KB, free: 631.8 MB)
2017-11-03 10:31:51,172 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:59789 in memory (size: 4.6 KB, free: 631.8 MB)
2017-11-03 10:31:51,172 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2017-11-03 10:31:51,172 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@7a6fb710{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:31:51,172 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2017-11-03 10:31:51,187 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2017-11-03 10:31:51,203 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2017-11-03 10:31:51,203 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2017-11-03 10:31:51,203 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2017-11-03 10:31:51,203 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2017-11-03 10:31:51,203 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2017-11-03 10:31:51,203 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2017-11-03 10:31:51,203 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-14dce342-9a35-40ce-864e-0cbbea5953d5
2017-11-03 10:34:06,174 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2017-11-03 10:34:06,757 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2017-11-03 10:34:06,789 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2017-11-03 10:34:06,789 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2017-11-03 10:34:06,789 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2017-11-03 10:34:06,789 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2017-11-03 10:34:06,789 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2017-11-03 10:34:07,118 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 59846.
2017-11-03 10:34:07,133 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2017-11-03 10:34:07,149 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2017-11-03 10:34:07,149 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2017-11-03 10:34:07,149 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2017-11-03 10:34:07,149 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-1064c9f6-772d-404d-a8de-3bc814597669
2017-11-03 10:34:07,196 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2017-11-03 10:34:07,243 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2017-11-03 10:34:07,306 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2200ms
2017-11-03 10:34:07,353 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2017-11-03 10:34:07,369 INFO[org.spark_project.jetty.server.Server:403] - Started @2264ms
2017-11-03 10:34:07,384 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@2d4c73f4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:34:07,384 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2017-11-03 10:34:07,400 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2a3c96e3{/jobs,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,400 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@45c8d09f{/jobs/json,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,400 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@14b030a0{/jobs/job,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,400 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18230356{/jobs/job/json,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,400 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@56bca85b{/stages,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,400 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@75e91545{/stages/json,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@531c311e{/stages/stage,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40c80397{/stages/stage/json,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@ea9b7c6{/stages/pool,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@ed3068a{/stages/pool/json,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fffff43{/storage,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c7fd41f{/storage/json,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7b324585{/storage/rdd,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@60dce7ea{/storage/rdd/json,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@fd8294b{/environment,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@27305e6{/environment/json,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@502f1f4c{/executors,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@75c9e76b{/executors/json,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c3b6c6e{/executors/threadDump,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@75f5fd58{/executors/threadDump/json,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,431 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@306851c7{/static,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,431 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@30d4b288{/,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,431 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40f1be1b{/api,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,431 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@61884cb1{/jobs/job/kill,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,431 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4fc5e095{/stages/stage/kill,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,431 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2017-11-03 10:34:07,494 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2017-11-03 10:34:07,511 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59859.
2017-11-03 10:34:07,511 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:59859
2017-11-03 10:34:07,511 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2017-11-03 10:34:07,527 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 59859, None)
2017-11-03 10:34:07,527 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:59859 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 59859, None)
2017-11-03 10:34:07,527 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 59859, None)
2017-11-03 10:34:07,527 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 59859, None)
2017-11-03 10:34:07,709 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1e11bc55{/metrics/json,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,756 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2017-11-03 10:34:07,756 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2017-11-03 10:34:07,756 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@57a4d5ee{/SQL,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,756 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3a45c42a{/SQL/json,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,756 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7692cd34{/SQL/execution,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,756 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@32c0915e{/SQL/execution/json,null,AVAILABLE,@Spark}
2017-11-03 10:34:07,756 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c282004{/static/sql,null,AVAILABLE,@Spark}
2017-11-03 10:34:08,709 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2017-11-03 10:34:09,795 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: feature
2017-11-03 10:34:10,030 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select *from feature
2017-11-03 10:34:10,469 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 182.843855 ms
2017-11-03 10:34:10,484 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 9.177379 ms
2017-11-03 10:34:10,834 INFO[org.apache.spark.SparkContext:54] - Starting job: first at RowMatrix.scala:61
2017-11-03 10:34:10,944 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (first at RowMatrix.scala:61) with 1 output partitions
2017-11-03 10:34:10,944 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 0
2017-11-03 10:34:10,944 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (first at RowMatrix.scala:61)
2017-11-03 10:34:10,944 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 10:34:10,944 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:34:10,959 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[4] at map at Correlation.scala:70), which has no missing parents
2017-11-03 10:34:11,038 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 8.1 KB, free 631.8 MB)
2017-11-03 10:34:11,069 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 631.8 MB)
2017-11-03 10:34:11,084 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:59859 (size: 4.2 KB, free: 631.8 MB)
2017-11-03 10:34:11,084 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:34:11,100 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at Correlation.scala:70) (first 15 tasks are for partitions Vector(0))
2017-11-03 10:34:11,100 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2017-11-03 10:34:11,131 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:34:11,131 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2017-11-03 10:34:11,220 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1159 bytes result sent to driver
2017-11-03 10:34:11,236 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 104 ms on localhost (executor driver) (1/1)
2017-11-03 10:34:11,236 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-11-03 10:34:11,236 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (first at RowMatrix.scala:61) finished in 0.120 s
2017-11-03 10:34:11,236 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: first at RowMatrix.scala:61, took 0.400720 s
2017-11-03 10:34:11,282 INFO[org.apache.spark.SparkContext:54] - Starting job: treeAggregate at RowMatrix.scala:419
2017-11-03 10:34:11,282 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (treeAggregate at RowMatrix.scala:419) with 2 output partitions
2017-11-03 10:34:11,282 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (treeAggregate at RowMatrix.scala:419)
2017-11-03 10:34:11,282 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 10:34:11,282 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:34:11,282 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[5] at treeAggregate at RowMatrix.scala:419), which has no missing parents
2017-11-03 10:34:11,282 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.0 KB, free 631.8 MB)
2017-11-03 10:34:11,282 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 631.8 MB)
2017-11-03 10:34:11,282 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:59859 (size: 4.6 KB, free: 631.8 MB)
2017-11-03 10:34:11,282 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:34:11,282 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at treeAggregate at RowMatrix.scala:419) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:34:11,282 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 2 tasks
2017-11-03 10:34:11,282 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:34:11,282 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:34:11,282 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2017-11-03 10:34:11,298 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 1.0 (TID 2)
2017-11-03 10:34:11,318 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1700 bytes result sent to driver
2017-11-03 10:34:11,318 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 1.0 (TID 2). 1614 bytes result sent to driver
2017-11-03 10:34:11,318 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 36 ms on localhost (executor driver) (1/2)
2017-11-03 10:34:11,318 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 2) in 36 ms on localhost (executor driver) (2/2)
2017-11-03 10:34:11,318 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (treeAggregate at RowMatrix.scala:419) finished in 0.036 s
2017-11-03 10:34:11,318 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: treeAggregate at RowMatrix.scala:419, took 0.041348 s
2017-11-03 10:34:11,318 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-11-03 10:34:11,709 INFO[org.apache.spark.SparkContext:54] - Starting job: treeAggregate at RowMatrix.scala:122
2017-11-03 10:34:11,709 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 2 (treeAggregate at RowMatrix.scala:122) with 2 output partitions
2017-11-03 10:34:11,709 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (treeAggregate at RowMatrix.scala:122)
2017-11-03 10:34:11,709 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 10:34:11,709 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:34:11,709 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[6] at treeAggregate at RowMatrix.scala:122), which has no missing parents
2017-11-03 10:34:11,709 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 8.9 KB, free 631.8 MB)
2017-11-03 10:34:11,709 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.5 KB, free 631.8 MB)
2017-11-03 10:34:11,709 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:59859 (size: 4.5 KB, free: 631.8 MB)
2017-11-03 10:34:11,709 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:34:11,709 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at treeAggregate at RowMatrix.scala:122) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:34:11,709 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 2 tasks
2017-11-03 10:34:11,725 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:34:11,725 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:34:11,725 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 3)
2017-11-03 10:34:11,725 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 2.0 (TID 4)
2017-11-03 10:34:11,741 WARN[com.github.fommil.netlib.BLAS:61] - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
2017-11-03 10:34:11,741 WARN[com.github.fommil.netlib.BLAS:61] - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
2017-11-03 10:34:11,741 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 3). 1094 bytes result sent to driver
2017-11-03 10:34:11,741 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 2.0 (TID 4). 1137 bytes result sent to driver
2017-11-03 10:34:11,741 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 3) in 16 ms on localhost (executor driver) (1/2)
2017-11-03 10:34:11,741 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 4) in 16 ms on localhost (executor driver) (2/2)
2017-11-03 10:34:11,741 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-11-03 10:34:11,741 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (treeAggregate at RowMatrix.scala:122) finished in 0.016 s
2017-11-03 10:34:11,741 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 2 finished: treeAggregate at RowMatrix.scala:122, took 0.038408 s
2017-11-03 10:34:11,991 WARN[org.apache.spark.mllib.stat.correlation.PearsonCorrelation:66] - Pearson correlation matrix contains NaN values.
2017-11-03 10:34:12,038 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 27.597565 ms
2017-11-03 10:34:12,053 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 7.665403 ms
2017-11-03 10:34:12,100 INFO[org.apache.spark.SparkContext:54] - Starting job: sortByKey at SpearmanCorrelation.scala:54
2017-11-03 10:34:12,100 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 3 (sortByKey at SpearmanCorrelation.scala:54) with 2 output partitions
2017-11-03 10:34:12,100 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 3 (sortByKey at SpearmanCorrelation.scala:54)
2017-11-03 10:34:12,100 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 10:34:12,100 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:34:12,100 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 3 (MapPartitionsRDD[15] at sortByKey at SpearmanCorrelation.scala:54), which has no missing parents
2017-11-03 10:34:12,100 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 8.9 KB, free 631.8 MB)
2017-11-03 10:34:12,100 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.5 KB, free 631.7 MB)
2017-11-03 10:34:12,100 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:59859 (size: 4.5 KB, free: 631.8 MB)
2017-11-03 10:34:12,100 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:34:12,100 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at sortByKey at SpearmanCorrelation.scala:54) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:34:12,100 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 2 tasks
2017-11-03 10:34:12,100 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:34:12,100 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 3.0 (TID 6, localhost, executor driver, partition 1, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:34:12,100 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 5)
2017-11-03 10:34:12,100 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 3.0 (TID 6)
2017-11-03 10:34:12,131 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 3.0 (TID 6). 1384 bytes result sent to driver
2017-11-03 10:34:12,131 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 5). 1427 bytes result sent to driver
2017-11-03 10:34:12,131 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 5) in 31 ms on localhost (executor driver) (1/2)
2017-11-03 10:34:12,147 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 3.0 (TID 6) in 47 ms on localhost (executor driver) (2/2)
2017-11-03 10:34:12,147 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2017-11-03 10:34:12,147 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 3 (sortByKey at SpearmanCorrelation.scala:54) finished in 0.047 s
2017-11-03 10:34:12,147 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 3 finished: sortByKey at SpearmanCorrelation.scala:54, took 0.047858 s
2017-11-03 10:34:12,147 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_2_piece0 on 192.168.11.26:59859 in memory (size: 4.5 KB, free: 631.8 MB)
2017-11-03 10:34:12,147 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:59859 in memory (size: 4.6 KB, free: 631.8 MB)
2017-11-03 10:34:12,147 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_0_piece0 on 192.168.11.26:59859 in memory (size: 4.2 KB, free: 631.8 MB)
2017-11-03 10:34:12,147 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 98
2017-11-03 10:34:12,147 INFO[org.apache.spark.SparkContext:54] - Starting job: zipWithIndex at SpearmanCorrelation.scala:56
2017-11-03 10:34:12,163 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 13 (flatMap at SpearmanCorrelation.scala:48)
2017-11-03 10:34:12,163 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 4 (zipWithIndex at SpearmanCorrelation.scala:56) with 1 output partitions
2017-11-03 10:34:12,163 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 5 (zipWithIndex at SpearmanCorrelation.scala:56)
2017-11-03 10:34:12,163 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 4)
2017-11-03 10:34:12,163 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 4)
2017-11-03 10:34:12,163 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 4 (MapPartitionsRDD[13] at flatMap at SpearmanCorrelation.scala:48), which has no missing parents
2017-11-03 10:34:12,163 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 9.8 KB, free 631.8 MB)
2017-11-03 10:34:12,163 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.1 KB, free 631.8 MB)
2017-11-03 10:34:12,178 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.11.26:59859 (size: 5.1 KB, free: 631.8 MB)
2017-11-03 10:34:12,178 INFO[org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:34:12,178 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[13] at flatMap at SpearmanCorrelation.scala:48) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:34:12,178 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 4.0 with 2 tasks
2017-11-03 10:34:12,178 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 4.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 5209 bytes)
2017-11-03 10:34:12,178 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 4.0 (TID 8, localhost, executor driver, partition 1, PROCESS_LOCAL, 5209 bytes)
2017-11-03 10:34:12,178 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 4.0 (TID 7)
2017-11-03 10:34:12,178 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 4.0 (TID 8)
2017-11-03 10:34:12,225 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 4.0 (TID 7). 1116 bytes result sent to driver
2017-11-03 10:34:12,241 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 4.0 (TID 8). 1073 bytes result sent to driver
2017-11-03 10:34:12,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 4.0 (TID 7) in 63 ms on localhost (executor driver) (1/2)
2017-11-03 10:34:12,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 4.0 (TID 8) in 63 ms on localhost (executor driver) (2/2)
2017-11-03 10:34:12,241 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2017-11-03 10:34:12,241 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 4 (flatMap at SpearmanCorrelation.scala:48) finished in 0.063 s
2017-11-03 10:34:12,241 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2017-11-03 10:34:12,241 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2017-11-03 10:34:12,241 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 5)
2017-11-03 10:34:12,241 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2017-11-03 10:34:12,241 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 5 (ShuffledRDD[16] at sortByKey at SpearmanCorrelation.scala:54), which has no missing parents
2017-11-03 10:34:12,256 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5 stored as values in memory (estimated size 3.1 KB, free 631.8 MB)
2017-11-03 10:34:12,256 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1897.0 B, free 631.8 MB)
2017-11-03 10:34:12,256 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_5_piece0 in memory on 192.168.11.26:59859 (size: 1897.0 B, free: 631.8 MB)
2017-11-03 10:34:12,256 INFO[org.apache.spark.SparkContext:54] - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:34:12,256 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 5 (ShuffledRDD[16] at sortByKey at SpearmanCorrelation.scala:54) (first 15 tasks are for partitions Vector(0))
2017-11-03 10:34:12,256 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 5.0 with 1 tasks
2017-11-03 10:34:12,272 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 5.0 (TID 9, localhost, executor driver, partition 0, ANY, 4621 bytes)
2017-11-03 10:34:12,272 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 5.0 (TID 9)
2017-11-03 10:34:12,288 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:34:12,288 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 16 ms
2017-11-03 10:34:12,320 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 5.0 (TID 9). 1047 bytes result sent to driver
2017-11-03 10:34:12,320 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 5.0 (TID 9) in 48 ms on localhost (executor driver) (1/1)
2017-11-03 10:34:12,320 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2017-11-03 10:34:12,320 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 5 (zipWithIndex at SpearmanCorrelation.scala:56) finished in 0.064 s
2017-11-03 10:34:12,320 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 4 finished: zipWithIndex at SpearmanCorrelation.scala:56, took 0.164840 s
2017-11-03 10:34:12,367 INFO[org.apache.spark.SparkContext:54] - Starting job: first at RowMatrix.scala:61
2017-11-03 10:34:12,383 INFO[org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 0 is 158 bytes
2017-11-03 10:34:12,383 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 18 (mapPartitions at SpearmanCorrelation.scala:56)
2017-11-03 10:34:12,383 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 5 (first at RowMatrix.scala:61) with 1 output partitions
2017-11-03 10:34:12,383 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 8 (first at RowMatrix.scala:61)
2017-11-03 10:34:12,383 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 7)
2017-11-03 10:34:12,383 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 7)
2017-11-03 10:34:12,383 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 7 (MapPartitionsRDD[18] at mapPartitions at SpearmanCorrelation.scala:56), which has no missing parents
2017-11-03 10:34:12,383 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_6 stored as values in memory (estimated size 4.7 KB, free 631.8 MB)
2017-11-03 10:34:12,383 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.5 KB, free 631.8 MB)
2017-11-03 10:34:12,383 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_6_piece0 in memory on 192.168.11.26:59859 (size: 2.5 KB, free: 631.8 MB)
2017-11-03 10:34:12,383 INFO[org.apache.spark.SparkContext:54] - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:34:12,383 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[18] at mapPartitions at SpearmanCorrelation.scala:56) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:34:12,383 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 7.0 with 2 tasks
2017-11-03 10:34:12,383 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 7.0 (TID 10, localhost, executor driver, partition 0, ANY, 4720 bytes)
2017-11-03 10:34:12,398 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 7.0 (TID 11, localhost, executor driver, partition 1, ANY, 4720 bytes)
2017-11-03 10:34:12,398 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 7.0 (TID 10)
2017-11-03 10:34:12,398 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 7.0 (TID 11)
2017-11-03 10:34:12,398 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:34:12,398 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:34:12,398 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 10:34:12,398 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 10:34:12,430 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 7.0 (TID 10). 1198 bytes result sent to driver
2017-11-03 10:34:12,430 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 7.0 (TID 11). 1198 bytes result sent to driver
2017-11-03 10:34:12,430 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 7.0 (TID 10) in 47 ms on localhost (executor driver) (1/2)
2017-11-03 10:34:12,430 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 7.0 (TID 11) in 32 ms on localhost (executor driver) (2/2)
2017-11-03 10:34:12,430 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2017-11-03 10:34:12,430 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 7 (mapPartitions at SpearmanCorrelation.scala:56) finished in 0.047 s
2017-11-03 10:34:12,430 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2017-11-03 10:34:12,430 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2017-11-03 10:34:12,430 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 8)
2017-11-03 10:34:12,430 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2017-11-03 10:34:12,430 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 8 (MapPartitionsRDD[20] at map at SpearmanCorrelation.scala:86), which has no missing parents
2017-11-03 10:34:12,445 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_7 stored as values in memory (estimated size 5.5 KB, free 631.8 MB)
2017-11-03 10:34:12,445 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.8 KB, free 631.8 MB)
2017-11-03 10:34:12,445 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_7_piece0 in memory on 192.168.11.26:59859 (size: 2.8 KB, free: 631.8 MB)
2017-11-03 10:34:12,445 INFO[org.apache.spark.SparkContext:54] - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:34:12,445 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[20] at map at SpearmanCorrelation.scala:86) (first 15 tasks are for partitions Vector(0))
2017-11-03 10:34:12,445 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 8.0 with 1 tasks
2017-11-03 10:34:12,445 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 8.0 (TID 12, localhost, executor driver, partition 0, ANY, 4621 bytes)
2017-11-03 10:34:12,445 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 8.0 (TID 12)
2017-11-03 10:34:12,445 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:34:12,445 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 10:34:12,461 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 8.0 (TID 12). 1156 bytes result sent to driver
2017-11-03 10:34:12,461 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 8.0 (TID 12) in 16 ms on localhost (executor driver) (1/1)
2017-11-03 10:34:12,461 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2017-11-03 10:34:12,461 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 8 (first at RowMatrix.scala:61) finished in 0.016 s
2017-11-03 10:34:12,461 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 5 finished: first at RowMatrix.scala:61, took 0.091915 s
2017-11-03 10:34:12,476 INFO[org.apache.spark.SparkContext:54] - Starting job: treeAggregate at RowMatrix.scala:419
2017-11-03 10:34:12,476 INFO[org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 0 is 158 bytes
2017-11-03 10:34:12,476 INFO[org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 1 is 158 bytes
2017-11-03 10:34:12,476 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 6 (treeAggregate at RowMatrix.scala:419) with 2 output partitions
2017-11-03 10:34:12,476 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 11 (treeAggregate at RowMatrix.scala:419)
2017-11-03 10:34:12,476 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 10)
2017-11-03 10:34:12,476 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:34:12,476 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 11 (MapPartitionsRDD[21] at treeAggregate at RowMatrix.scala:419), which has no missing parents
2017-11-03 10:34:12,476 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_8 stored as values in memory (estimated size 6.4 KB, free 631.7 MB)
2017-11-03 10:34:12,476 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.3 KB, free 631.7 MB)
2017-11-03 10:34:12,476 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_8_piece0 in memory on 192.168.11.26:59859 (size: 3.3 KB, free: 631.8 MB)
2017-11-03 10:34:12,476 INFO[org.apache.spark.SparkContext:54] - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:34:12,476 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[21] at treeAggregate at RowMatrix.scala:419) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:34:12,476 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 11.0 with 2 tasks
2017-11-03 10:34:12,476 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 11.0 (TID 13, localhost, executor driver, partition 0, ANY, 4621 bytes)
2017-11-03 10:34:12,476 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 11.0 (TID 14, localhost, executor driver, partition 1, ANY, 4621 bytes)
2017-11-03 10:34:12,476 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 11.0 (TID 13)
2017-11-03 10:34:12,476 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 11.0 (TID 14)
2017-11-03 10:34:12,476 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:34:12,476 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 10:34:12,492 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:34:12,492 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 10:34:12,492 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 11.0 (TID 14). 1696 bytes result sent to driver
2017-11-03 10:34:12,492 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 11.0 (TID 13). 1739 bytes result sent to driver
2017-11-03 10:34:12,492 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 11.0 (TID 14) in 16 ms on localhost (executor driver) (1/2)
2017-11-03 10:34:12,492 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 11.0 (TID 13) in 16 ms on localhost (executor driver) (2/2)
2017-11-03 10:34:12,492 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2017-11-03 10:34:12,492 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 11 (treeAggregate at RowMatrix.scala:419) finished in 0.016 s
2017-11-03 10:34:12,492 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 6 finished: treeAggregate at RowMatrix.scala:419, took 0.020464 s
2017-11-03 10:34:12,492 INFO[org.apache.spark.SparkContext:54] - Starting job: treeAggregate at RowMatrix.scala:122
2017-11-03 10:34:12,492 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 7 (treeAggregate at RowMatrix.scala:122) with 2 output partitions
2017-11-03 10:34:12,492 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 14 (treeAggregate at RowMatrix.scala:122)
2017-11-03 10:34:12,492 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 13)
2017-11-03 10:34:12,492 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:34:12,492 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 14 (MapPartitionsRDD[22] at treeAggregate at RowMatrix.scala:122), which has no missing parents
2017-11-03 10:34:12,508 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_9 stored as values in memory (estimated size 6.4 KB, free 631.7 MB)
2017-11-03 10:34:12,508 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.2 KB, free 631.7 MB)
2017-11-03 10:34:12,508 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_9_piece0 in memory on 192.168.11.26:59859 (size: 3.2 KB, free: 631.8 MB)
2017-11-03 10:34:12,508 INFO[org.apache.spark.SparkContext:54] - Created broadcast 9 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:34:12,508 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[22] at treeAggregate at RowMatrix.scala:122) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:34:12,508 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 14.0 with 2 tasks
2017-11-03 10:34:12,508 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 14.0 (TID 15, localhost, executor driver, partition 0, ANY, 4621 bytes)
2017-11-03 10:34:12,508 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 14.0 (TID 16, localhost, executor driver, partition 1, ANY, 4621 bytes)
2017-11-03 10:34:12,508 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 14.0 (TID 15)
2017-11-03 10:34:12,508 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 14.0 (TID 16)
2017-11-03 10:34:12,508 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:34:12,508 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 10:34:12,508 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:34:12,508 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 10:34:12,508 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 14.0 (TID 16). 1176 bytes result sent to driver
2017-11-03 10:34:12,523 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 14.0 (TID 16) in 15 ms on localhost (executor driver) (1/2)
2017-11-03 10:34:12,523 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 14.0 (TID 15). 1219 bytes result sent to driver
2017-11-03 10:34:12,523 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 14.0 (TID 15) in 15 ms on localhost (executor driver) (2/2)
2017-11-03 10:34:12,523 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2017-11-03 10:34:12,523 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 14 (treeAggregate at RowMatrix.scala:122) finished in 0.015 s
2017-11-03 10:34:12,523 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 7 finished: treeAggregate at RowMatrix.scala:122, took 0.024635 s
2017-11-03 10:34:12,523 WARN[org.apache.spark.mllib.stat.correlation.PearsonCorrelation:66] - Pearson correlation matrix contains NaN values.
2017-11-03 10:34:12,539 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2017-11-03 10:34:12,539 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@2d4c73f4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:34:12,539 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2017-11-03 10:34:12,557 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2017-11-03 10:34:12,620 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2017-11-03 10:34:12,620 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2017-11-03 10:34:12,620 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2017-11-03 10:34:12,620 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2017-11-03 10:34:12,620 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2017-11-03 10:34:12,620 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2017-11-03 10:34:12,620 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-b688163a-b7e8-461c-8cc7-ac11d0b966ff
2017-11-03 10:35:26,672 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2017-11-03 10:35:27,349 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2017-11-03 10:35:27,369 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2017-11-03 10:35:27,370 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2017-11-03 10:35:27,370 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2017-11-03 10:35:27,372 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2017-11-03 10:35:27,372 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2017-11-03 10:35:27,737 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 59902.
2017-11-03 10:35:27,757 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2017-11-03 10:35:27,775 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2017-11-03 10:35:27,778 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2017-11-03 10:35:27,778 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2017-11-03 10:35:27,787 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-d3fc24cc-c6d3-4332-b4d1-d380413a9d9b
2017-11-03 10:35:27,838 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2017-11-03 10:35:27,891 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2017-11-03 10:35:27,947 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2501ms
2017-11-03 10:35:28,010 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2017-11-03 10:35:28,010 INFO[org.spark_project.jetty.server.Server:403] - Started @2563ms
2017-11-03 10:35:28,026 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@21a74f18{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:35:28,026 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@30bcf3c1{/jobs,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ee37ca3{/jobs/json,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53812a9b{/jobs/job,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@422c3c7a{/jobs/job/json,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@d8305c2{/stages,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@75d0911a{/stages/json,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@60d1a32f{/stages/stage,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@726e5805{/stages/stage/json,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4b672daa{/stages/pool,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e077866{/stages/pool/json,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c2b6087{/storage,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a8e6492{/storage/json,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3b77a04f{/storage/rdd,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2e11485{/storage/rdd/json,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@662f5666{/environment,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5974109{/environment/json,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ef3efa8{/executors,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f8f9349{/executors/json,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7446d8d5{/executors/threadDump,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4fbda97b{/executors/threadDump/json,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,072 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@37b70343{/static,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,072 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51cd7ffc{/,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,072 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4cc6fa2a{/api,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,072 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6b9ce1bf{/jobs/job/kill,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,072 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@75ed9710{/stages/stage/kill,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,072 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2017-11-03 10:35:28,157 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2017-11-03 10:35:28,193 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59915.
2017-11-03 10:35:28,194 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:59915
2017-11-03 10:35:28,196 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2017-11-03 10:35:28,198 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 59915, None)
2017-11-03 10:35:28,204 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:59915 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 59915, None)
2017-11-03 10:35:28,207 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 59915, None)
2017-11-03 10:35:28,207 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 59915, None)
2017-11-03 10:35:28,388 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63fd4873{/metrics/json,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,437 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2017-11-03 10:35:28,437 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2017-11-03 10:35:28,443 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e9c413e{/SQL,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,444 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5af5def9{/SQL/json,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,445 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@27a0a5a2{/SQL/execution,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,445 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@33aa93c{/SQL/execution/json,null,AVAILABLE,@Spark}
2017-11-03 10:35:28,446 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7dd712e8{/static/sql,null,AVAILABLE,@Spark}
2017-11-03 10:35:29,407 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2017-11-03 10:35:30,596 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: feature
2017-11-03 10:35:30,867 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select *from feature
2017-11-03 10:35:31,362 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 232.595682 ms
2017-11-03 10:35:31,387 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 12.003111 ms
2017-11-03 10:35:31,709 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2017-11-03 10:35:31,714 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@21a74f18{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:35:31,716 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2017-11-03 10:35:31,722 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2017-11-03 10:35:31,728 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2017-11-03 10:35:31,729 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2017-11-03 10:35:31,734 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2017-11-03 10:35:31,736 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2017-11-03 10:35:31,740 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2017-11-03 10:35:31,740 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2017-11-03 10:35:31,741 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-8d001f78-2ef5-41ff-9a24-0b6a54f63dff
2017-11-03 10:36:06,320 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2017-11-03 10:36:06,935 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2017-11-03 10:36:06,951 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2017-11-03 10:36:06,951 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2017-11-03 10:36:06,951 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2017-11-03 10:36:06,951 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2017-11-03 10:36:06,951 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2017-11-03 10:36:07,320 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 60089.
2017-11-03 10:36:07,336 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2017-11-03 10:36:07,352 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2017-11-03 10:36:07,367 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2017-11-03 10:36:07,368 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2017-11-03 10:36:07,376 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-35da46fd-9f6e-434f-9c46-29d2574c76b6
2017-11-03 10:36:07,440 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2017-11-03 10:36:07,472 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2017-11-03 10:36:07,534 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2385ms
2017-11-03 10:36:07,592 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2017-11-03 10:36:07,602 INFO[org.spark_project.jetty.server.Server:403] - Started @2445ms
2017-11-03 10:36:07,604 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@2bd2b28e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:36:07,604 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2017-11-03 10:36:07,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2a3c96e3{/jobs,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@45c8d09f{/jobs/json,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@14b030a0{/jobs/job,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18230356{/jobs/job/json,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@56bca85b{/stages,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@75e91545{/stages/json,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@531c311e{/stages/stage,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40c80397{/stages/stage/json,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@ea9b7c6{/stages/pool,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@ed3068a{/stages/pool/json,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fffff43{/storage,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c7fd41f{/storage/json,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7b324585{/storage/rdd,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@60dce7ea{/storage/rdd/json,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@fd8294b{/environment,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,651 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@27305e6{/environment/json,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,651 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@502f1f4c{/executors,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,651 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@75c9e76b{/executors/json,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,651 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c3b6c6e{/executors/threadDump,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,651 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@75f5fd58{/executors/threadDump/json,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,651 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@306851c7{/static,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,651 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@30d4b288{/,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,651 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40f1be1b{/api,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,667 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@61884cb1{/jobs/job/kill,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,667 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4fc5e095{/stages/stage/kill,null,AVAILABLE,@Spark}
2017-11-03 10:36:07,667 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2017-11-03 10:36:07,783 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2017-11-03 10:36:07,814 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60102.
2017-11-03 10:36:07,814 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:60102
2017-11-03 10:36:07,814 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2017-11-03 10:36:07,814 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 60102, None)
2017-11-03 10:36:07,814 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:60102 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 60102, None)
2017-11-03 10:36:07,830 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 60102, None)
2017-11-03 10:36:07,836 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 60102, None)
2017-11-03 10:36:08,009 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1e11bc55{/metrics/json,null,AVAILABLE,@Spark}
2017-11-03 10:36:08,056 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2017-11-03 10:36:08,056 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2017-11-03 10:36:08,056 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@57a4d5ee{/SQL,null,AVAILABLE,@Spark}
2017-11-03 10:36:08,056 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3a45c42a{/SQL/json,null,AVAILABLE,@Spark}
2017-11-03 10:36:08,056 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7692cd34{/SQL/execution,null,AVAILABLE,@Spark}
2017-11-03 10:36:08,056 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@32c0915e{/SQL/execution/json,null,AVAILABLE,@Spark}
2017-11-03 10:36:08,056 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c282004{/static/sql,null,AVAILABLE,@Spark}
2017-11-03 10:36:08,999 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2017-11-03 10:36:10,132 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: feature
2017-11-03 10:36:10,379 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select *from feature
2017-11-03 10:36:10,823 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 199.421061 ms
2017-11-03 10:36:10,839 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 8.676059 ms
2017-11-03 10:36:11,216 INFO[org.apache.spark.SparkContext:54] - Starting job: first at RowMatrix.scala:61
2017-11-03 10:36:11,337 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 0
2017-11-03 10:36:11,337 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (first at RowMatrix.scala:61) with 1 output partitions
2017-11-03 10:36:11,337 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (first at RowMatrix.scala:61)
2017-11-03 10:36:11,337 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 10:36:11,352 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:36:11,352 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[4] at map at Correlation.scala:70), which has no missing parents
2017-11-03 10:36:11,427 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 8.1 KB, free 631.8 MB)
2017-11-03 10:36:11,458 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 631.8 MB)
2017-11-03 10:36:11,458 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:60102 (size: 4.2 KB, free: 631.8 MB)
2017-11-03 10:36:11,458 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:36:11,490 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at map at Correlation.scala:70) (first 15 tasks are for partitions Vector(0))
2017-11-03 10:36:11,490 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2017-11-03 10:36:11,505 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:36:11,521 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2017-11-03 10:36:11,624 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1159 bytes result sent to driver
2017-11-03 10:36:11,624 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 119 ms on localhost (executor driver) (1/1)
2017-11-03 10:36:11,639 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (first at RowMatrix.scala:61) finished in 0.149 s
2017-11-03 10:36:11,639 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: first at RowMatrix.scala:61, took 0.424069 s
2017-11-03 10:36:11,655 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2017-11-03 10:36:11,705 INFO[org.apache.spark.SparkContext:54] - Starting job: treeAggregate at RowMatrix.scala:419
2017-11-03 10:36:11,710 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (treeAggregate at RowMatrix.scala:419) with 2 output partitions
2017-11-03 10:36:11,710 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (treeAggregate at RowMatrix.scala:419)
2017-11-03 10:36:11,710 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 10:36:11,710 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:36:11,711 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[5] at treeAggregate at RowMatrix.scala:419), which has no missing parents
2017-11-03 10:36:11,717 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.0 KB, free 631.8 MB)
2017-11-03 10:36:11,719 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.6 KB, free 631.8 MB)
2017-11-03 10:36:11,719 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:60102 (size: 4.6 KB, free: 631.8 MB)
2017-11-03 10:36:11,719 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:36:11,719 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at treeAggregate at RowMatrix.scala:419) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:36:11,719 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 2 tasks
2017-11-03 10:36:11,719 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:36:11,719 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 2, localhost, executor driver, partition 1, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:36:11,719 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2017-11-03 10:36:11,719 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 1.0 (TID 2)
2017-11-03 10:36:11,750 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 1.0 (TID 2). 1657 bytes result sent to driver
2017-11-03 10:36:11,750 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1700 bytes result sent to driver
2017-11-03 10:36:11,750 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 2) in 31 ms on localhost (executor driver) (1/2)
2017-11-03 10:36:11,750 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 31 ms on localhost (executor driver) (2/2)
2017-11-03 10:36:11,750 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2017-11-03 10:36:11,750 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (treeAggregate at RowMatrix.scala:419) finished in 0.031 s
2017-11-03 10:36:11,750 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: treeAggregate at RowMatrix.scala:419, took 0.054815 s
2017-11-03 10:36:12,082 INFO[org.apache.spark.SparkContext:54] - Starting job: treeAggregate at RowMatrix.scala:122
2017-11-03 10:36:12,082 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 2 (treeAggregate at RowMatrix.scala:122) with 2 output partitions
2017-11-03 10:36:12,097 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (treeAggregate at RowMatrix.scala:122)
2017-11-03 10:36:12,097 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 10:36:12,097 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:36:12,097 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[6] at treeAggregate at RowMatrix.scala:122), which has no missing parents
2017-11-03 10:36:12,097 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 8.9 KB, free 631.8 MB)
2017-11-03 10:36:12,097 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.5 KB, free 631.8 MB)
2017-11-03 10:36:12,097 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:60102 (size: 4.5 KB, free: 631.8 MB)
2017-11-03 10:36:12,097 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:36:12,097 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at treeAggregate at RowMatrix.scala:122) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:36:12,097 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 2 tasks
2017-11-03 10:36:12,097 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:36:12,097 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:36:12,097 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 3)
2017-11-03 10:36:12,097 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 2.0 (TID 4)
2017-11-03 10:36:12,113 WARN[com.github.fommil.netlib.BLAS:61] - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
2017-11-03 10:36:12,113 WARN[com.github.fommil.netlib.BLAS:61] - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
2017-11-03 10:36:12,113 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 2.0 (TID 4). 1137 bytes result sent to driver
2017-11-03 10:36:12,113 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 3). 1094 bytes result sent to driver
2017-11-03 10:36:12,129 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 3) in 32 ms on localhost (executor driver) (1/2)
2017-11-03 10:36:12,129 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 4) in 32 ms on localhost (executor driver) (2/2)
2017-11-03 10:36:12,129 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2017-11-03 10:36:12,129 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (treeAggregate at RowMatrix.scala:122) finished in 0.032 s
2017-11-03 10:36:12,129 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 2 finished: treeAggregate at RowMatrix.scala:122, took 0.035253 s
2017-11-03 10:36:12,305 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_2_piece0 on 192.168.11.26:60102 in memory (size: 4.5 KB, free: 631.8 MB)
2017-11-03 10:36:12,321 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:60102 in memory (size: 4.6 KB, free: 631.8 MB)
2017-11-03 10:36:12,321 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_0_piece0 on 192.168.11.26:60102 in memory (size: 4.2 KB, free: 631.8 MB)
2017-11-03 10:36:12,384 WARN[org.apache.spark.mllib.stat.correlation.PearsonCorrelation:66] - Pearson correlation matrix contains NaN values.
2017-11-03 10:36:12,430 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 25.780244 ms
2017-11-03 10:36:12,450 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 10.352896 ms
2017-11-03 10:36:12,497 INFO[org.apache.spark.SparkContext:54] - Starting job: sortByKey at SpearmanCorrelation.scala:54
2017-11-03 10:36:12,497 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 3 (sortByKey at SpearmanCorrelation.scala:54) with 2 output partitions
2017-11-03 10:36:12,497 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 3 (sortByKey at SpearmanCorrelation.scala:54)
2017-11-03 10:36:12,497 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2017-11-03 10:36:12,497 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:36:12,497 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 3 (MapPartitionsRDD[15] at sortByKey at SpearmanCorrelation.scala:54), which has no missing parents
2017-11-03 10:36:12,497 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 8.9 KB, free 631.8 MB)
2017-11-03 10:36:12,497 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.5 KB, free 631.8 MB)
2017-11-03 10:36:12,512 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:60102 (size: 4.5 KB, free: 631.8 MB)
2017-11-03 10:36:12,512 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:36:12,512 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at sortByKey at SpearmanCorrelation.scala:54) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:36:12,512 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 2 tasks
2017-11-03 10:36:12,512 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:36:12,512 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 3.0 (TID 6, localhost, executor driver, partition 1, PROCESS_LOCAL, 5220 bytes)
2017-11-03 10:36:12,512 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 5)
2017-11-03 10:36:12,512 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 3.0 (TID 6)
2017-11-03 10:36:12,543 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 5). 1427 bytes result sent to driver
2017-11-03 10:36:12,543 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 3.0 (TID 6). 1384 bytes result sent to driver
2017-11-03 10:36:12,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 5) in 31 ms on localhost (executor driver) (1/2)
2017-11-03 10:36:12,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 3.0 (TID 6) in 31 ms on localhost (executor driver) (2/2)
2017-11-03 10:36:12,543 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2017-11-03 10:36:12,543 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 3 (sortByKey at SpearmanCorrelation.scala:54) finished in 0.031 s
2017-11-03 10:36:12,543 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 3 finished: sortByKey at SpearmanCorrelation.scala:54, took 0.052739 s
2017-11-03 10:36:12,575 INFO[org.apache.spark.SparkContext:54] - Starting job: zipWithIndex at SpearmanCorrelation.scala:56
2017-11-03 10:36:12,578 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 13 (flatMap at SpearmanCorrelation.scala:48)
2017-11-03 10:36:12,578 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 4 (zipWithIndex at SpearmanCorrelation.scala:56) with 1 output partitions
2017-11-03 10:36:12,578 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 5 (zipWithIndex at SpearmanCorrelation.scala:56)
2017-11-03 10:36:12,578 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 4)
2017-11-03 10:36:12,578 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 4)
2017-11-03 10:36:12,578 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 4 (MapPartitionsRDD[13] at flatMap at SpearmanCorrelation.scala:48), which has no missing parents
2017-11-03 10:36:12,578 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 9.8 KB, free 631.8 MB)
2017-11-03 10:36:12,594 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.1 KB, free 631.8 MB)
2017-11-03 10:36:12,594 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.11.26:60102 (size: 5.1 KB, free: 631.8 MB)
2017-11-03 10:36:12,594 INFO[org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:36:12,594 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[13] at flatMap at SpearmanCorrelation.scala:48) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:36:12,594 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 4.0 with 2 tasks
2017-11-03 10:36:12,594 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 4.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 5209 bytes)
2017-11-03 10:36:12,594 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 4.0 (TID 8, localhost, executor driver, partition 1, PROCESS_LOCAL, 5209 bytes)
2017-11-03 10:36:12,594 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 4.0 (TID 7)
2017-11-03 10:36:12,594 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 4.0 (TID 8)
2017-11-03 10:36:12,656 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 4.0 (TID 8). 1202 bytes result sent to driver
2017-11-03 10:36:12,656 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 4.0 (TID 7). 1159 bytes result sent to driver
2017-11-03 10:36:12,656 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 4.0 (TID 8) in 62 ms on localhost (executor driver) (1/2)
2017-11-03 10:36:12,656 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 4.0 (TID 7) in 62 ms on localhost (executor driver) (2/2)
2017-11-03 10:36:12,656 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2017-11-03 10:36:12,656 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 4 (flatMap at SpearmanCorrelation.scala:48) finished in 0.062 s
2017-11-03 10:36:12,656 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2017-11-03 10:36:12,656 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2017-11-03 10:36:12,656 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 5)
2017-11-03 10:36:12,656 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2017-11-03 10:36:12,656 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 5 (ShuffledRDD[16] at sortByKey at SpearmanCorrelation.scala:54), which has no missing parents
2017-11-03 10:36:12,672 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5 stored as values in memory (estimated size 3.1 KB, free 631.8 MB)
2017-11-03 10:36:12,672 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 1897.0 B, free 631.8 MB)
2017-11-03 10:36:12,672 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_5_piece0 in memory on 192.168.11.26:60102 (size: 1897.0 B, free: 631.8 MB)
2017-11-03 10:36:12,672 INFO[org.apache.spark.SparkContext:54] - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:36:12,672 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 5 (ShuffledRDD[16] at sortByKey at SpearmanCorrelation.scala:54) (first 15 tasks are for partitions Vector(0))
2017-11-03 10:36:12,672 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 5.0 with 1 tasks
2017-11-03 10:36:12,672 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 5.0 (TID 9, localhost, executor driver, partition 0, ANY, 4621 bytes)
2017-11-03 10:36:12,672 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 5.0 (TID 9)
2017-11-03 10:36:12,694 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:36:12,696 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
2017-11-03 10:36:12,725 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 5.0 (TID 9). 1047 bytes result sent to driver
2017-11-03 10:36:12,725 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 5.0 (TID 9) in 53 ms on localhost (executor driver) (1/1)
2017-11-03 10:36:12,725 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2017-11-03 10:36:12,725 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 5 (zipWithIndex at SpearmanCorrelation.scala:56) finished in 0.053 s
2017-11-03 10:36:12,725 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 4 finished: zipWithIndex at SpearmanCorrelation.scala:56, took 0.152283 s
2017-11-03 10:36:12,756 INFO[org.apache.spark.SparkContext:54] - Starting job: first at RowMatrix.scala:61
2017-11-03 10:36:12,756 INFO[org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 0 is 158 bytes
2017-11-03 10:36:12,756 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 18 (mapPartitions at SpearmanCorrelation.scala:56)
2017-11-03 10:36:12,772 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 5 (first at RowMatrix.scala:61) with 1 output partitions
2017-11-03 10:36:12,772 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 8 (first at RowMatrix.scala:61)
2017-11-03 10:36:12,772 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 7)
2017-11-03 10:36:12,772 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 7)
2017-11-03 10:36:12,772 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 7 (MapPartitionsRDD[18] at mapPartitions at SpearmanCorrelation.scala:56), which has no missing parents
2017-11-03 10:36:12,772 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_6 stored as values in memory (estimated size 4.7 KB, free 631.8 MB)
2017-11-03 10:36:12,772 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 2.5 KB, free 631.8 MB)
2017-11-03 10:36:12,772 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_6_piece0 in memory on 192.168.11.26:60102 (size: 2.5 KB, free: 631.8 MB)
2017-11-03 10:36:12,772 INFO[org.apache.spark.SparkContext:54] - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:36:12,772 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[18] at mapPartitions at SpearmanCorrelation.scala:56) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:36:12,772 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 7.0 with 2 tasks
2017-11-03 10:36:12,772 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 7.0 (TID 10, localhost, executor driver, partition 0, ANY, 4720 bytes)
2017-11-03 10:36:12,772 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 7.0 (TID 11, localhost, executor driver, partition 1, ANY, 4720 bytes)
2017-11-03 10:36:12,772 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 7.0 (TID 10)
2017-11-03 10:36:12,772 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 7.0 (TID 11)
2017-11-03 10:36:12,772 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:36:12,772 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:36:12,772 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 10:36:12,772 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 10:36:12,803 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 7.0 (TID 11). 1198 bytes result sent to driver
2017-11-03 10:36:12,803 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 7.0 (TID 11) in 31 ms on localhost (executor driver) (1/2)
2017-11-03 10:36:12,803 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 7.0 (TID 10). 1198 bytes result sent to driver
2017-11-03 10:36:12,803 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 7.0 (TID 10) in 31 ms on localhost (executor driver) (2/2)
2017-11-03 10:36:12,803 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 7.0, whose tasks have all completed, from pool 
2017-11-03 10:36:12,803 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 7 (mapPartitions at SpearmanCorrelation.scala:56) finished in 0.031 s
2017-11-03 10:36:12,803 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2017-11-03 10:36:12,803 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2017-11-03 10:36:12,803 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 8)
2017-11-03 10:36:12,803 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2017-11-03 10:36:12,803 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 8 (MapPartitionsRDD[20] at map at SpearmanCorrelation.scala:86), which has no missing parents
2017-11-03 10:36:12,803 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_7 stored as values in memory (estimated size 5.5 KB, free 631.8 MB)
2017-11-03 10:36:12,819 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 2.8 KB, free 631.8 MB)
2017-11-03 10:36:12,819 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_7_piece0 in memory on 192.168.11.26:60102 (size: 2.8 KB, free: 631.8 MB)
2017-11-03 10:36:12,819 INFO[org.apache.spark.SparkContext:54] - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:36:12,819 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[20] at map at SpearmanCorrelation.scala:86) (first 15 tasks are for partitions Vector(0))
2017-11-03 10:36:12,819 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 8.0 with 1 tasks
2017-11-03 10:36:12,819 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 8.0 (TID 12, localhost, executor driver, partition 0, ANY, 4621 bytes)
2017-11-03 10:36:12,819 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 8.0 (TID 12)
2017-11-03 10:36:12,819 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:36:12,819 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 10:36:12,834 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 8.0 (TID 12). 1113 bytes result sent to driver
2017-11-03 10:36:12,834 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 8.0 (TID 12) in 15 ms on localhost (executor driver) (1/1)
2017-11-03 10:36:12,834 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2017-11-03 10:36:12,834 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 8 (first at RowMatrix.scala:61) finished in 0.015 s
2017-11-03 10:36:12,834 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 5 finished: first at RowMatrix.scala:61, took 0.075201 s
2017-11-03 10:36:12,850 INFO[org.apache.spark.SparkContext:54] - Starting job: treeAggregate at RowMatrix.scala:419
2017-11-03 10:36:12,850 INFO[org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 0 is 158 bytes
2017-11-03 10:36:12,850 INFO[org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 1 is 158 bytes
2017-11-03 10:36:12,850 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 6 (treeAggregate at RowMatrix.scala:419) with 2 output partitions
2017-11-03 10:36:12,850 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 11 (treeAggregate at RowMatrix.scala:419)
2017-11-03 10:36:12,850 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 10)
2017-11-03 10:36:12,850 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:36:12,850 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 11 (MapPartitionsRDD[21] at treeAggregate at RowMatrix.scala:419), which has no missing parents
2017-11-03 10:36:12,850 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_8 stored as values in memory (estimated size 6.4 KB, free 631.7 MB)
2017-11-03 10:36:12,850 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.3 KB, free 631.7 MB)
2017-11-03 10:36:12,850 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_8_piece0 in memory on 192.168.11.26:60102 (size: 3.3 KB, free: 631.8 MB)
2017-11-03 10:36:12,850 INFO[org.apache.spark.SparkContext:54] - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:36:12,850 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 11 (MapPartitionsRDD[21] at treeAggregate at RowMatrix.scala:419) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:36:12,850 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 11.0 with 2 tasks
2017-11-03 10:36:12,850 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 11.0 (TID 13, localhost, executor driver, partition 0, ANY, 4621 bytes)
2017-11-03 10:36:12,850 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 11.0 (TID 14, localhost, executor driver, partition 1, ANY, 4621 bytes)
2017-11-03 10:36:12,850 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 11.0 (TID 13)
2017-11-03 10:36:12,850 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 11.0 (TID 14)
2017-11-03 10:36:12,868 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:36:12,868 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 10:36:12,869 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:36:12,869 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 10:36:12,872 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 11.0 (TID 14). 1782 bytes result sent to driver
2017-11-03 10:36:12,872 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 11.0 (TID 13). 1782 bytes result sent to driver
2017-11-03 10:36:12,874 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 11.0 (TID 14) in 24 ms on localhost (executor driver) (1/2)
2017-11-03 10:36:12,875 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 11.0 (TID 13) in 25 ms on localhost (executor driver) (2/2)
2017-11-03 10:36:12,875 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2017-11-03 10:36:12,875 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 11 (treeAggregate at RowMatrix.scala:419) finished in 0.025 s
2017-11-03 10:36:12,875 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 6 finished: treeAggregate at RowMatrix.scala:419, took 0.022548 s
2017-11-03 10:36:12,875 INFO[org.apache.spark.SparkContext:54] - Starting job: treeAggregate at RowMatrix.scala:122
2017-11-03 10:36:12,875 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 7 (treeAggregate at RowMatrix.scala:122) with 2 output partitions
2017-11-03 10:36:12,875 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 14 (treeAggregate at RowMatrix.scala:122)
2017-11-03 10:36:12,875 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 13)
2017-11-03 10:36:12,875 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2017-11-03 10:36:12,875 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 14 (MapPartitionsRDD[22] at treeAggregate at RowMatrix.scala:122), which has no missing parents
2017-11-03 10:36:12,875 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_9 stored as values in memory (estimated size 6.4 KB, free 631.7 MB)
2017-11-03 10:36:12,875 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.2 KB, free 631.7 MB)
2017-11-03 10:36:12,875 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_9_piece0 in memory on 192.168.11.26:60102 (size: 3.2 KB, free: 631.8 MB)
2017-11-03 10:36:12,890 INFO[org.apache.spark.SparkContext:54] - Created broadcast 9 from broadcast at DAGScheduler.scala:1006
2017-11-03 10:36:12,890 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 14 (MapPartitionsRDD[22] at treeAggregate at RowMatrix.scala:122) (first 15 tasks are for partitions Vector(0, 1))
2017-11-03 10:36:12,890 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 14.0 with 2 tasks
2017-11-03 10:36:12,890 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 14.0 (TID 15, localhost, executor driver, partition 0, ANY, 4621 bytes)
2017-11-03 10:36:12,890 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 14.0 (TID 16, localhost, executor driver, partition 1, ANY, 4621 bytes)
2017-11-03 10:36:12,890 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 14.0 (TID 15)
2017-11-03 10:36:12,890 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 14.0 (TID 16)
2017-11-03 10:36:12,890 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:36:12,890 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 10:36:12,890 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2017-11-03 10:36:12,890 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2017-11-03 10:36:12,890 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 14.0 (TID 16). 1176 bytes result sent to driver
2017-11-03 10:36:12,890 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 14.0 (TID 16) in 0 ms on localhost (executor driver) (1/2)
2017-11-03 10:36:12,906 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 14.0 (TID 15). 1176 bytes result sent to driver
2017-11-03 10:36:12,906 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 14.0 (TID 15) in 16 ms on localhost (executor driver) (2/2)
2017-11-03 10:36:12,906 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 14.0, whose tasks have all completed, from pool 
2017-11-03 10:36:12,906 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 14 (treeAggregate at RowMatrix.scala:122) finished in 0.016 s
2017-11-03 10:36:12,906 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 7 finished: treeAggregate at RowMatrix.scala:122, took 0.027582 s
2017-11-03 10:36:12,906 WARN[org.apache.spark.mllib.stat.correlation.PearsonCorrelation:66] - Pearson correlation matrix contains NaN values.
2017-11-03 10:36:12,922 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2017-11-03 10:36:12,922 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@2bd2b28e{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2017-11-03 10:36:12,922 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2017-11-03 10:36:12,937 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2017-11-03 10:36:13,017 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2017-11-03 10:36:13,017 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2017-11-03 10:36:13,017 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2017-11-03 10:36:13,017 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2017-11-03 10:36:13,017 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2017-11-03 10:36:13,017 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2017-11-03 10:36:13,017 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-08d0c61c-5498-469a-93a3-d344997e5fda
