2018-02-06 09:36:00,837 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 09:36:01,638 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 09:36:01,683 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 09:36:01,685 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 09:36:01,686 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 09:36:01,686 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 09:36:01,687 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 09:36:02,141 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 60031.
2018-02-06 09:36:02,169 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 09:36:02,226 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 09:36:02,230 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 09:36:02,231 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 09:36:02,245 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-e2d69868-856d-4aa3-b155-bf9312127105
2018-02-06 09:36:02,283 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 09:36:02,346 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 09:36:02,461 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3113ms
2018-02-06 09:36:02,551 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 09:36:02,570 INFO[org.spark_project.jetty.server.Server:403] - Started @3224ms
2018-02-06 09:36:02,598 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@748ba9a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 09:36:02,599 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 09:36:02,630 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,631 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,632 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,633 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,633 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/stages,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,634 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,635 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ce3db41{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,637 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,638 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,638 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/storage,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,639 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,640 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,640 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,642 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/environment,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,643 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,644 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/executors,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,645 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,645 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,646 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,658 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/static,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,659 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,661 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/api,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,662 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,663 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 09:36:02,666 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 09:36:02,779 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 09:36:02,811 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60044.
2018-02-06 09:36:02,812 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:60044
2018-02-06 09:36:02,813 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 09:36:02,817 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 60044, None)
2018-02-06 09:36:02,821 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:60044 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 60044, None)
2018-02-06 09:36:02,827 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 60044, None)
2018-02-06 09:36:02,829 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 60044, None)
2018-02-06 09:36:03,041 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@420bc288{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:03,136 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 09:36:03,137 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 09:36:03,148 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@742d4e15{/SQL,null,AVAILABLE,@Spark}
2018-02-06 09:36:03,149 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50b1f030{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:03,150 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@23aae55{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 09:36:03,151 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@680bddf5{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:03,153 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53bf7094{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 09:36:04,434 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 09:36:06,239 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 228.344393 ms
2018-02-06 09:36:06,778 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 9.226243 ms
2018-02-06 09:36:06,798 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 8.237122 ms
2018-02-06 09:36:06,825 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 09:36:06,832 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@748ba9a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 09:36:06,833 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 09:36:06,843 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 09:36:06,851 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 09:36:06,851 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 09:36:06,858 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 09:36:06,862 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 09:36:06,865 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 09:36:06,866 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 09:36:06,867 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-6c327ea1-2a73-4f7c-a728-190c07212296
2018-02-06 09:36:36,898 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 09:36:37,484 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 09:36:37,508 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 09:36:37,509 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 09:36:37,510 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 09:36:37,511 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 09:36:37,512 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 09:36:37,900 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 60067.
2018-02-06 09:36:37,920 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 09:36:37,974 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 09:36:37,977 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 09:36:37,977 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 09:36:37,986 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-b5df0b78-3a77-4388-8912-99d14704231a
2018-02-06 09:36:38,013 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 09:36:38,064 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 09:36:38,147 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2660ms
2018-02-06 09:36:38,219 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 09:36:38,236 INFO[org.spark_project.jetty.server.Server:403] - Started @2751ms
2018-02-06 09:36:38,259 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@4b3c354a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 09:36:38,260 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 09:36:38,287 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c51bb7{/jobs,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,288 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,289 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,290 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,291 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/stages,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,291 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,293 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,295 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64df9a61{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,295 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e260766{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,297 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a97744{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,297 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c56e013{/storage,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,298 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40258c2f{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,299 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6731787b{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,299 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,300 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/environment,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,301 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,302 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/executors,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,303 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,304 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,305 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5860f3d7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,313 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/static,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,313 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@59d2103b{/,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,315 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e4de19b{/api,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,316 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c6daf0{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,316 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@659eef7{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,319 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 09:36:38,423 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 09:36:38,452 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60080.
2018-02-06 09:36:38,453 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:60080
2018-02-06 09:36:38,458 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 09:36:38,461 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 60080, None)
2018-02-06 09:36:38,468 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:60080 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 60080, None)
2018-02-06 09:36:38,475 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 60080, None)
2018-02-06 09:36:38,476 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 60080, None)
2018-02-06 09:36:38,715 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@437e951d{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,791 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 09:36:38,793 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 09:36:38,804 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@553f3b6e{/SQL,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,804 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e406694{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,805 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@15b986cd{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,806 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@41c62850{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 09:36:38,808 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7d2a6eac{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 09:36:39,891 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 09:36:41,450 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 180.737978 ms
2018-02-06 09:36:42,022 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 8.510083 ms
2018-02-06 09:36:42,195 INFO[org.apache.spark.SparkContext:54] - Starting job: json at SchemaRDDSparkSQL.java:86
2018-02-06 09:36:42,213 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at SchemaRDDSparkSQL.java:86) with 1 output partitions
2018-02-06 09:36:42,214 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at SchemaRDDSparkSQL.java:86)
2018-02-06 09:36:42,214 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 09:36:42,216 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 09:36:42,223 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at SchemaRDDSparkSQL.java:86), which has no missing parents
2018-02-06 09:36:42,486 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 5.8 KB, free 631.8 MB)
2018-02-06 09:36:42,524 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.6 KB, free 631.8 MB)
2018-02-06 09:36:42,536 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:60080 (size: 3.6 KB, free: 631.8 MB)
2018-02-06 09:36:42,540 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-06 09:36:42,554 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at SchemaRDDSparkSQL.java:86) (first 15 tasks are for partitions Vector(0))
2018-02-06 09:36:42,555 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 09:36:42,600 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5067 bytes)
2018-02-06 09:36:42,613 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 09:36:42,756 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1797 bytes result sent to driver
2018-02-06 09:36:42,765 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 180 ms on localhost (executor driver) (1/1)
2018-02-06 09:36:42,768 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 09:36:42,777 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at SchemaRDDSparkSQL.java:86) finished in 0.204 s
2018-02-06 09:36:42,785 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at SchemaRDDSparkSQL.java:86, took 0.590442 s
2018-02-06 09:36:42,978 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:87
2018-02-06 09:36:42,979 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at SchemaRDDSparkSQL.java:87) with 1 output partitions
2018-02-06 09:36:42,979 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at SchemaRDDSparkSQL.java:87)
2018-02-06 09:36:42,979 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 09:36:42,980 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 09:36:42,980 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[10] at show at SchemaRDDSparkSQL.java:87), which has no missing parents
2018-02-06 09:36:43,012 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.8 KB, free 631.8 MB)
2018-02-06 09:36:43,018 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KB, free 631.8 MB)
2018-02-06 09:36:43,019 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:60080 (size: 5.5 KB, free: 631.8 MB)
2018-02-06 09:36:43,020 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 09:36:43,021 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at show at SchemaRDDSparkSQL.java:87) (first 15 tasks are for partitions Vector(0))
2018-02-06 09:36:43,021 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 09:36:43,022 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5067 bytes)
2018-02-06 09:36:43,022 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 09:36:43,056 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 10.873284 ms
2018-02-06 09:36:43,097 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 15.379205 ms
2018-02-06 09:36:43,107 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1168 bytes result sent to driver
2018-02-06 09:36:43,108 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 87 ms on localhost (executor driver) (1/1)
2018-02-06 09:36:43,109 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 09:36:43,109 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at SchemaRDDSparkSQL.java:87) finished in 0.088 s
2018-02-06 09:36:43,110 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at SchemaRDDSparkSQL.java:87, took 0.131399 s
2018-02-06 09:36:43,148 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 23.719367 ms
2018-02-06 09:36:43,171 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 09:36:43,177 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@4b3c354a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 09:36:43,179 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 09:36:43,194 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 09:36:43,210 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 09:36:43,211 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 09:36:43,216 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 09:36:43,219 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 09:36:43,223 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 09:36:43,223 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 09:36:43,224 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-40c550b1-fb66-4993-a813-467d16276920
2018-02-06 09:44:03,064 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 09:44:03,988 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 09:44:04,029 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 09:44:04,030 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 09:44:04,031 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 09:44:04,031 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 09:44:04,032 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 09:44:04,530 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 60167.
2018-02-06 09:44:04,550 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 09:44:04,601 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 09:44:04,604 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 09:44:04,605 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 09:44:04,615 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-9d342296-ef27-4627-bacc-03e12cba6f9e
2018-02-06 09:44:04,641 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 09:44:04,699 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 09:44:04,783 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2728ms
2018-02-06 09:44:04,856 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 09:44:04,873 INFO[org.spark_project.jetty.server.Server:403] - Started @2820ms
2018-02-06 09:44:04,895 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@553f1d75{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 09:44:04,896 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 09:44:04,923 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@458342d3{/jobs,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,924 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4f2613d1{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,925 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,925 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,926 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@83298d7{/stages,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,927 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5b080f3a{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,927 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6b54655f{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,929 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2756c0a7{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,929 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@69637b10{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,930 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@165b2f7f{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,931 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@277f7dd3{/storage,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,931 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2364305a{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,932 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bc715b8{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,934 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,935 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/environment,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,936 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,937 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/executors,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,937 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,938 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,939 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,946 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51850751{/static,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,947 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,948 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/api,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,948 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,949 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@427b5f92{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 09:44:04,950 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 09:44:05,036 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 09:44:05,061 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60180.
2018-02-06 09:44:05,062 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:60180
2018-02-06 09:44:05,064 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 09:44:05,066 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 60180, None)
2018-02-06 09:44:05,069 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:60180 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 60180, None)
2018-02-06 09:44:05,072 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 60180, None)
2018-02-06 09:44:05,073 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 60180, None)
2018-02-06 09:44:05,281 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@11ee02f8{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 09:44:06,095 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 316.2 KB, free 631.5 MB)
2018-02-06 09:44:06,160 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 631.5 MB)
2018-02-06 09:44:06,164 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:60180 (size: 27.2 KB, free: 631.8 MB)
2018-02-06 09:44:06,168 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from textFile at SparkWordCount.java:29
2018-02-06 09:44:13,852 INFO[org.apache.hadoop.mapred.FileInputFormat:256] - Total input files to process : 2
2018-02-06 09:44:13,898 INFO[org.apache.spark.SparkContext:54] - Starting job: collect at SparkWordCount.java:49
2018-02-06 09:44:13,910 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (collect at SparkWordCount.java:49) with 3 output partitions
2018-02-06 09:44:13,910 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (collect at SparkWordCount.java:49)
2018-02-06 09:44:13,911 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 09:44:13,912 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 09:44:13,919 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:33), which has no missing parents
2018-02-06 09:44:13,937 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 4.6 KB, free 631.5 MB)
2018-02-06 09:44:13,942 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 631.5 MB)
2018-02-06 09:44:13,943 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:60180 (size: 2.6 KB, free: 631.8 MB)
2018-02-06 09:44:13,943 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 09:44:13,956 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:33) (first 15 tasks are for partitions Vector(0, 1, 2))
2018-02-06 09:44:13,957 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 3 tasks
2018-02-06 09:44:14,000 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4862 bytes)
2018-02-06 09:44:14,003 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4862 bytes)
2018-02-06 09:44:14,008 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 0.0 (TID 1)
2018-02-06 09:44:14,008 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 09:44:14,076 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mumu/spark/file/2:0+68
2018-02-06 09:44:14,076 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mumu/spark/file/1:0+58
2018-02-06 09:44:14,334 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1467 bytes result sent to driver
2018-02-06 09:44:14,334 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 0.0 (TID 1). 1538 bytes result sent to driver
2018-02-06 09:44:14,336 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, ANY, 4862 bytes)
2018-02-06 09:44:14,337 INFO[org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 0.0 (TID 2)
2018-02-06 09:44:14,343 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 356 ms on localhost (executor driver) (1/3)
2018-02-06 09:44:14,343 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mumu/spark/file/2:68+10
2018-02-06 09:44:14,346 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 344 ms on localhost (executor driver) (2/3)
2018-02-06 09:44:14,353 INFO[org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 0.0 (TID 2). 1234 bytes result sent to driver
2018-02-06 09:44:14,358 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 0.0 (TID 2) in 22 ms on localhost (executor driver) (3/3)
2018-02-06 09:44:14,358 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 09:44:14,360 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (collect at SparkWordCount.java:49) finished in 0.384 s
2018-02-06 09:44:14,364 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: collect at SparkWordCount.java:49, took 0.466284 s
2018-02-06 09:44:14,408 INFO[org.apache.spark.SparkContext:54] - Starting job: countByKey at SparkWordCount.java:53
2018-02-06 09:44:14,414 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (countByKey at SparkWordCount.java:53)
2018-02-06 09:44:14,415 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (countByKey at SparkWordCount.java:53) with 3 output partitions
2018-02-06 09:44:14,416 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (countByKey at SparkWordCount.java:53)
2018-02-06 09:44:14,416 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-06 09:44:14,416 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-06 09:44:14,419 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:53), which has no missing parents
2018-02-06 09:44:14,426 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 5.6 KB, free 631.5 MB)
2018-02-06 09:44:14,430 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.1 KB, free 631.4 MB)
2018-02-06 09:44:14,433 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:60180 (size: 3.1 KB, free: 631.8 MB)
2018-02-06 09:44:14,433 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-06 09:44:14,436 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 3 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:53) (first 15 tasks are for partitions Vector(0, 1, 2))
2018-02-06 09:44:14,436 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 3 tasks
2018-02-06 09:44:14,438 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 3, localhost, executor driver, partition 0, ANY, 4851 bytes)
2018-02-06 09:44:14,439 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 4, localhost, executor driver, partition 1, ANY, 4851 bytes)
2018-02-06 09:44:14,439 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 3)
2018-02-06 09:44:14,439 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 1.0 (TID 4)
2018-02-06 09:44:14,451 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mumu/spark/file/2:0+68
2018-02-06 09:44:14,451 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mumu/spark/file/1:0+58
2018-02-06 09:44:14,459 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block rdd_2_1 stored as values in memory (estimated size 1216.0 B, free 631.4 MB)
2018-02-06 09:44:14,460 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block rdd_2_0 stored as values in memory (estimated size 904.0 B, free 631.4 MB)
2018-02-06 09:44:14,460 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_2_1 in memory on 192.168.11.26:60180 (size: 1216.0 B, free: 631.8 MB)
2018-02-06 09:44:14,460 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_2_0 in memory on 192.168.11.26:60180 (size: 904.0 B, free: 631.8 MB)
2018-02-06 09:44:14,520 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 3). 2136 bytes result sent to driver
2018-02-06 09:44:14,520 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 1.0 (TID 4). 2136 bytes result sent to driver
2018-02-06 09:44:14,521 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 1.0 (TID 5, localhost, executor driver, partition 2, ANY, 4851 bytes)
2018-02-06 09:44:14,521 INFO[org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 1.0 (TID 5)
2018-02-06 09:44:14,527 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mumu/spark/file/2:68+10
2018-02-06 09:44:14,527 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 4) in 89 ms on localhost (executor driver) (1/3)
2018-02-06 09:44:14,531 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 3) in 94 ms on localhost (executor driver) (2/3)
2018-02-06 09:44:14,537 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block rdd_2_2 stored as values in memory (estimated size 104.0 B, free 631.4 MB)
2018-02-06 09:44:14,538 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_2_2 in memory on 192.168.11.26:60180 (size: 104.0 B, free: 631.8 MB)
2018-02-06 09:44:14,550 INFO[org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 1.0 (TID 5). 2093 bytes result sent to driver
2018-02-06 09:44:14,551 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 1.0 (TID 5) in 30 ms on localhost (executor driver) (3/3)
2018-02-06 09:44:14,552 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 09:44:14,552 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (countByKey at SparkWordCount.java:53) finished in 0.115 s
2018-02-06 09:44:14,553 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-06 09:44:14,553 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-06 09:44:14,553 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-06 09:44:14,554 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-06 09:44:14,558 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:53), which has no missing parents
2018-02-06 09:44:14,562 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 631.4 MB)
2018-02-06 09:44:14,566 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1812.0 B, free 631.4 MB)
2018-02-06 09:44:14,567 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:60180 (size: 1812.0 B, free: 631.8 MB)
2018-02-06 09:44:14,569 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 09:44:14,569 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 3 missing tasks from ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:53) (first 15 tasks are for partitions Vector(0, 1, 2))
2018-02-06 09:44:14,570 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 3 tasks
2018-02-06 09:44:14,571 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 6, localhost, executor driver, partition 0, ANY, 4621 bytes)
2018-02-06 09:44:14,572 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 7, localhost, executor driver, partition 1, ANY, 4621 bytes)
2018-02-06 09:44:14,572 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 2.0 (TID 7)
2018-02-06 09:44:14,572 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 6)
2018-02-06 09:44:14,586 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 3 non-empty blocks out of 3 blocks
2018-02-06 09:44:14,586 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 3 blocks
2018-02-06 09:44:14,587 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
2018-02-06 09:44:14,587 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
2018-02-06 09:44:14,612 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 2.0 (TID 7). 1290 bytes result sent to driver
2018-02-06 09:44:14,612 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 6). 1345 bytes result sent to driver
2018-02-06 09:44:14,613 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 2.0 (TID 8, localhost, executor driver, partition 2, ANY, 4621 bytes)
2018-02-06 09:44:14,613 INFO[org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 2.0 (TID 8)
2018-02-06 09:44:14,614 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 7) in 43 ms on localhost (executor driver) (1/3)
2018-02-06 09:44:14,614 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 6) in 44 ms on localhost (executor driver) (2/3)
2018-02-06 09:44:14,616 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 3 blocks
2018-02-06 09:44:14,616 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 09:44:14,620 INFO[org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 2.0 (TID 8). 1191 bytes result sent to driver
2018-02-06 09:44:14,621 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 2.0 (TID 8) in 8 ms on localhost (executor driver) (3/3)
2018-02-06 09:44:14,622 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-06 09:44:14,622 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (countByKey at SparkWordCount.java:53) finished in 0.052 s
2018-02-06 09:44:14,623 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: countByKey at SparkWordCount.java:53, took 0.213590 s
2018-02-06 09:44:14,629 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 09:44:14,634 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@553f1d75{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 09:44:14,636 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 09:44:14,645 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 09:44:14,689 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 09:44:14,689 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 09:44:14,693 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 09:44:14,694 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 09:44:14,697 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 09:44:14,698 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 09:44:14,699 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-4cee4ca0-73ba-478a-9ffd-2c1cb2546436
2018-02-06 09:48:19,444 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 09:48:20,049 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 09:48:20,074 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 09:48:20,075 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 09:48:20,076 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 09:48:20,076 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 09:48:20,077 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 09:48:20,464 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 60246.
2018-02-06 09:48:20,484 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 09:48:20,534 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 09:48:20,537 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 09:48:20,537 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 09:48:20,547 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-937687b2-9567-47cb-98f9-9e3b1396a8eb
2018-02-06 09:48:20,573 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 09:48:20,622 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 09:48:20,710 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2544ms
2018-02-06 09:48:20,781 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 09:48:20,797 INFO[org.spark_project.jetty.server.Server:403] - Started @2632ms
2018-02-06 09:48:20,818 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@13d0b0e3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 09:48:20,818 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 09:48:20,843 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,844 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,844 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,846 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,847 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/stages,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,848 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,849 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,850 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ce3db41{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,851 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,852 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,852 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/storage,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,852 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,853 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,854 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,855 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/environment,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,856 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,857 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/executors,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,857 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,858 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,859 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,868 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/static,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,869 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,870 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/api,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,872 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,873 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 09:48:20,875 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 09:48:20,960 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 09:48:20,990 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60259.
2018-02-06 09:48:20,991 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:60259
2018-02-06 09:48:20,993 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 09:48:20,995 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 60259, None)
2018-02-06 09:48:21,003 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:60259 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 60259, None)
2018-02-06 09:48:21,008 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 60259, None)
2018-02-06 09:48:21,009 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 60259, None)
2018-02-06 09:48:21,221 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@420bc288{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 09:48:21,306 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 09:48:21,307 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 09:48:21,315 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@742d4e15{/SQL,null,AVAILABLE,@Spark}
2018-02-06 09:48:21,316 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50b1f030{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 09:48:21,316 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@23aae55{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 09:48:21,317 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@680bddf5{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 09:48:21,319 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53bf7094{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 09:48:22,515 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 09:48:24,836 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 194.987582 ms
2018-02-06 09:48:24,889 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 21.074567 ms
2018-02-06 09:48:24,911 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: person
2018-02-06 09:48:25,079 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 0
2018-02-06 09:48:25,115 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select *from person
2018-02-06 09:48:25,624 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select max(age) from person
2018-02-06 09:48:26,532 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 2
2018-02-06 09:48:26,533 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 1
2018-02-06 09:48:26,551 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 148.355247 ms
2018-02-06 09:48:26,566 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 11.819524 ms
2018-02-06 09:48:26,573 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 6.292162 ms
2018-02-06 09:48:26,714 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:71
2018-02-06 09:48:26,730 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (show at SchemaRDDSparkSQL.java:71)
2018-02-06 09:48:26,732 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at SchemaRDDSparkSQL.java:71) with 1 output partitions
2018-02-06 09:48:26,732 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at SchemaRDDSparkSQL.java:71)
2018-02-06 09:48:26,732 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 0)
2018-02-06 09:48:26,733 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 0)
2018-02-06 09:48:26,743 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at SchemaRDDSparkSQL.java:71), which has no missing parents
2018-02-06 09:48:26,863 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 8.1 KB, free 631.8 MB)
2018-02-06 09:48:26,894 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.3 KB, free 631.8 MB)
2018-02-06 09:48:26,897 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:60259 (size: 4.3 KB, free: 631.8 MB)
2018-02-06 09:48:26,899 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-06 09:48:26,912 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at SchemaRDDSparkSQL.java:71) (first 15 tasks are for partitions Vector(0, 1))
2018-02-06 09:48:26,913 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 2 tasks
2018-02-06 09:48:26,977 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Stage 0 contains a task of very large size (488 KB). The maximum recommended task size is 100 KB.
2018-02-06 09:48:26,979 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 499959 bytes)
2018-02-06 09:48:26,997 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 499959 bytes)
2018-02-06 09:48:27,004 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 09:48:27,004 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 0.0 (TID 1)
2018-02-06 09:48:27,212 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 0.0 (TID 1). 1546 bytes result sent to driver
2018-02-06 09:48:27,213 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1546 bytes result sent to driver
2018-02-06 09:48:27,228 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 292 ms on localhost (executor driver) (1/2)
2018-02-06 09:48:27,230 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 249 ms on localhost (executor driver) (2/2)
2018-02-06 09:48:27,231 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 09:48:27,240 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 0 (show at SchemaRDDSparkSQL.java:71) finished in 0.310 s
2018-02-06 09:48:27,240 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-06 09:48:27,241 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-06 09:48:27,242 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 1)
2018-02-06 09:48:27,242 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-06 09:48:27,247 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SchemaRDDSparkSQL.java:71), which has no missing parents
2018-02-06 09:48:27,267 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 7.1 KB, free 631.8 MB)
2018-02-06 09:48:27,271 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.8 KB, free 631.8 MB)
2018-02-06 09:48:27,272 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:60259 (size: 3.8 KB, free: 631.8 MB)
2018-02-06 09:48:27,273 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 09:48:27,275 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SchemaRDDSparkSQL.java:71) (first 15 tasks are for partitions Vector(0))
2018-02-06 09:48:27,276 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 09:48:27,283 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
2018-02-06 09:48:27,284 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 2)
2018-02-06 09:48:27,300 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2018-02-06 09:48:27,303 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
2018-02-06 09:48:27,324 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 2). 1514 bytes result sent to driver
2018-02-06 09:48:27,326 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 2) in 44 ms on localhost (executor driver) (1/1)
2018-02-06 09:48:27,326 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 09:48:27,327 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at SchemaRDDSparkSQL.java:71) finished in 0.046 s
2018-02-06 09:48:27,332 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at SchemaRDDSparkSQL.java:71, took 0.618484 s
2018-02-06 09:48:27,347 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 8.358083 ms
2018-02-06 09:48:27,617 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 7.144002 ms
2018-02-06 09:48:27,635 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 9.422083 ms
2018-02-06 09:48:27,945 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:74
2018-02-06 09:48:27,946 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 10 (show at SchemaRDDSparkSQL.java:74)
2018-02-06 09:48:27,947 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at SchemaRDDSparkSQL.java:74) with 1 output partitions
2018-02-06 09:48:27,947 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 3 (show at SchemaRDDSparkSQL.java:74)
2018-02-06 09:48:27,947 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 2)
2018-02-06 09:48:27,947 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 2)
2018-02-06 09:48:27,948 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at show at SchemaRDDSparkSQL.java:74), which has no missing parents
2018-02-06 09:48:27,950 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 8.1 KB, free 631.8 MB)
2018-02-06 09:48:27,952 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.3 KB, free 631.8 MB)
2018-02-06 09:48:27,955 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:60259 (size: 4.3 KB, free: 631.8 MB)
2018-02-06 09:48:27,955 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-06 09:48:27,956 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at show at SchemaRDDSparkSQL.java:74) (first 15 tasks are for partitions Vector(0, 1))
2018-02-06 09:48:27,956 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 2 tasks
2018-02-06 09:48:27,963 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Stage 2 contains a task of very large size (488 KB). The maximum recommended task size is 100 KB.
2018-02-06 09:48:27,964 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 499959 bytes)
2018-02-06 09:48:27,971 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 499959 bytes)
2018-02-06 09:48:27,972 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 3)
2018-02-06 09:48:27,972 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 2.0 (TID 4)
2018-02-06 09:48:28,042 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 3). 1503 bytes result sent to driver
2018-02-06 09:48:28,044 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 3) in 87 ms on localhost (executor driver) (1/2)
2018-02-06 09:48:28,045 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 2.0 (TID 4). 1503 bytes result sent to driver
2018-02-06 09:48:28,046 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 4) in 82 ms on localhost (executor driver) (2/2)
2018-02-06 09:48:28,046 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-06 09:48:28,047 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 2 (show at SchemaRDDSparkSQL.java:74) finished in 0.090 s
2018-02-06 09:48:28,047 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-06 09:48:28,047 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-06 09:48:28,047 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 3)
2018-02-06 09:48:28,047 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-06 09:48:28,047 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 3 (MapPartitionsRDD[13] at show at SchemaRDDSparkSQL.java:74), which has no missing parents
2018-02-06 09:48:28,050 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 7.1 KB, free 631.8 MB)
2018-02-06 09:48:28,053 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.8 KB, free 631.8 MB)
2018-02-06 09:48:28,054 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:60259 (size: 3.8 KB, free: 631.8 MB)
2018-02-06 09:48:28,055 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 09:48:28,055 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at show at SchemaRDDSparkSQL.java:74) (first 15 tasks are for partitions Vector(0))
2018-02-06 09:48:28,056 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 1 tasks
2018-02-06 09:48:28,057 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 5, localhost, executor driver, partition 0, ANY, 4726 bytes)
2018-02-06 09:48:28,057 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 5)
2018-02-06 09:48:28,059 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2018-02-06 09:48:28,060 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 09:48:28,063 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 5). 1514 bytes result sent to driver
2018-02-06 09:48:28,064 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 5) in 7 ms on localhost (executor driver) (1/1)
2018-02-06 09:48:28,064 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2018-02-06 09:48:28,064 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 3 (show at SchemaRDDSparkSQL.java:74) finished in 0.008 s
2018-02-06 09:48:28,065 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at SchemaRDDSparkSQL.java:74, took 0.120291 s
2018-02-06 09:48:28,373 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 89
2018-02-06 09:48:28,386 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:60259 in memory (size: 3.8 KB, free: 631.8 MB)
2018-02-06 09:48:28,389 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 92
2018-02-06 09:48:28,389 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 97
2018-02-06 09:48:28,390 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_3_piece0 on 192.168.11.26:60259 in memory (size: 3.8 KB, free: 631.8 MB)
2018-02-06 09:48:28,390 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 100
2018-02-06 09:48:28,390 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 98
2018-02-06 09:48:28,390 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 91
2018-02-06 09:48:28,391 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_0_piece0 on 192.168.11.26:60259 in memory (size: 4.3 KB, free: 631.8 MB)
2018-02-06 09:48:28,392 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 88
2018-02-06 09:48:28,392 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 87
2018-02-06 09:48:28,395 INFO[org.apache.spark.ContextCleaner:54] - Cleaned shuffle 1
2018-02-06 09:48:28,395 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 94
2018-02-06 09:48:28,396 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 95
2018-02-06 09:48:28,396 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 99
2018-02-06 09:48:28,397 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_2_piece0 on 192.168.11.26:60259 in memory (size: 4.3 KB, free: 631.8 MB)
2018-02-06 09:48:28,399 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 93
2018-02-06 09:48:28,400 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 90
2018-02-06 09:48:28,400 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 96
2018-02-06 09:48:35,854 INFO[org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat:54] - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 09:48:35,983 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 09:48:35,983 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 09:48:35,985 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 09:48:35,985 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 09:48:35,985 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 09:48:35,987 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 09:48:36,090 INFO[org.apache.spark.SparkContext:54] - Starting job: parquet at SchemaRDDSparkSQL.java:76
2018-02-06 09:48:36,091 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 2 (parquet at SchemaRDDSparkSQL.java:76) with 2 output partitions
2018-02-06 09:48:36,091 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 4 (parquet at SchemaRDDSparkSQL.java:76)
2018-02-06 09:48:36,091 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 09:48:36,091 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 09:48:36,092 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 4 (MapPartitionsRDD[15] at parquet at SchemaRDDSparkSQL.java:76), which has no missing parents
2018-02-06 09:48:36,106 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 85.5 KB, free 631.7 MB)
2018-02-06 09:48:36,108 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 31.3 KB, free 631.7 MB)
2018-02-06 09:48:36,109 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.11.26:60259 (size: 31.3 KB, free: 631.8 MB)
2018-02-06 09:48:36,110 INFO[org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2018-02-06 09:48:36,110 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at parquet at SchemaRDDSparkSQL.java:76) (first 15 tasks are for partitions Vector(0, 1))
2018-02-06 09:48:36,110 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 4.0 with 2 tasks
2018-02-06 09:48:36,126 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Stage 4 contains a task of very large size (1074 KB). The maximum recommended task size is 100 KB.
2018-02-06 09:48:36,126 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 4.0 (TID 6, localhost, executor driver, partition 0, PROCESS_LOCAL, 1099970 bytes)
2018-02-06 09:48:36,136 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 4.0 (TID 7, localhost, executor driver, partition 1, PROCESS_LOCAL, 1099970 bytes)
2018-02-06 09:48:36,137 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 4.0 (TID 6)
2018-02-06 09:48:36,137 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 4.0 (TID 7)
2018-02-06 09:48:36,177 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 09:48:36,177 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 09:48:36,179 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 09:48:36,179 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 09:48:36,180 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 09:48:36,180 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 09:48:36,181 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 09:48:36,181 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 09:48:36,182 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 09:48:36,182 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 09:48:36,182 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 09:48:36,183 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 09:48:36,189 INFO[org.apache.parquet.hadoop.codec.CodecConfig:95] - Compression: SNAPPY
2018-02-06 09:48:36,189 INFO[org.apache.parquet.hadoop.codec.CodecConfig:95] - Compression: SNAPPY
2018-02-06 09:48:36,193 INFO[org.apache.parquet.hadoop.codec.CodecConfig:95] - Compression: SNAPPY
2018-02-06 09:48:36,194 INFO[org.apache.parquet.hadoop.codec.CodecConfig:95] - Compression: SNAPPY
2018-02-06 09:48:36,219 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:329] - Parquet block size to 134217728
2018-02-06 09:48:36,219 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:329] - Parquet block size to 134217728
2018-02-06 09:48:36,219 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:330] - Parquet page size to 1048576
2018-02-06 09:48:36,219 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:330] - Parquet page size to 1048576
2018-02-06 09:48:36,219 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:331] - Parquet dictionary page size to 1048576
2018-02-06 09:48:36,219 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:331] - Parquet dictionary page size to 1048576
2018-02-06 09:48:36,219 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:332] - Dictionary is on
2018-02-06 09:48:36,219 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:332] - Dictionary is on
2018-02-06 09:48:36,220 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:333] - Validation is off
2018-02-06 09:48:36,220 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:333] - Validation is off
2018-02-06 09:48:36,220 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:334] - Writer version is: PARQUET_1_0
2018-02-06 09:48:36,220 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:334] - Writer version is: PARQUET_1_0
2018-02-06 09:48:36,220 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:335] - Maximum row group padding size is 0 bytes
2018-02-06 09:48:36,220 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:335] - Maximum row group padding size is 0 bytes
2018-02-06 09:48:36,220 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:336] - Page size checking is: estimated
2018-02-06 09:48:36,220 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:336] - Page size checking is: estimated
2018-02-06 09:48:36,220 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:337] - Min row count for page size check is: 100
2018-02-06 09:48:36,220 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:337] - Min row count for page size check is: 100
2018-02-06 09:48:36,220 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:338] - Max row count for page size check is: 10000
2018-02-06 09:48:36,220 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:338] - Max row count for page size check is: 10000
2018-02-06 09:48:36,248 INFO[org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport:54] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "age",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sex",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "birthday",
    "type" : "date",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary name (UTF8);
  optional int32 age;
  optional binary sex (UTF8);
  optional int32 birthday (DATE);
}

       
2018-02-06 09:48:36,248 INFO[org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport:54] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "name",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "age",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "sex",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "birthday",
    "type" : "date",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary name (UTF8);
  optional int32 age;
  optional binary sex (UTF8);
  optional int32 birthday (DATE);
}

       
2018-02-06 09:48:36,323 INFO[org.apache.hadoop.io.compress.CodecPool:153] - Got brand-new compressor [.snappy]
2018-02-06 09:48:36,323 INFO[org.apache.hadoop.io.compress.CodecPool:153] - Got brand-new compressor [.snappy]
2018-02-06 09:48:36,552 INFO[org.apache.parquet.hadoop.InternalParquetRecordWriter:160] - Flushing mem columnStore to file. allocated memory: 260062
2018-02-06 09:48:36,562 INFO[org.apache.parquet.hadoop.InternalParquetRecordWriter:160] - Flushing mem columnStore to file. allocated memory: 260062
2018-02-06 09:48:36,892 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:582] - Saved output of task 'attempt_20180206094836_0004_m_000000_0' to hdfs://192.168.11.25:9000/mumu/spark/sparksql/person.parquet/_temporary/0/task_20180206094836_0004_m_000000
2018-02-06 09:48:36,892 INFO[org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20180206094836_0004_m_000000_0: Committed
2018-02-06 09:48:36,897 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 4.0 (TID 6). 1471 bytes result sent to driver
2018-02-06 09:48:36,898 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 4.0 (TID 6) in 787 ms on localhost (executor driver) (1/2)
2018-02-06 09:48:36,899 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:582] - Saved output of task 'attempt_20180206094836_0004_m_000001_0' to hdfs://192.168.11.25:9000/mumu/spark/sparksql/person.parquet/_temporary/0/task_20180206094836_0004_m_000001
2018-02-06 09:48:36,899 INFO[org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20180206094836_0004_m_000001_0: Committed
2018-02-06 09:48:36,900 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 4.0 (TID 7). 1428 bytes result sent to driver
2018-02-06 09:48:36,901 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 4.0 (TID 7) in 774 ms on localhost (executor driver) (2/2)
2018-02-06 09:48:36,901 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2018-02-06 09:48:36,902 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 4 (parquet at SchemaRDDSparkSQL.java:76) finished in 0.791 s
2018-02-06 09:48:36,902 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 2 finished: parquet at SchemaRDDSparkSQL.java:76, took 0.811852 s
2018-02-06 09:48:36,972 INFO[org.apache.spark.sql.execution.datasources.FileFormatWriter:54] - Job null committed.
2018-02-06 09:48:36,985 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 09:48:36,991 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@13d0b0e3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 09:48:36,992 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 09:48:37,001 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 09:48:37,046 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 09:48:37,046 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 09:48:37,047 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 09:48:37,049 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 09:48:37,053 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 09:48:37,053 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 09:48:37,054 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-e2983b97-acdc-44f5-bd3c-dc31500c9cfa
2018-02-06 09:49:03,164 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 09:49:03,785 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 09:49:03,812 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 09:49:03,813 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 09:49:03,815 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 09:49:03,816 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 09:49:03,817 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 09:49:04,228 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 60296.
2018-02-06 09:49:04,249 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 09:49:04,297 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 09:49:04,301 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 09:49:04,301 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 09:49:04,311 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-285551f9-112a-482f-a886-8046d0fa11fd
2018-02-06 09:49:04,334 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 09:49:04,393 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 09:49:04,480 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3035ms
2018-02-06 09:49:04,561 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 09:49:04,575 INFO[org.spark_project.jetty.server.Server:403] - Started @3132ms
2018-02-06 09:49:04,600 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@73d06cf2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 09:49:04,601 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 09:49:04,632 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,632 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,633 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,634 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,635 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/stages,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,635 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,637 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ce3db41{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,639 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,640 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,641 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/storage,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,641 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,643 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,644 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,646 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/environment,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,647 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,648 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/executors,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,649 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,651 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,651 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,659 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/static,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,660 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,661 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/api,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,662 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,663 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 09:49:04,667 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 09:49:04,784 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 09:49:04,811 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60309.
2018-02-06 09:49:04,812 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:60309
2018-02-06 09:49:04,813 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 09:49:04,815 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 60309, None)
2018-02-06 09:49:04,819 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:60309 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 60309, None)
2018-02-06 09:49:04,822 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 60309, None)
2018-02-06 09:49:04,822 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 60309, None)
2018-02-06 09:49:05,029 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@420bc288{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 09:49:05,095 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 09:49:05,096 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 09:49:05,103 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fa05212{/SQL,null,AVAILABLE,@Spark}
2018-02-06 09:49:05,104 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c09d180{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 09:49:05,105 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2d83c5a5{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 09:49:05,106 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e053511{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 09:49:05,108 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28b576a9{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 09:49:06,353 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 09:49:14,366 INFO[org.apache.spark.SparkContext:54] - Starting job: parquet at SchemaRDDSparkSQL.java:101
2018-02-06 09:49:14,383 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (parquet at SchemaRDDSparkSQL.java:101) with 1 output partitions
2018-02-06 09:49:14,384 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (parquet at SchemaRDDSparkSQL.java:101)
2018-02-06 09:49:14,384 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 09:49:14,385 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 09:49:14,393 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at SchemaRDDSparkSQL.java:101), which has no missing parents
2018-02-06 09:49:14,447 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 80.5 KB, free 631.7 MB)
2018-02-06 09:49:14,487 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.9 KB, free 631.7 MB)
2018-02-06 09:49:14,490 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:60309 (size: 28.9 KB, free: 631.8 MB)
2018-02-06 09:49:14,492 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-06 09:49:14,507 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at SchemaRDDSparkSQL.java:101) (first 15 tasks are for partitions Vector(0))
2018-02-06 09:49:14,508 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 09:49:14,549 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5054 bytes)
2018-02-06 09:49:14,557 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 09:49:14,989 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1750 bytes result sent to driver
2018-02-06 09:49:14,996 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 462 ms on localhost (executor driver) (1/1)
2018-02-06 09:49:14,998 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 09:49:15,002 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (parquet at SchemaRDDSparkSQL.java:101) finished in 0.481 s
2018-02-06 09:49:15,008 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: parquet at SchemaRDDSparkSQL.java:101, took 0.640812 s
2018-02-06 09:49:15,479 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_0_piece0 on 192.168.11.26:60309 in memory (size: 28.9 KB, free: 631.8 MB)
2018-02-06 09:49:16,492 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 09:49:16,499 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 09:49:16,504 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<name: string, age: int, sex: string, birthday: date ... 2 more fields>
2018-02-06 09:49:16,515 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 09:49:16,946 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 229.744394 ms
2018-02-06 09:49:16,988 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 325.1 KB, free 631.5 MB)
2018-02-06 09:49:17,008 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 28.0 KB, free 631.5 MB)
2018-02-06 09:49:17,009 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:60309 (size: 28.0 KB, free: 631.8 MB)
2018-02-06 09:49:17,010 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from show at SchemaRDDSparkSQL.java:104
2018-02-06 09:49:17,027 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4234669 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 09:49:17,083 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:104
2018-02-06 09:49:17,085 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at SchemaRDDSparkSQL.java:104) with 1 output partitions
2018-02-06 09:49:17,085 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at SchemaRDDSparkSQL.java:104)
2018-02-06 09:49:17,085 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 09:49:17,085 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 09:49:17,086 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[4] at show at SchemaRDDSparkSQL.java:104), which has no missing parents
2018-02-06 09:49:17,101 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 9.7 KB, free 631.4 MB)
2018-02-06 09:49:17,105 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.6 KB, free 631.4 MB)
2018-02-06 09:49:17,107 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:60309 (size: 4.6 KB, free: 631.8 MB)
2018-02-06 09:49:17,108 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-06 09:49:17,109 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at show at SchemaRDDSparkSQL.java:104) (first 15 tasks are for partitions Vector(0))
2018-02-06 09:49:17,109 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 09:49:17,115 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5365 bytes)
2018-02-06 09:49:17,115 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 09:49:17,133 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/person.parquet/part-00000-ff8f7d48-88ab-4ff4-9ac4-abc53e883b32-c000.snappy.parquet, range: 0-40365, partition values: [empty row]
2018-02-06 09:49:17,260 INFO[org.apache.hadoop.io.compress.CodecPool:181] - Got brand-new decompressor [.snappy]
2018-02-06 09:49:17,368 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1433 bytes result sent to driver
2018-02-06 09:49:17,369 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 257 ms on localhost (executor driver) (1/1)
2018-02-06 09:49:17,370 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 09:49:17,371 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at SchemaRDDSparkSQL.java:104) finished in 0.259 s
2018-02-06 09:49:17,372 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at SchemaRDDSparkSQL.java:104, took 0.288013 s
2018-02-06 09:49:17,413 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 25.099528 ms
2018-02-06 09:49:17,436 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 09:49:17,441 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@73d06cf2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 09:49:17,443 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 09:49:17,452 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 09:49:17,481 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 09:49:17,481 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 09:49:17,483 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 09:49:17,485 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 09:49:17,487 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 09:49:17,488 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 09:49:17,489 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-bf85ebed-705c-4e6f-875b-f87fb9dfc27d
2018-02-06 10:24:34,348 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 10:24:35,207 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 10:24:35,248 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 10:24:35,249 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 10:24:35,249 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 10:24:35,250 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 10:24:35,251 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 10:24:35,687 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 61327.
2018-02-06 10:24:35,713 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 10:24:35,767 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 10:24:35,772 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 10:24:35,773 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 10:24:35,786 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-e4220833-5284-481a-aa9f-a00047c82815
2018-02-06 10:24:35,820 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 10:24:35,876 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 10:24:35,988 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3231ms
2018-02-06 10:24:36,078 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 10:24:36,093 INFO[org.spark_project.jetty.server.Server:403] - Started @3337ms
2018-02-06 10:24:36,129 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@c30c7c8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 10:24:36,130 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 10:24:36,167 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,168 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,169 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,170 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,171 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/stages,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,171 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,172 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,173 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ce3db41{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,173 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,174 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,175 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/storage,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,175 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,176 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,176 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,177 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/environment,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,178 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,178 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/executors,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,179 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,180 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,180 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,190 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/static,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,191 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,193 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/api,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,194 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,194 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,197 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 10:24:36,302 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 10:24:36,332 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61340.
2018-02-06 10:24:36,333 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:61340
2018-02-06 10:24:36,336 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 10:24:36,339 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 61340, None)
2018-02-06 10:24:36,344 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:61340 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 61340, None)
2018-02-06 10:24:36,349 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 61340, None)
2018-02-06 10:24:36,350 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 61340, None)
2018-02-06 10:24:36,661 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@420bc288{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,768 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 10:24:36,771 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 10:24:36,780 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@742d4e15{/SQL,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,782 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50b1f030{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,783 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@23aae55{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,784 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@680bddf5{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 10:24:36,788 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53bf7094{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 10:24:38,247 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 10:24:38,330 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 10:24:38,337 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@c30c7c8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 10:24:38,339 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 10:24:38,350 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 10:24:38,359 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 10:24:38,360 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 10:24:38,367 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 10:24:38,373 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 10:24:38,375 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 10:24:38,376 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 10:24:38,378 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-e30402f0-d99c-4408-a8cf-5b4db7f120ba
2018-02-06 10:25:30,518 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 10:25:31,466 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 10:25:31,506 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 10:25:31,507 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 10:25:31,508 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 10:25:31,509 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 10:25:31,509 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 10:25:32,036 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 61416.
2018-02-06 10:25:32,062 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 10:25:32,115 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 10:25:32,119 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 10:25:32,120 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 10:25:32,137 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-eb3f7b26-0843-4496-b680-20b58b7c79fb
2018-02-06 10:25:32,172 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 10:25:32,231 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 10:25:32,350 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @4001ms
2018-02-06 10:25:32,433 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 10:25:32,449 INFO[org.spark_project.jetty.server.Server:403] - Started @4101ms
2018-02-06 10:25:32,477 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@56671b91{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 10:25:32,478 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 10:25:32,509 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@83298d7{/jobs,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,510 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,511 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,512 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,512 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,513 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,513 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@38f57b3d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,515 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,516 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,517 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,518 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,518 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,519 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,520 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,522 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,522 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,523 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,523 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,524 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,524 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,533 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ddae9b5{/static,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,533 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,534 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f6c03cb{/api,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,535 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,536 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@162be91c{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,539 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 10:25:32,676 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 10:25:32,703 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61429.
2018-02-06 10:25:32,704 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:61429
2018-02-06 10:25:32,706 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 10:25:32,708 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 61429, None)
2018-02-06 10:25:32,711 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:61429 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 61429, None)
2018-02-06 10:25:32,714 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 61429, None)
2018-02-06 10:25:32,715 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 61429, None)
2018-02-06 10:25:32,905 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@308a6984{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 10:25:32,995 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 10:25:32,996 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 10:25:33,004 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50b1f030{/SQL,null,AVAILABLE,@Spark}
2018-02-06 10:25:33,005 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fa05212{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 10:25:33,006 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@680bddf5{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 10:25:33,006 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2d83c5a5{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 10:25:33,008 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@710b30ef{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 10:25:34,296 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 10:25:34,639 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 10:25:34,645 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@56671b91{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 10:25:34,647 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 10:25:34,656 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 10:25:34,664 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 10:25:34,665 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 10:25:34,672 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 10:25:34,675 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 10:25:34,677 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 10:25:34,678 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 10:25:34,679 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-31ca7430-a365-415c-bedc-09ad93eeab03
2018-02-06 10:27:06,281 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 10:27:06,923 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 10:27:06,961 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 10:27:06,962 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 10:27:06,962 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 10:27:06,963 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 10:27:06,963 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 10:27:07,372 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 61519.
2018-02-06 10:27:07,399 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 10:27:07,460 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 10:27:07,464 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 10:27:07,464 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 10:27:07,478 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-4d49a62e-3f19-4525-a565-bcb90b5419c2
2018-02-06 10:27:07,510 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 10:27:07,570 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 10:27:07,683 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2956ms
2018-02-06 10:27:07,783 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 10:27:07,800 INFO[org.spark_project.jetty.server.Server:403] - Started @3075ms
2018-02-06 10:27:07,830 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@125ae04c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 10:27:07,830 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 10:27:07,860 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@83298d7{/jobs,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,861 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,862 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,863 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,863 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,864 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,865 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@38f57b3d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,866 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,867 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,868 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,869 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,870 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,873 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,874 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,874 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,875 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,876 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,876 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,877 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,878 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,886 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ddae9b5{/static,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,887 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,888 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f6c03cb{/api,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,891 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,892 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@162be91c{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 10:27:07,895 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 10:27:08,015 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 10:27:08,048 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61532.
2018-02-06 10:27:08,049 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:61532
2018-02-06 10:27:08,052 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 10:27:08,057 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 61532, None)
2018-02-06 10:27:08,067 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:61532 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 61532, None)
2018-02-06 10:27:08,081 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 61532, None)
2018-02-06 10:27:08,083 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 61532, None)
2018-02-06 10:27:08,370 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@308a6984{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:08,519 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 10:27:08,520 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 10:27:08,531 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c09d180{/SQL,null,AVAILABLE,@Spark}
2018-02-06 10:27:08,531 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5f574cc2{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:08,532 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e053511{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 10:27:08,533 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53bf7094{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:08,535 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f6bbeb0{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 10:27:09,714 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 10:27:10,062 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 10:27:10,068 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@125ae04c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 10:27:10,070 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 10:27:10,080 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 10:27:10,086 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 10:27:10,087 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 10:27:10,094 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 10:27:10,098 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 10:27:10,100 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 10:27:10,101 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 10:27:10,102 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-3f6ed5e2-a5a6-4ee3-b03c-92196364ada1
2018-02-06 10:27:46,704 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 10:27:47,407 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 10:27:47,432 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 10:27:47,433 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 10:27:47,433 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 10:27:47,434 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 10:27:47,435 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 10:27:47,808 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 61575.
2018-02-06 10:27:47,828 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 10:27:47,877 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 10:27:47,880 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 10:27:47,881 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 10:27:47,889 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-0b27f1d4-24a1-404d-85ac-2e6749bb899b
2018-02-06 10:27:47,914 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 10:27:47,963 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 10:27:48,051 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2662ms
2018-02-06 10:27:48,128 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 10:27:48,142 INFO[org.spark_project.jetty.server.Server:403] - Started @2755ms
2018-02-06 10:27:48,165 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@68596877{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 10:27:48,165 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 10:27:48,192 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@83298d7{/jobs,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,193 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,193 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,194 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,195 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,195 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,196 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@38f57b3d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,197 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,198 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,198 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,199 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,199 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,202 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,202 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,203 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,204 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,204 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,205 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,206 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,207 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,213 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ddae9b5{/static,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,214 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,215 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f6c03cb{/api,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,216 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,216 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@162be91c{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,218 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 10:27:48,304 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 10:27:48,332 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61589.
2018-02-06 10:27:48,333 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:61589
2018-02-06 10:27:48,334 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 10:27:48,336 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 61589, None)
2018-02-06 10:27:48,343 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:61589 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 61589, None)
2018-02-06 10:27:48,347 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 61589, None)
2018-02-06 10:27:48,348 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 61589, None)
2018-02-06 10:27:48,578 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@308a6984{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,660 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 10:27:48,661 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 10:27:48,670 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50b1f030{/SQL,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,671 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fa05212{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,672 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@680bddf5{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,674 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2d83c5a5{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 10:27:48,676 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@710b30ef{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 10:27:49,735 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 10:27:51,865 WARN[org.apache.spark.util.Utils:66] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
2018-02-06 10:27:52,616 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 369.925239 ms
2018-02-06 10:27:52,706 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:111
2018-02-06 10:27:52,732 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at SchemaRDDSparkSQL.java:111) with 1 output partitions
2018-02-06 10:27:52,732 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (show at SchemaRDDSparkSQL.java:111)
2018-02-06 10:27:52,733 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 10:27:52,735 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 10:27:52,744 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at show at SchemaRDDSparkSQL.java:111), which has no missing parents
2018-02-06 10:27:52,929 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 17.9 KB, free 631.8 MB)
2018-02-06 10:27:52,972 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.9 KB, free 631.8 MB)
2018-02-06 10:27:52,975 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:61589 (size: 6.9 KB, free: 631.8 MB)
2018-02-06 10:27:52,978 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-06 10:27:52,994 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at SchemaRDDSparkSQL.java:111) (first 15 tasks are for partitions Vector(0))
2018-02-06 10:27:52,995 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 10:27:53,047 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 10:27:53,060 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 10:27:53,181 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 10:27:53,200 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 5508 bytes result sent to driver
2018-02-06 10:27:53,208 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 172 ms on localhost (executor driver) (1/1)
2018-02-06 10:27:53,212 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 10:27:53,219 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (show at SchemaRDDSparkSQL.java:111) finished in 0.201 s
2018-02-06 10:27:53,225 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at SchemaRDDSparkSQL.java:111, took 0.518345 s
2018-02-06 10:27:53,349 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 88.424029 ms
2018-02-06 10:27:53,388 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 10:27:53,394 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@68596877{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 10:27:53,397 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 10:27:53,408 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 10:27:53,419 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 10:27:53,420 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 10:27:53,426 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 10:27:53,429 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 10:27:53,432 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 10:27:53,432 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 10:27:53,433 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-d9562bf3-b55c-43ec-b692-577ee5809ed4
2018-02-06 10:40:57,714 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 10:40:58,506 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 10:40:58,546 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 10:40:58,547 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 10:40:58,548 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 10:40:58,549 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 10:40:58,549 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 10:40:59,009 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 63190.
2018-02-06 10:40:59,036 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 10:40:59,092 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 10:40:59,097 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 10:40:59,098 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 10:40:59,112 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-8ac87884-f874-4167-b5d1-ff3955fe83be
2018-02-06 10:40:59,145 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 10:40:59,204 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 10:40:59,322 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3485ms
2018-02-06 10:40:59,416 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 10:40:59,430 INFO[org.spark_project.jetty.server.Server:403] - Started @3595ms
2018-02-06 10:40:59,454 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@2c779312{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 10:40:59,454 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 10:40:59,486 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@83298d7{/jobs,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,487 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,488 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,490 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,491 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,491 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,492 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@38f57b3d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,493 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,494 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,494 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,495 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,495 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,496 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,498 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,499 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,500 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,500 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,501 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,502 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,502 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,515 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ddae9b5{/static,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,516 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,518 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f6c03cb{/api,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,519 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,520 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@162be91c{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 10:40:59,522 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 10:40:59,635 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 10:40:59,666 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63203.
2018-02-06 10:40:59,667 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:63203
2018-02-06 10:40:59,670 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 10:40:59,673 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 63203, None)
2018-02-06 10:40:59,678 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:63203 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 63203, None)
2018-02-06 10:40:59,681 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 63203, None)
2018-02-06 10:40:59,682 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 63203, None)
2018-02-06 10:40:59,923 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@308a6984{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 10:41:00,030 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 10:41:00,031 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 10:41:00,045 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c09d180{/SQL,null,AVAILABLE,@Spark}
2018-02-06 10:41:00,046 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5f574cc2{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 10:41:00,046 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e053511{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 10:41:00,047 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53bf7094{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 10:41:00,049 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f6bbeb0{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 10:41:01,431 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 10:41:03,835 WARN[org.apache.spark.util.Utils:66] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
2018-02-06 10:41:04,551 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 355.308914 ms
2018-02-06 10:41:04,644 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:111
2018-02-06 10:41:04,670 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at SchemaRDDSparkSQL.java:111) with 1 output partitions
2018-02-06 10:41:04,671 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (show at SchemaRDDSparkSQL.java:111)
2018-02-06 10:41:04,672 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 10:41:04,674 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 10:41:04,681 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at show at SchemaRDDSparkSQL.java:111), which has no missing parents
2018-02-06 10:41:04,865 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 17.9 KB, free 631.8 MB)
2018-02-06 10:41:04,905 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.9 KB, free 631.8 MB)
2018-02-06 10:41:04,909 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:63203 (size: 6.9 KB, free: 631.8 MB)
2018-02-06 10:41:04,912 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-06 10:41:04,928 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at SchemaRDDSparkSQL.java:111) (first 15 tasks are for partitions Vector(0))
2018-02-06 10:41:04,929 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 10:41:04,973 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 10:41:04,983 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 10:41:05,086 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 10:41:05,104 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 5465 bytes result sent to driver
2018-02-06 10:41:05,112 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 151 ms on localhost (executor driver) (1/1)
2018-02-06 10:41:05,114 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 10:41:05,118 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (show at SchemaRDDSparkSQL.java:111) finished in 0.171 s
2018-02-06 10:41:05,125 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at SchemaRDDSparkSQL.java:111, took 0.480114 s
2018-02-06 10:41:05,293 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 118.517798 ms
2018-02-06 10:41:05,582 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 18.708486 ms
2018-02-06 10:41:05,599 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 13.063684 ms
2018-02-06 10:41:05,634 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:113
2018-02-06 10:41:05,641 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 5 (show at SchemaRDDSparkSQL.java:113)
2018-02-06 10:41:05,642 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at SchemaRDDSparkSQL.java:113) with 1 output partitions
2018-02-06 10:41:05,642 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (show at SchemaRDDSparkSQL.java:113)
2018-02-06 10:41:05,642 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-06 10:41:05,642 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-06 10:41:05,643 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at show at SchemaRDDSparkSQL.java:113), which has no missing parents
2018-02-06 10:41:05,656 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 10.5 KB, free 631.8 MB)
2018-02-06 10:41:05,660 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KB, free 631.8 MB)
2018-02-06 10:41:05,661 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:63203 (size: 5.5 KB, free: 631.8 MB)
2018-02-06 10:41:05,661 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 10:41:05,664 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at show at SchemaRDDSparkSQL.java:113) (first 15 tasks are for partitions Vector(0))
2018-02-06 10:41:05,664 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 10:41:05,666 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4638 bytes)
2018-02-06 10:41:05,666 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 10:41:05,701 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 10:41:05,736 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1589 bytes result sent to driver
2018-02-06 10:41:05,739 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 74 ms on localhost (executor driver) (1/1)
2018-02-06 10:41:05,739 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 10:41:05,741 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (show at SchemaRDDSparkSQL.java:113) finished in 0.075 s
2018-02-06 10:41:05,741 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-06 10:41:05,741 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-06 10:41:05,742 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-06 10:41:05,743 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-06 10:41:05,747 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SchemaRDDSparkSQL.java:113), which has no missing parents
2018-02-06 10:41:05,758 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 7.5 KB, free 631.8 MB)
2018-02-06 10:41:05,760 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 631.7 MB)
2018-02-06 10:41:05,761 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:63203 (size: 4.0 KB, free: 631.8 MB)
2018-02-06 10:41:05,762 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-06 10:41:05,764 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SchemaRDDSparkSQL.java:113) (first 15 tasks are for partitions Vector(0))
2018-02-06 10:41:05,765 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
2018-02-06 10:41:05,769 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
2018-02-06 10:41:05,770 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 2)
2018-02-06 10:41:05,792 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 1 blocks
2018-02-06 10:41:05,794 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
2018-02-06 10:41:05,817 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 2). 1514 bytes result sent to driver
2018-02-06 10:41:05,820 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 2) in 52 ms on localhost (executor driver) (1/1)
2018-02-06 10:41:05,820 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-06 10:41:05,821 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (show at SchemaRDDSparkSQL.java:113) finished in 0.052 s
2018-02-06 10:41:05,822 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at SchemaRDDSparkSQL.java:113, took 0.186611 s
2018-02-06 10:41:05,836 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 9.515843 ms
2018-02-06 10:41:05,842 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 10:41:05,851 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@2c779312{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 10:41:05,853 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 10:41:05,867 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 10:41:05,888 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 10:41:05,889 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 10:41:05,895 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 10:41:05,898 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 10:41:05,901 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 10:41:05,903 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 10:41:05,905 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-14d9ee43-2cbc-45fe-87de-73b45b737f47
2018-02-06 10:51:30,907 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 10:51:31,602 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 10:51:31,632 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 10:51:31,632 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 10:51:31,633 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 10:51:31,641 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 10:51:31,642 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 10:51:32,230 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 51425.
2018-02-06 10:51:32,252 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 10:51:32,309 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 10:51:32,312 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 10:51:32,312 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 10:51:32,322 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-1da9b493-5270-4fe3-920c-e6af20fe681a
2018-02-06 10:51:32,347 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 10:51:32,408 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 10:51:32,533 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3176ms
2018-02-06 10:51:32,672 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 10:51:32,689 INFO[org.spark_project.jetty.server.Server:403] - Started @3333ms
2018-02-06 10:51:32,724 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@279226fe{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 10:51:32,724 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 10:51:32,754 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@83298d7{/jobs,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,765 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,766 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,768 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,768 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,769 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,769 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@38f57b3d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,770 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,771 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,772 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,773 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,773 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,774 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,776 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,777 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,778 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,778 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,780 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,781 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,781 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,787 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ddae9b5{/static,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,789 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,790 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f6c03cb{/api,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,791 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,792 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@162be91c{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 10:51:32,795 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 10:51:32,890 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 10:51:32,915 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51438.
2018-02-06 10:51:32,915 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:51438
2018-02-06 10:51:32,917 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 10:51:32,919 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 51438, None)
2018-02-06 10:51:32,922 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:51438 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 51438, None)
2018-02-06 10:51:32,924 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 51438, None)
2018-02-06 10:51:32,925 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 51438, None)
2018-02-06 10:51:33,086 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@308a6984{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 10:51:33,150 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 10:51:33,151 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 10:51:33,157 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50b1f030{/SQL,null,AVAILABLE,@Spark}
2018-02-06 10:51:33,158 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fa05212{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 10:51:33,160 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@680bddf5{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 10:51:33,161 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2d83c5a5{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 10:51:33,163 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@710b30ef{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 10:51:34,233 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 10:51:36,141 WARN[org.apache.spark.util.Utils:66] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
2018-02-06 10:51:36,765 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 333.532907 ms
2018-02-06 10:51:36,833 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:111
2018-02-06 10:51:36,851 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at SchemaRDDSparkSQL.java:111) with 1 output partitions
2018-02-06 10:51:36,852 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (show at SchemaRDDSparkSQL.java:111)
2018-02-06 10:51:36,852 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 10:51:36,853 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 10:51:36,859 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at show at SchemaRDDSparkSQL.java:111), which has no missing parents
2018-02-06 10:51:36,992 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 17.9 KB, free 631.8 MB)
2018-02-06 10:51:37,025 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.9 KB, free 631.8 MB)
2018-02-06 10:51:37,028 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:51438 (size: 6.9 KB, free: 631.8 MB)
2018-02-06 10:51:37,030 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-06 10:51:37,041 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at SchemaRDDSparkSQL.java:111) (first 15 tasks are for partitions Vector(0))
2018-02-06 10:51:37,044 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 10:51:37,078 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 10:51:37,086 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 10:51:37,167 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 10:51:37,179 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 5465 bytes result sent to driver
2018-02-06 10:51:37,187 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 118 ms on localhost (executor driver) (1/1)
2018-02-06 10:51:37,189 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 10:51:37,194 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (show at SchemaRDDSparkSQL.java:111) finished in 0.134 s
2018-02-06 10:51:37,198 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at SchemaRDDSparkSQL.java:111, took 0.364782 s
2018-02-06 10:51:37,309 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 79.875866 ms
2018-02-06 10:51:37,549 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 14.837125 ms
2018-02-06 10:51:37,563 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 10.623363 ms
2018-02-06 10:51:37,592 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:113
2018-02-06 10:51:37,596 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 5 (show at SchemaRDDSparkSQL.java:113)
2018-02-06 10:51:37,597 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at SchemaRDDSparkSQL.java:113) with 1 output partitions
2018-02-06 10:51:37,597 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (show at SchemaRDDSparkSQL.java:113)
2018-02-06 10:51:37,597 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-06 10:51:37,597 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-06 10:51:37,598 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at show at SchemaRDDSparkSQL.java:113), which has no missing parents
2018-02-06 10:51:37,611 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 10.5 KB, free 631.8 MB)
2018-02-06 10:51:37,613 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KB, free 631.8 MB)
2018-02-06 10:51:37,614 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:51438 (size: 5.5 KB, free: 631.8 MB)
2018-02-06 10:51:37,614 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 10:51:37,617 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at show at SchemaRDDSparkSQL.java:113) (first 15 tasks are for partitions Vector(0))
2018-02-06 10:51:37,617 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 10:51:37,619 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4638 bytes)
2018-02-06 10:51:37,619 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 10:51:37,646 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 10:51:37,674 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1589 bytes result sent to driver
2018-02-06 10:51:37,677 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 58 ms on localhost (executor driver) (1/1)
2018-02-06 10:51:37,677 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 10:51:37,678 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (show at SchemaRDDSparkSQL.java:113) finished in 0.060 s
2018-02-06 10:51:37,678 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-06 10:51:37,678 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-06 10:51:37,679 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-06 10:51:37,679 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-06 10:51:37,682 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[8] at show at SchemaRDDSparkSQL.java:113), which has no missing parents
2018-02-06 10:51:37,689 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 7.5 KB, free 631.8 MB)
2018-02-06 10:51:37,691 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 631.7 MB)
2018-02-06 10:51:37,692 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:51438 (size: 4.0 KB, free: 631.8 MB)
2018-02-06 10:51:37,693 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-06 10:51:37,694 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at SchemaRDDSparkSQL.java:113) (first 15 tasks are for partitions Vector(0))
2018-02-06 10:51:37,694 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
2018-02-06 10:51:37,698 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
2018-02-06 10:51:37,700 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 2)
2018-02-06 10:51:37,716 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 1 blocks
2018-02-06 10:51:37,717 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
2018-02-06 10:51:37,731 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 2). 1514 bytes result sent to driver
2018-02-06 10:51:37,733 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 2) in 36 ms on localhost (executor driver) (1/1)
2018-02-06 10:51:37,733 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-06 10:51:37,733 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (show at SchemaRDDSparkSQL.java:113) finished in 0.036 s
2018-02-06 10:51:37,734 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at SchemaRDDSparkSQL.java:113, took 0.142012 s
2018-02-06 10:51:37,745 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 7.736002 ms
2018-02-06 10:51:37,748 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: bill
2018-02-06 10:51:37,900 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select *from bill
2018-02-06 10:51:38,053 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:116
2018-02-06 10:51:38,055 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 2 (show at SchemaRDDSparkSQL.java:116) with 1 output partitions
2018-02-06 10:51:38,055 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 3 (show at SchemaRDDSparkSQL.java:116)
2018-02-06 10:51:38,056 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 10:51:38,056 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 10:51:38,056 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 3 (MapPartitionsRDD[11] at show at SchemaRDDSparkSQL.java:116), which has no missing parents
2018-02-06 10:51:38,058 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 17.9 KB, free 631.7 MB)
2018-02-06 10:51:38,064 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.9 KB, free 631.7 MB)
2018-02-06 10:51:38,065 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:51438 (size: 6.9 KB, free: 631.8 MB)
2018-02-06 10:51:38,066 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 10:51:38,067 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at show at SchemaRDDSparkSQL.java:116) (first 15 tasks are for partitions Vector(0))
2018-02-06 10:51:38,067 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 1 tasks
2018-02-06 10:51:38,068 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 10:51:38,068 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 3)
2018-02-06 10:51:38,091 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 10:51:38,093 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 3). 5422 bytes result sent to driver
2018-02-06 10:51:38,096 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 3) in 29 ms on localhost (executor driver) (1/1)
2018-02-06 10:51:38,097 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2018-02-06 10:51:38,097 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 3 (show at SchemaRDDSparkSQL.java:116) finished in 0.030 s
2018-02-06 10:51:38,098 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 2 finished: show at SchemaRDDSparkSQL.java:116, took 0.043800 s
2018-02-06 10:51:38,114 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 10:51:38,123 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@279226fe{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 10:51:38,126 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 10:51:38,161 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 10:51:38,186 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 10:51:38,188 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 10:51:38,193 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 10:51:38,195 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 10:51:38,201 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 10:51:38,202 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 10:51:38,204 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-24b25999-34fa-4373-acd5-881b1947d24e
2018-02-06 12:37:27,908 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 12:37:28,828 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 12:37:28,874 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 12:37:28,875 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 12:37:28,876 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 12:37:28,877 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 12:37:28,878 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 12:37:29,385 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 53971.
2018-02-06 12:37:29,415 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 12:37:29,481 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 12:37:29,486 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 12:37:29,487 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 12:37:29,501 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-55cb1067-5275-49f7-a40a-42508d7023d1
2018-02-06 12:37:29,538 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 12:37:29,615 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 12:37:29,750 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3623ms
2018-02-06 12:37:29,857 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 12:37:29,875 INFO[org.spark_project.jetty.server.Server:403] - Started @3749ms
2018-02-06 12:37:29,905 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@4f903c69{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 12:37:29,906 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 12:37:29,938 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c51bb7{/jobs,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,939 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,940 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,941 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,942 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/stages,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,944 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,945 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,947 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64df9a61{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,948 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e260766{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,949 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a97744{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,950 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c56e013{/storage,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,950 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40258c2f{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,951 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6731787b{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,952 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,953 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/environment,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,953 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,954 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/executors,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,955 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,955 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,956 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5860f3d7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,972 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/static,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,973 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@59d2103b{/,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,974 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e4de19b{/api,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,976 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c6daf0{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,977 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@659eef7{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 12:37:29,981 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 12:37:30,098 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 12:37:30,140 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53985.
2018-02-06 12:37:30,141 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:53985
2018-02-06 12:37:30,146 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 12:37:30,149 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 53985, None)
2018-02-06 12:37:30,158 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:53985 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 53985, None)
2018-02-06 12:37:30,170 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 53985, None)
2018-02-06 12:37:30,171 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 53985, None)
2018-02-06 12:37:30,451 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@df5f5c0{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 12:37:30,571 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 12:37:30,573 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 12:37:30,585 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e681bc{/SQL,null,AVAILABLE,@Spark}
2018-02-06 12:37:30,586 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@23aae55{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 12:37:30,589 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@48d7ad8b{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 12:37:30,590 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@60222fd8{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 12:37:30,593 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7cf7aee{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 12:37:31,982 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 12:37:34,100 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 252.910801 ms
2018-02-06 12:37:34,902 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 10.059204 ms
2018-02-06 12:37:35,123 INFO[org.apache.spark.SparkContext:54] - Starting job: json at JsonDataSet.java:27
2018-02-06 12:37:35,151 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at JsonDataSet.java:27) with 1 output partitions
2018-02-06 12:37:35,152 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at JsonDataSet.java:27)
2018-02-06 12:37:35,153 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 12:37:35,155 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 12:37:35,169 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at JsonDataSet.java:27), which has no missing parents
2018-02-06 12:37:35,495 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 5.8 KB, free 631.8 MB)
2018-02-06 12:37:35,546 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.6 KB, free 631.8 MB)
2018-02-06 12:37:35,551 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:53985 (size: 3.6 KB, free: 631.8 MB)
2018-02-06 12:37:35,553 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:37:35,571 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at JsonDataSet.java:27) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:37:35,572 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 12:37:35,633 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5067 bytes)
2018-02-06 12:37:35,649 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 12:37:35,757 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1754 bytes result sent to driver
2018-02-06 12:37:35,769 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 157 ms on localhost (executor driver) (1/1)
2018-02-06 12:37:35,772 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 12:37:35,781 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at JsonDataSet.java:27) finished in 0.185 s
2018-02-06 12:37:35,787 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at JsonDataSet.java:27, took 0.663078 s
2018-02-06 12:37:36,015 INFO[org.apache.spark.SparkContext:54] - Starting job: show at JsonDataSet.java:30
2018-02-06 12:37:36,016 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at JsonDataSet.java:30) with 1 output partitions
2018-02-06 12:37:36,016 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at JsonDataSet.java:30)
2018-02-06 12:37:36,016 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 12:37:36,017 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 12:37:36,017 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[10] at show at JsonDataSet.java:30), which has no missing parents
2018-02-06 12:37:36,053 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.8 KB, free 631.8 MB)
2018-02-06 12:37:36,056 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KB, free 631.8 MB)
2018-02-06 12:37:36,057 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:53985 (size: 5.5 KB, free: 631.8 MB)
2018-02-06 12:37:36,060 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:37:36,062 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at show at JsonDataSet.java:30) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:37:36,062 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 12:37:36,063 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5067 bytes)
2018-02-06 12:37:36,064 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 12:37:36,097 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 10.658564 ms
2018-02-06 12:37:36,142 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 18.446406 ms
2018-02-06 12:37:36,154 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1125 bytes result sent to driver
2018-02-06 12:37:36,156 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 94 ms on localhost (executor driver) (1/1)
2018-02-06 12:37:36,156 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 12:37:36,157 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at JsonDataSet.java:30) finished in 0.095 s
2018-02-06 12:37:36,158 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at JsonDataSet.java:30, took 0.142666 s
2018-02-06 12:37:36,200 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 29.018889 ms
2018-02-06 12:37:36,226 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 12:37:36,236 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@4f903c69{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 12:37:36,238 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 12:37:36,253 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 12:37:36,273 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 12:37:36,273 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 12:37:36,282 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 12:37:36,286 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 12:37:36,289 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 12:37:36,290 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 12:37:36,292 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-9af839f3-c7ea-45fe-99df-e0edfac24c46
2018-02-06 12:47:34,687 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 12:47:35,402 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 12:47:35,435 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 12:47:35,436 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 12:47:35,444 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 12:47:35,445 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 12:47:35,446 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 12:47:35,914 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 54327.
2018-02-06 12:47:35,937 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 12:47:35,994 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 12:47:35,998 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 12:47:35,999 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 12:47:36,009 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-5509b7c7-4264-49b1-9f33-ba45a01ba6af
2018-02-06 12:47:36,038 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 12:47:36,088 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 12:47:36,194 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3101ms
2018-02-06 12:47:36,274 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 12:47:36,291 INFO[org.spark_project.jetty.server.Server:403] - Started @3200ms
2018-02-06 12:47:36,317 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@457da848{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 12:47:36,317 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 12:47:36,350 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,350 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,352 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,354 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,355 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/stages,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,356 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,357 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,358 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ce3db41{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,359 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,360 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,361 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/storage,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,362 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,363 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,365 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,365 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/environment,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,366 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,367 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/executors,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,367 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,369 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,371 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,379 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/static,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,380 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,381 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/api,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,382 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,383 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 12:47:36,386 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 12:47:36,504 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 12:47:36,532 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54340.
2018-02-06 12:47:36,533 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:54340
2018-02-06 12:47:36,535 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 12:47:36,537 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 54340, None)
2018-02-06 12:47:36,547 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:54340 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 54340, None)
2018-02-06 12:47:36,550 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 54340, None)
2018-02-06 12:47:36,550 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 54340, None)
2018-02-06 12:47:36,884 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@420bc288{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 12:47:37,001 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 12:47:37,002 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 12:47:37,016 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fa05212{/SQL,null,AVAILABLE,@Spark}
2018-02-06 12:47:37,017 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c09d180{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 12:47:37,019 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2d83c5a5{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 12:47:37,020 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e053511{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 12:47:37,027 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28b576a9{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 12:47:38,288 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 12:47:38,795 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 256.242642 ms
2018-02-06 12:47:40,622 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 12.535684 ms
2018-02-06 12:47:40,655 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 20.381766 ms
2018-02-06 12:47:40,687 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 12:47:40,692 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@457da848{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 12:47:40,694 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 12:47:40,702 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 12:47:40,710 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 12:47:40,711 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 12:47:40,718 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 12:47:40,721 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 12:47:40,724 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 12:47:40,725 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 12:47:40,726 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-c668aa4f-0e48-4612-8ff5-9bdf9338bc4f
2018-02-06 12:53:36,954 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 12:53:37,703 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 12:53:37,731 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 12:53:37,733 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 12:53:37,734 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 12:53:37,734 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 12:53:37,735 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 12:53:38,231 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 54640.
2018-02-06 12:53:38,252 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 12:53:38,316 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 12:53:38,319 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 12:53:38,320 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 12:53:38,329 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-f78b5c86-a536-4806-8b8e-4088a4e28d11
2018-02-06 12:53:38,358 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 12:53:38,431 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 12:53:38,538 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3080ms
2018-02-06 12:53:38,643 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 12:53:38,670 INFO[org.spark_project.jetty.server.Server:403] - Started @3212ms
2018-02-06 12:53:38,699 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@4b3c354a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 12:53:38,699 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 12:53:38,728 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c51bb7{/jobs,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,729 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,730 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,731 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,731 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/stages,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,732 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,733 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,734 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64df9a61{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,735 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e260766{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,736 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a97744{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,736 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c56e013{/storage,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,737 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40258c2f{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,739 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6731787b{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,741 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/environment,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,741 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,742 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/executors,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,744 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,744 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,745 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5860f3d7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,754 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/static,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,755 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@59d2103b{/,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,756 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e4de19b{/api,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,757 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c6daf0{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,758 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@659eef7{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 12:53:38,760 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 12:53:38,856 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 12:53:38,898 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54653.
2018-02-06 12:53:38,902 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:54653
2018-02-06 12:53:38,912 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 12:53:38,916 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 54653, None)
2018-02-06 12:53:38,931 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:54653 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 54653, None)
2018-02-06 12:53:38,943 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 54653, None)
2018-02-06 12:53:38,943 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 54653, None)
2018-02-06 12:53:39,247 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@df5f5c0{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 12:53:39,336 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 12:53:39,340 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 12:53:39,379 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e681bc{/SQL,null,AVAILABLE,@Spark}
2018-02-06 12:53:39,379 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@23aae55{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 12:53:39,380 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@48d7ad8b{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 12:53:39,381 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@60222fd8{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 12:53:39,384 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7cf7aee{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 12:53:40,662 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 12:53:43,091 WARN[org.apache.spark.util.Utils:66] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
2018-02-06 12:53:43,842 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 394.815806 ms
2018-02-06 12:53:43,934 INFO[org.apache.spark.SparkContext:54] - Starting job: show at CreateDataSetFromDatabase.java:27
2018-02-06 12:53:43,957 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at CreateDataSetFromDatabase.java:27) with 1 output partitions
2018-02-06 12:53:43,958 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (show at CreateDataSetFromDatabase.java:27)
2018-02-06 12:53:43,958 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 12:53:43,960 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 12:53:43,967 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at show at CreateDataSetFromDatabase.java:27), which has no missing parents
2018-02-06 12:53:44,113 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 17.9 KB, free 631.8 MB)
2018-02-06 12:53:44,155 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.9 KB, free 631.8 MB)
2018-02-06 12:53:44,158 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:54653 (size: 6.9 KB, free: 631.8 MB)
2018-02-06 12:53:44,161 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:53:44,174 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at CreateDataSetFromDatabase.java:27) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:53:44,175 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 12:53:44,218 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 12:53:44,227 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 12:53:44,329 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 12:53:44,346 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 5459 bytes result sent to driver
2018-02-06 12:53:44,357 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 150 ms on localhost (executor driver) (1/1)
2018-02-06 12:53:44,360 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 12:53:44,364 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (show at CreateDataSetFromDatabase.java:27) finished in 0.169 s
2018-02-06 12:53:44,371 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at CreateDataSetFromDatabase.java:27, took 0.435066 s
2018-02-06 12:53:44,503 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 96.925471 ms
2018-02-06 12:53:44,810 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 19.610566 ms
2018-02-06 12:53:44,832 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 15.833285 ms
2018-02-06 12:53:44,868 INFO[org.apache.spark.SparkContext:54] - Starting job: show at CreateDataSetFromDatabase.java:29
2018-02-06 12:53:44,872 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 5 (show at CreateDataSetFromDatabase.java:29)
2018-02-06 12:53:44,873 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at CreateDataSetFromDatabase.java:29) with 1 output partitions
2018-02-06 12:53:44,873 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (show at CreateDataSetFromDatabase.java:29)
2018-02-06 12:53:44,873 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-06 12:53:44,874 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-06 12:53:44,874 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at show at CreateDataSetFromDatabase.java:29), which has no missing parents
2018-02-06 12:53:44,888 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 10.5 KB, free 631.8 MB)
2018-02-06 12:53:44,893 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KB, free 631.8 MB)
2018-02-06 12:53:44,894 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:54653 (size: 5.5 KB, free: 631.8 MB)
2018-02-06 12:53:44,895 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:53:44,898 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at show at CreateDataSetFromDatabase.java:29) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:53:44,898 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 12:53:44,900 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4638 bytes)
2018-02-06 12:53:44,901 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 12:53:44,935 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 12:53:44,973 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1546 bytes result sent to driver
2018-02-06 12:53:44,976 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 77 ms on localhost (executor driver) (1/1)
2018-02-06 12:53:44,976 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 12:53:44,978 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (show at CreateDataSetFromDatabase.java:29) finished in 0.079 s
2018-02-06 12:53:44,979 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-06 12:53:44,979 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-06 12:53:44,980 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-06 12:53:44,980 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-06 12:53:44,985 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[8] at show at CreateDataSetFromDatabase.java:29), which has no missing parents
2018-02-06 12:53:44,996 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 7.5 KB, free 631.8 MB)
2018-02-06 12:53:44,998 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 631.7 MB)
2018-02-06 12:53:45,000 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:54653 (size: 4.0 KB, free: 631.8 MB)
2018-02-06 12:53:45,001 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:53:45,002 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at CreateDataSetFromDatabase.java:29) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:53:45,002 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
2018-02-06 12:53:45,009 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
2018-02-06 12:53:45,009 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 2)
2018-02-06 12:53:45,034 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 1 blocks
2018-02-06 12:53:45,036 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
2018-02-06 12:53:45,064 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 2). 1514 bytes result sent to driver
2018-02-06 12:53:45,067 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 2) in 60 ms on localhost (executor driver) (1/1)
2018-02-06 12:53:45,068 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-06 12:53:45,068 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (show at CreateDataSetFromDatabase.java:29) finished in 0.061 s
2018-02-06 12:53:45,068 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at CreateDataSetFromDatabase.java:29, took 0.199488 s
2018-02-06 12:53:45,079 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 8.412483 ms
2018-02-06 12:53:45,085 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: bill
2018-02-06 12:53:45,276 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select *from bill
2018-02-06 12:53:45,536 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 52
2018-02-06 12:53:45,536 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 51
2018-02-06 12:53:45,536 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 56
2018-02-06 12:53:45,537 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 60
2018-02-06 12:53:45,537 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 50
2018-02-06 12:53:45,537 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 59
2018-02-06 12:53:45,537 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 58
2018-02-06 12:53:45,537 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 54
2018-02-06 12:53:45,537 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 55
2018-02-06 12:53:45,537 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 53
2018-02-06 12:53:45,545 INFO[org.apache.spark.SparkContext:54] - Starting job: show at CreateDataSetFromDatabase.java:32
2018-02-06 12:53:45,551 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 2 (show at CreateDataSetFromDatabase.java:32) with 1 output partitions
2018-02-06 12:53:45,554 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 3 (show at CreateDataSetFromDatabase.java:32)
2018-02-06 12:53:45,554 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 12:53:45,554 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 12:53:45,555 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 3 (MapPartitionsRDD[11] at show at CreateDataSetFromDatabase.java:32), which has no missing parents
2018-02-06 12:53:45,563 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 17.9 KB, free 631.7 MB)
2018-02-06 12:53:45,567 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.9 KB, free 631.7 MB)
2018-02-06 12:53:45,568 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:54653 (size: 6.9 KB, free: 631.8 MB)
2018-02-06 12:53:45,571 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:53:45,572 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at show at CreateDataSetFromDatabase.java:32) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:53:45,572 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 1 tasks
2018-02-06 12:53:45,574 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 12:53:45,575 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 3)
2018-02-06 12:53:45,579 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:54653 in memory (size: 5.5 KB, free: 631.8 MB)
2018-02-06 12:53:45,583 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 62
2018-02-06 12:53:45,584 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 61
2018-02-06 12:53:45,585 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_0_piece0 on 192.168.11.26:54653 in memory (size: 6.9 KB, free: 631.8 MB)
2018-02-06 12:53:45,590 INFO[org.apache.spark.ContextCleaner:54] - Cleaned shuffle 0
2018-02-06 12:53:45,591 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 57
2018-02-06 12:53:45,593 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_2_piece0 on 192.168.11.26:54653 in memory (size: 4.0 KB, free: 631.8 MB)
2018-02-06 12:53:45,597 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 12:53:45,599 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 3). 5373 bytes result sent to driver
2018-02-06 12:53:45,601 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 3) in 28 ms on localhost (executor driver) (1/1)
2018-02-06 12:53:45,602 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2018-02-06 12:53:45,602 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 3 (show at CreateDataSetFromDatabase.java:32) finished in 0.029 s
2018-02-06 12:53:45,603 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 2 finished: show at CreateDataSetFromDatabase.java:32, took 0.057614 s
2018-02-06 12:53:46,044 INFO[org.apache.spark.SparkContext:54] - Starting job: jdbc at CreateDataSetFromDatabase.java:34
2018-02-06 12:53:46,045 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 3 (jdbc at CreateDataSetFromDatabase.java:34) with 1 output partitions
2018-02-06 12:53:46,045 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 4 (jdbc at CreateDataSetFromDatabase.java:34)
2018-02-06 12:53:46,045 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 12:53:46,046 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 12:53:46,046 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 4 (MapPartitionsRDD[16] at jdbc at CreateDataSetFromDatabase.java:34), which has no missing parents
2018-02-06 12:53:46,075 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 26.7 KB, free 631.7 MB)
2018-02-06 12:53:46,078 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.2 KB, free 631.7 MB)
2018-02-06 12:53:46,080 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.11.26:54653 (size: 10.2 KB, free: 631.8 MB)
2018-02-06 12:53:46,081 INFO[org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:53:46,082 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at jdbc at CreateDataSetFromDatabase.java:34) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:53:46,083 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 4.0 with 1 tasks
2018-02-06 12:53:46,084 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 12:53:46,085 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 4.0 (TID 4)
2018-02-06 12:53:46,148 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 12:53:46,333 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 4.0 (TID 4). 1150 bytes result sent to driver
2018-02-06 12:53:46,334 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 4.0 (TID 4) in 250 ms on localhost (executor driver) (1/1)
2018-02-06 12:53:46,334 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2018-02-06 12:53:46,334 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 4 (jdbc at CreateDataSetFromDatabase.java:34) finished in 0.251 s
2018-02-06 12:53:46,335 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 3 finished: jdbc at CreateDataSetFromDatabase.java:34, took 0.290130 s
2018-02-06 12:53:46,358 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 12:53:46,363 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@4b3c354a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 12:53:46,365 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 12:53:46,376 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 12:53:46,395 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 12:53:46,395 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 12:53:46,397 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 12:53:46,399 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 12:53:46,402 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 12:53:46,403 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 12:53:46,405 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-ad867112-3330-416f-ac25-1f800b4e6211
2018-02-06 12:57:43,559 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 12:57:44,365 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 12:57:44,396 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 12:57:44,397 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 12:57:44,398 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 12:57:44,400 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 12:57:44,400 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 12:57:44,866 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 54797.
2018-02-06 12:57:44,892 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 12:57:44,950 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 12:57:44,954 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 12:57:44,955 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 12:57:44,963 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-6af22548-3bcb-43e3-ac33-18718d87adff
2018-02-06 12:57:44,988 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 12:57:45,063 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 12:57:45,177 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3235ms
2018-02-06 12:57:45,281 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 12:57:45,297 INFO[org.spark_project.jetty.server.Server:403] - Started @3358ms
2018-02-06 12:57:45,322 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@1737dff5{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 12:57:45,322 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 12:57:45,349 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,351 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,353 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,354 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,355 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/stages,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,356 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,358 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,359 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ce3db41{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,362 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,363 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,365 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/storage,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,366 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,367 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,370 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,371 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/environment,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,372 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,373 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/executors,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,373 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,374 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,375 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,383 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/static,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,384 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,385 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/api,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,386 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,387 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 12:57:45,391 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 12:57:45,504 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 12:57:45,539 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54810.
2018-02-06 12:57:45,540 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:54810
2018-02-06 12:57:45,542 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 12:57:45,550 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 54810, None)
2018-02-06 12:57:45,560 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:54810 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 54810, None)
2018-02-06 12:57:45,563 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 54810, None)
2018-02-06 12:57:45,564 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 54810, None)
2018-02-06 12:57:45,912 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@420bc288{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 12:57:46,006 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 12:57:46,007 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 12:57:46,018 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fa05212{/SQL,null,AVAILABLE,@Spark}
2018-02-06 12:57:46,019 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c09d180{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 12:57:46,025 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2d83c5a5{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 12:57:46,027 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e053511{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 12:57:46,030 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28b576a9{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 12:57:47,298 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 12:57:49,892 WARN[org.apache.spark.util.Utils:66] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
2018-02-06 12:57:50,694 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 402.443969 ms
2018-02-06 12:57:50,781 INFO[org.apache.spark.SparkContext:54] - Starting job: show at CreateDataSetFromDatabase.java:28
2018-02-06 12:57:50,803 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at CreateDataSetFromDatabase.java:28) with 1 output partitions
2018-02-06 12:57:50,804 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (show at CreateDataSetFromDatabase.java:28)
2018-02-06 12:57:50,804 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 12:57:50,806 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 12:57:50,814 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at show at CreateDataSetFromDatabase.java:28), which has no missing parents
2018-02-06 12:57:50,959 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 17.9 KB, free 631.8 MB)
2018-02-06 12:57:50,998 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.9 KB, free 631.8 MB)
2018-02-06 12:57:51,001 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:54810 (size: 6.9 KB, free: 631.8 MB)
2018-02-06 12:57:51,003 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:57:51,017 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at CreateDataSetFromDatabase.java:28) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:57:51,018 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 12:57:51,059 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 12:57:51,072 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 12:57:51,182 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 12:57:51,199 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 5459 bytes result sent to driver
2018-02-06 12:57:51,212 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 163 ms on localhost (executor driver) (1/1)
2018-02-06 12:57:51,214 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 12:57:51,218 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (show at CreateDataSetFromDatabase.java:28) finished in 0.182 s
2018-02-06 12:57:51,224 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at CreateDataSetFromDatabase.java:28, took 0.442942 s
2018-02-06 12:57:51,330 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 68.105302 ms
2018-02-06 12:57:51,619 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 21.367367 ms
2018-02-06 12:57:51,636 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 12.745924 ms
2018-02-06 12:57:51,682 INFO[org.apache.spark.SparkContext:54] - Starting job: show at CreateDataSetFromDatabase.java:30
2018-02-06 12:57:51,685 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 5 (show at CreateDataSetFromDatabase.java:30)
2018-02-06 12:57:51,686 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at CreateDataSetFromDatabase.java:30) with 1 output partitions
2018-02-06 12:57:51,686 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (show at CreateDataSetFromDatabase.java:30)
2018-02-06 12:57:51,687 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-06 12:57:51,687 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-06 12:57:51,689 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[5] at show at CreateDataSetFromDatabase.java:30), which has no missing parents
2018-02-06 12:57:51,704 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 10.5 KB, free 631.8 MB)
2018-02-06 12:57:51,707 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KB, free 631.8 MB)
2018-02-06 12:57:51,708 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:54810 (size: 5.5 KB, free: 631.8 MB)
2018-02-06 12:57:51,708 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:57:51,711 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[5] at show at CreateDataSetFromDatabase.java:30) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:57:51,711 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 12:57:51,715 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 4638 bytes)
2018-02-06 12:57:51,716 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 12:57:51,746 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 12:57:51,779 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1546 bytes result sent to driver
2018-02-06 12:57:51,784 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 69 ms on localhost (executor driver) (1/1)
2018-02-06 12:57:51,784 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 12:57:51,785 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (show at CreateDataSetFromDatabase.java:30) finished in 0.072 s
2018-02-06 12:57:51,785 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-06 12:57:51,786 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-06 12:57:51,786 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-06 12:57:51,787 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-06 12:57:51,790 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[8] at show at CreateDataSetFromDatabase.java:30), which has no missing parents
2018-02-06 12:57:51,801 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 7.5 KB, free 631.8 MB)
2018-02-06 12:57:51,802 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.0 KB, free 631.7 MB)
2018-02-06 12:57:51,805 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:54810 (size: 4.0 KB, free: 631.8 MB)
2018-02-06 12:57:51,806 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:57:51,808 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at CreateDataSetFromDatabase.java:30) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:57:51,808 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
2018-02-06 12:57:51,812 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
2018-02-06 12:57:51,813 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 2)
2018-02-06 12:57:51,834 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 1 blocks
2018-02-06 12:57:51,836 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
2018-02-06 12:57:51,857 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 2). 1514 bytes result sent to driver
2018-02-06 12:57:51,861 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 2) in 50 ms on localhost (executor driver) (1/1)
2018-02-06 12:57:51,861 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-06 12:57:51,862 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (show at CreateDataSetFromDatabase.java:30) finished in 0.051 s
2018-02-06 12:57:51,863 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at CreateDataSetFromDatabase.java:30, took 0.180486 s
2018-02-06 12:57:51,877 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 11.389443 ms
2018-02-06 12:57:51,881 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: bill
2018-02-06 12:57:52,099 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 1
2018-02-06 12:57:52,100 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 50
2018-02-06 12:57:52,100 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 53
2018-02-06 12:57:52,100 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 52
2018-02-06 12:57:52,100 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 57
2018-02-06 12:57:52,100 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 61
2018-02-06 12:57:52,107 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 60
2018-02-06 12:57:52,124 INFO[org.apache.spark.ContextCleaner:54] - Cleaned shuffle 0
2018-02-06 12:57:52,124 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 59
2018-02-06 12:57:52,124 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 55
2018-02-06 12:57:52,125 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 56
2018-02-06 12:57:52,125 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 54
2018-02-06 12:57:52,155 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:54810 in memory (size: 5.5 KB, free: 631.8 MB)
2018-02-06 12:57:52,158 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 62
2018-02-06 12:57:52,159 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_0_piece0 on 192.168.11.26:54810 in memory (size: 6.9 KB, free: 631.8 MB)
2018-02-06 12:57:52,160 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 51
2018-02-06 12:57:52,160 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 58
2018-02-06 12:57:52,161 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_2_piece0 on 192.168.11.26:54810 in memory (size: 4.0 KB, free: 631.8 MB)
2018-02-06 12:57:52,163 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select *from bill
2018-02-06 12:57:52,163 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 0
2018-02-06 12:57:52,416 INFO[org.apache.spark.SparkContext:54] - Starting job: show at CreateDataSetFromDatabase.java:33
2018-02-06 12:57:52,422 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 2 (show at CreateDataSetFromDatabase.java:33) with 1 output partitions
2018-02-06 12:57:52,422 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 3 (show at CreateDataSetFromDatabase.java:33)
2018-02-06 12:57:52,422 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 12:57:52,422 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 12:57:52,423 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 3 (MapPartitionsRDD[11] at show at CreateDataSetFromDatabase.java:33), which has no missing parents
2018-02-06 12:57:52,428 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 17.9 KB, free 631.8 MB)
2018-02-06 12:57:52,432 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.9 KB, free 631.8 MB)
2018-02-06 12:57:52,434 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:54810 (size: 6.9 KB, free: 631.8 MB)
2018-02-06 12:57:52,435 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:57:52,437 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at show at CreateDataSetFromDatabase.java:33) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:57:52,437 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 1 tasks
2018-02-06 12:57:52,438 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 12:57:52,439 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 3)
2018-02-06 12:57:52,468 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 12:57:52,469 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 3). 5373 bytes result sent to driver
2018-02-06 12:57:52,476 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 3) in 38 ms on localhost (executor driver) (1/1)
2018-02-06 12:57:52,477 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 3 (show at CreateDataSetFromDatabase.java:33) finished in 0.040 s
2018-02-06 12:57:52,477 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 2 finished: show at CreateDataSetFromDatabase.java:33, took 0.060865 s
2018-02-06 12:57:52,478 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2018-02-06 12:58:54,936 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 12:58:55,599 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 12:58:55,624 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 12:58:55,625 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 12:58:55,626 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 12:58:55,627 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 12:58:55,628 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 12:58:56,007 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 54941.
2018-02-06 12:58:56,027 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 12:58:56,078 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 12:58:56,082 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 12:58:56,082 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 12:58:56,091 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-b84df9c6-29c9-472d-b31d-2c91d5d47523
2018-02-06 12:58:56,114 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 12:58:56,165 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 12:58:56,250 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2778ms
2018-02-06 12:58:56,318 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 12:58:56,334 INFO[org.spark_project.jetty.server.Server:403] - Started @2864ms
2018-02-06 12:58:56,354 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@644f06a1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 12:58:56,354 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 12:58:56,378 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,379 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,379 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,380 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,381 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/stages,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,381 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,382 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,383 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ce3db41{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,385 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,386 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,387 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/storage,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,387 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,388 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,388 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,390 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/environment,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,391 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,391 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/executors,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,392 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,393 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,393 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,401 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/static,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,402 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,403 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/api,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,403 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,404 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,406 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 12:58:56,489 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 12:58:56,517 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54954.
2018-02-06 12:58:56,518 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:54954
2018-02-06 12:58:56,520 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 12:58:56,523 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 54954, None)
2018-02-06 12:58:56,529 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:54954 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 54954, None)
2018-02-06 12:58:56,532 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 54954, None)
2018-02-06 12:58:56,533 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 54954, None)
2018-02-06 12:58:56,753 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@420bc288{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,827 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 12:58:56,828 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 12:58:56,836 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fa05212{/SQL,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,838 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c09d180{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,840 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2d83c5a5{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,841 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e053511{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 12:58:56,843 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28b576a9{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 12:58:57,900 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 12:58:59,875 WARN[org.apache.spark.util.Utils:66] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
2018-02-06 12:59:00,368 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 0
2018-02-06 12:59:00,479 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 295.821535 ms
2018-02-06 12:59:00,501 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 15.026885 ms
2018-02-06 12:59:00,600 INFO[org.apache.spark.SparkContext:54] - Starting job: show at CreateDataSetFromDatabase.java:29
2018-02-06 12:59:00,621 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 2 (show at CreateDataSetFromDatabase.java:29)
2018-02-06 12:59:00,623 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at CreateDataSetFromDatabase.java:29) with 1 output partitions
2018-02-06 12:59:00,624 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at CreateDataSetFromDatabase.java:29)
2018-02-06 12:59:00,624 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 0)
2018-02-06 12:59:00,625 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 0)
2018-02-06 12:59:00,629 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at show at CreateDataSetFromDatabase.java:29), which has no missing parents
2018-02-06 12:59:00,754 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 10.5 KB, free 631.8 MB)
2018-02-06 12:59:00,788 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.5 KB, free 631.8 MB)
2018-02-06 12:59:00,791 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:54954 (size: 5.5 KB, free: 631.8 MB)
2018-02-06 12:59:00,793 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:59:00,809 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at show at CreateDataSetFromDatabase.java:29) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:59:00,811 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 12:59:00,851 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4638 bytes)
2018-02-06 12:59:00,861 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 12:59:00,950 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 12:59:00,990 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1589 bytes result sent to driver
2018-02-06 12:59:00,997 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 155 ms on localhost (executor driver) (1/1)
2018-02-06 12:59:00,999 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 12:59:01,004 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 0 (show at CreateDataSetFromDatabase.java:29) finished in 0.172 s
2018-02-06 12:59:01,005 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-06 12:59:01,005 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-06 12:59:01,006 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 1)
2018-02-06 12:59:01,006 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-06 12:59:01,010 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at CreateDataSetFromDatabase.java:29), which has no missing parents
2018-02-06 12:59:01,027 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 7.5 KB, free 631.8 MB)
2018-02-06 12:59:01,030 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.0 KB, free 631.8 MB)
2018-02-06 12:59:01,032 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:54954 (size: 4.0 KB, free: 631.8 MB)
2018-02-06 12:59:01,032 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:59:01,035 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at CreateDataSetFromDatabase.java:29) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:59:01,035 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 12:59:01,042 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4726 bytes)
2018-02-06 12:59:01,043 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 12:59:01,065 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 1 blocks
2018-02-06 12:59:01,067 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
2018-02-06 12:59:01,092 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1514 bytes result sent to driver
2018-02-06 12:59:01,093 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 53 ms on localhost (executor driver) (1/1)
2018-02-06 12:59:01,094 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 12:59:01,095 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at CreateDataSetFromDatabase.java:29) finished in 0.055 s
2018-02-06 12:59:01,099 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at CreateDataSetFromDatabase.java:29, took 0.499573 s
2018-02-06 12:59:01,126 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 13.311044 ms
2018-02-06 12:59:01,138 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: bill
2018-02-06 12:59:01,302 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select *from bill
2018-02-06 12:59:01,502 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 40.493133 ms
2018-02-06 12:59:01,512 INFO[org.apache.spark.SparkContext:54] - Starting job: show at CreateDataSetFromDatabase.java:32
2018-02-06 12:59:01,516 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at CreateDataSetFromDatabase.java:32) with 1 output partitions
2018-02-06 12:59:01,516 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (show at CreateDataSetFromDatabase.java:32)
2018-02-06 12:59:01,516 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 12:59:01,516 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 12:59:01,517 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[8] at show at CreateDataSetFromDatabase.java:32), which has no missing parents
2018-02-06 12:59:01,529 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 17.9 KB, free 631.8 MB)
2018-02-06 12:59:01,530 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.9 KB, free 631.7 MB)
2018-02-06 12:59:01,533 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:54954 (size: 6.9 KB, free: 631.8 MB)
2018-02-06 12:59:01,534 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:59:01,535 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at CreateDataSetFromDatabase.java:32) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:59:01,535 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
2018-02-06 12:59:01,536 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 12:59:01,537 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 2)
2018-02-06 12:59:01,570 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 12:59:01,573 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 2). 5416 bytes result sent to driver
2018-02-06 12:59:01,575 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 2) in 39 ms on localhost (executor driver) (1/1)
2018-02-06 12:59:01,576 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-06 12:59:01,576 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (show at CreateDataSetFromDatabase.java:32) finished in 0.041 s
2018-02-06 12:59:01,577 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at CreateDataSetFromDatabase.java:32, took 0.064408 s
2018-02-06 12:59:01,637 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 48.910096 ms
2018-02-06 12:59:01,854 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:54954 in memory (size: 4.0 KB, free: 631.8 MB)
2018-02-06 12:59:01,858 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 86
2018-02-06 12:59:01,859 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_0_piece0 on 192.168.11.26:54954 in memory (size: 5.5 KB, free: 631.8 MB)
2018-02-06 12:59:01,860 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 85
2018-02-06 12:59:01,860 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_2_piece0 on 192.168.11.26:54954 in memory (size: 6.9 KB, free: 631.8 MB)
2018-02-06 12:59:09,327 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 12:59:09,327 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 12:59:09,329 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2018-02-06 12:59:09,422 INFO[org.apache.spark.SparkContext:54] - Starting job: json at CreateDataSetFromDatabase.java:36
2018-02-06 12:59:09,423 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 2 (json at CreateDataSetFromDatabase.java:36) with 1 output partitions
2018-02-06 12:59:09,424 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 3 (json at CreateDataSetFromDatabase.java:36)
2018-02-06 12:59:09,424 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 12:59:09,426 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 12:59:09,427 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 3 (MapPartitionsRDD[12] at json at CreateDataSetFromDatabase.java:36), which has no missing parents
2018-02-06 12:59:09,451 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 122.6 KB, free 631.7 MB)
2018-02-06 12:59:09,456 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 44.4 KB, free 631.6 MB)
2018-02-06 12:59:09,457 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:54954 (size: 44.4 KB, free: 631.8 MB)
2018-02-06 12:59:09,457 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:59:09,459 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[12] at json at CreateDataSetFromDatabase.java:36) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:59:09,459 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 1 tasks
2018-02-06 12:59:09,460 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 12:59:09,460 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 3)
2018-02-06 12:59:09,598 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 12:59:09,611 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block rdd_11_0 stored as values in memory (estimated size 26.2 KB, free 631.6 MB)
2018-02-06 12:59:09,612 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_11_0 in memory on 192.168.11.26:54954 (size: 26.2 KB, free: 631.7 MB)
2018-02-06 12:59:09,626 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 6.286402 ms
2018-02-06 12:59:09,710 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 67.174101 ms
2018-02-06 12:59:09,714 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 12:59:09,714 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 12:59:09,717 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2018-02-06 12:59:10,038 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:582] - Saved output of task 'attempt_20180206125909_0003_m_000000_0' to hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/_temporary/0/task_20180206125909_0003_m_000000
2018-02-06 12:59:10,039 INFO[org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20180206125909_0003_m_000000_0: Committed
2018-02-06 12:59:10,043 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 3). 2394 bytes result sent to driver
2018-02-06 12:59:10,044 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 3) in 585 ms on localhost (executor driver) (1/1)
2018-02-06 12:59:10,045 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2018-02-06 12:59:10,045 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 3 (json at CreateDataSetFromDatabase.java:36) finished in 0.586 s
2018-02-06 12:59:10,047 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 2 finished: json at CreateDataSetFromDatabase.java:36, took 0.623858 s
2018-02-06 12:59:10,101 INFO[org.apache.spark.sql.execution.datasources.FileFormatWriter:54] - Job null committed.
2018-02-06 12:59:10,177 INFO[org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat:54] - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 12:59:10,242 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 12:59:10,242 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 12:59:10,248 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 12:59:10,249 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 12:59:10,250 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 12:59:10,251 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 12:59:10,286 INFO[org.apache.spark.SparkContext:54] - Starting job: parquet at CreateDataSetFromDatabase.java:37
2018-02-06 12:59:10,287 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 3 (parquet at CreateDataSetFromDatabase.java:37) with 1 output partitions
2018-02-06 12:59:10,287 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 4 (parquet at CreateDataSetFromDatabase.java:37)
2018-02-06 12:59:10,287 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 12:59:10,288 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 12:59:10,290 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 4 (MapPartitionsRDD[15] at parquet at CreateDataSetFromDatabase.java:37), which has no missing parents
2018-02-06 12:59:10,308 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 125.6 KB, free 631.5 MB)
2018-02-06 12:59:10,312 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 44.4 KB, free 631.4 MB)
2018-02-06 12:59:10,314 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.11.26:54954 (size: 44.4 KB, free: 631.7 MB)
2018-02-06 12:59:10,315 INFO[org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:59:10,315 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at parquet at CreateDataSetFromDatabase.java:37) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:59:10,315 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 4.0 with 1 tasks
2018-02-06 12:59:10,318 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 12:59:10,319 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 4.0 (TID 4)
2018-02-06 12:59:10,329 INFO[org.apache.spark.storage.BlockManager:54] - Found block rdd_11_0 locally
2018-02-06 12:59:10,332 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 12:59:10,332 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 12:59:10,333 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 12:59:10,333 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 12:59:10,333 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 12:59:10,333 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 12:59:10,339 INFO[org.apache.parquet.hadoop.codec.CodecConfig:95] - Compression: SNAPPY
2018-02-06 12:59:10,341 INFO[org.apache.parquet.hadoop.codec.CodecConfig:95] - Compression: SNAPPY
2018-02-06 12:59:10,361 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:329] - Parquet block size to 134217728
2018-02-06 12:59:10,361 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:330] - Parquet page size to 1048576
2018-02-06 12:59:10,361 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:331] - Parquet dictionary page size to 1048576
2018-02-06 12:59:10,361 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:332] - Dictionary is on
2018-02-06 12:59:10,361 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:333] - Validation is off
2018-02-06 12:59:10,361 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:334] - Writer version is: PARQUET_1_0
2018-02-06 12:59:10,361 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:335] - Maximum row group padding size is 0 bytes
2018-02-06 12:59:10,361 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:336] - Page size checking is: estimated
2018-02-06 12:59:10,362 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:337] - Min row count for page size check is: 100
2018-02-06 12:59:10,362 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:338] - Max row count for page size check is: 10000
2018-02-06 12:59:10,400 INFO[org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport:54] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bill_id",
    "type" : "integer",
    "nullable" : false,
    "metadata" : {
      "name" : "bill_id",
      "scale" : 0
    }
  }, {
    "name" : "bill_status",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_status",
      "scale" : 0
    }
  }, {
    "name" : "bill_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_type",
      "scale" : 0
    }
  }, {
    "name" : "create_date",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : {
      "name" : "create_date",
      "scale" : 0
    }
  }, {
    "name" : "bill_category_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_category_id",
      "scale" : 0
    }
  }, {
    "name" : "bill_category_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_category_name",
      "scale" : 0
    }
  }, {
    "name" : "bill_trade_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_trade_type",
      "scale" : 0
    }
  }, {
    "name" : "bill_trade_area",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_trade_area",
      "scale" : 0
    }
  }, {
    "name" : "bill_accept_org",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_accept_org",
      "scale" : 0
    }
  }, {
    "name" : "bill_user_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_user_name",
      "scale" : 0
    }
  }, {
    "name" : "bill_feature",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_feature",
      "scale" : 0
    }
  }, {
    "name" : "bill_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_no",
      "scale" : 0
    }
  }, {
    "name" : "bill_classify",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_classify",
      "scale" : 0
    }
  }, {
    "name" : "bill_sstatus",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_sstatus",
      "scale" : 0
    }
  }, {
    "name" : "bill_company",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_company",
      "scale" : 0
    }
  }, {
    "name" : "bill_bank",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_bank",
      "scale" : 0
    }
  }, {
    "name" : "bill_money",
    "type" : "double",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_money",
      "scale" : 2
    }
  }, {
    "name" : "bill_expire",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_expire",
      "scale" : 0
    }
  }, {
    "name" : "bill_endorse",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_endorse",
      "scale" : 0
    }
  }, {
    "name" : "bill_position",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_position",
      "scale" : 0
    }
  }, {
    "name" : "bill_img",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_img",
      "scale" : 0
    }
  }, {
    "name" : "bill_endorse_img",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_endorse_img",
      "scale" : 0
    }
  }, {
    "name" : "bill_evidence",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_evidence",
      "scale" : 0
    }
  }, {
    "name" : "bill_bg_img",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_bg_img",
      "scale" : 0
    }
  }, {
    "name" : "bill_bg_img2",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_bg_img2",
      "scale" : 0
    }
  }, {
    "name" : "bill_img_health",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_img_health",
      "scale" : 0
    }
  }, {
    "name" : "bill_rate",
    "type" : "double",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_rate",
      "scale" : 2
    }
  }, {
    "name" : "bill_increment",
    "type" : "double",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_increment",
      "scale" : 2
    }
  }, {
    "name" : "bill_discount_amount",
    "type" : "double",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_discount_amount",
      "scale" : 2
    }
  }, {
    "name" : "bill_desc",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_desc",
      "scale" : 0
    }
  }, {
    "name" : "bill_quote_count",
    "type" : "integer",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_quote_count",
      "scale" : 0
    }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : {
      "name" : "user_id",
      "scale" : 0
    }
  }, {
    "name" : "bill_visible",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_visible",
      "scale" : 0
    }
  }, {
    "name" : "trade_date",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : {
      "name" : "trade_date",
      "scale" : 0
    }
  }, {
    "name" : "trade_desc",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "trade_desc",
      "scale" : 0
    }
  }, {
    "name" : "bill_return",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_return",
      "scale" : 0
    }
  }, {
    "name" : "bill_quote_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_quote_type",
      "scale" : 0
    }
  }, {
    "name" : "bill_fixed_price",
    "type" : "double",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_fixed_price",
      "scale" : 2
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 bill_id;
  optional binary bill_status (UTF8);
  optional binary bill_type (UTF8);
  optional int96 create_date;
  optional int32 bill_category_id;
  optional binary bill_category_name (UTF8);
  optional binary bill_trade_type (UTF8);
  optional binary bill_trade_area (UTF8);
  optional binary bill_accept_org (UTF8);
  optional binary bill_user_name (UTF8);
  optional binary bill_feature (UTF8);
  optional binary bill_no (UTF8);
  optional binary bill_classify (UTF8);
  optional binary bill_sstatus (UTF8);
  optional binary bill_company (UTF8);
  optional binary bill_bank (UTF8);
  optional double bill_money;
  optional binary bill_expire (UTF8);
  optional binary bill_endorse (UTF8);
  optional binary bill_position (UTF8);
  optional binary bill_img (UTF8);
  optional binary bill_endorse_img (UTF8);
  optional binary bill_evidence (UTF8);
  optional binary bill_bg_img (UTF8);
  optional binary bill_bg_img2 (UTF8);
  optional binary bill_img_health (UTF8);
  optional double bill_rate;
  optional double bill_increment;
  optional double bill_discount_amount;
  optional binary bill_desc (UTF8);
  optional int32 bill_quote_count;
  optional int32 user_id;
  optional binary bill_visible (UTF8);
  optional int96 trade_date;
  optional binary trade_desc (UTF8);
  optional binary bill_return (UTF8);
  optional binary bill_quote_type (UTF8);
  optional double bill_fixed_price;
}

       
2018-02-06 12:59:10,439 INFO[org.apache.hadoop.io.compress.CodecPool:153] - Got brand-new compressor [.snappy]
2018-02-06 12:59:10,624 INFO[org.apache.parquet.hadoop.InternalParquetRecordWriter:160] - Flushing mem columnStore to file. allocated memory: 20148
2018-02-06 12:59:10,812 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:582] - Saved output of task 'attempt_20180206125910_0004_m_000000_0' to hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.parquet/_temporary/0/task_20180206125910_0004_m_000000
2018-02-06 12:59:10,812 INFO[org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20180206125910_0004_m_000000_0: Committed
2018-02-06 12:59:10,813 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 4.0 (TID 4). 1627 bytes result sent to driver
2018-02-06 12:59:10,815 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 4.0 (TID 4) in 498 ms on localhost (executor driver) (1/1)
2018-02-06 12:59:10,815 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2018-02-06 12:59:10,816 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 4 (parquet at CreateDataSetFromDatabase.java:37) finished in 0.499 s
2018-02-06 12:59:10,816 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 3 finished: parquet at CreateDataSetFromDatabase.java:37, took 0.529523 s
2018-02-06 12:59:10,856 INFO[org.apache.spark.sql.execution.datasources.FileFormatWriter:54] - Job null committed.
2018-02-06 12:59:10,921 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 12:59:10,921 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 12:59:10,922 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2018-02-06 12:59:10,958 INFO[org.apache.spark.SparkContext:54] - Starting job: csv at CreateDataSetFromDatabase.java:38
2018-02-06 12:59:10,960 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 4 (csv at CreateDataSetFromDatabase.java:38) with 1 output partitions
2018-02-06 12:59:10,960 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 5 (csv at CreateDataSetFromDatabase.java:38)
2018-02-06 12:59:10,960 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 12:59:10,960 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 12:59:10,960 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 5 (MapPartitionsRDD[18] at csv at CreateDataSetFromDatabase.java:38), which has no missing parents
2018-02-06 12:59:10,974 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5 stored as values in memory (estimated size 122.9 KB, free 631.3 MB)
2018-02-06 12:59:10,977 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 44.5 KB, free 631.3 MB)
2018-02-06 12:59:10,980 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_5_piece0 in memory on 192.168.11.26:54954 (size: 44.5 KB, free: 631.6 MB)
2018-02-06 12:59:10,981 INFO[org.apache.spark.SparkContext:54] - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
2018-02-06 12:59:10,981 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[18] at csv at CreateDataSetFromDatabase.java:38) (first 15 tasks are for partitions Vector(0))
2018-02-06 12:59:10,981 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 5.0 with 1 tasks
2018-02-06 12:59:10,982 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 12:59:10,983 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 5.0 (TID 5)
2018-02-06 12:59:10,992 INFO[org.apache.spark.storage.BlockManager:54] - Found block rdd_11_0 locally
2018-02-06 12:59:10,994 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 12:59:10,994 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 12:59:10,996 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2018-02-06 12:59:11,071 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:582] - Saved output of task 'attempt_20180206125910_0005_m_000000_0' to hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.csv/_temporary/0/task_20180206125910_0005_m_000000
2018-02-06 12:59:11,071 INFO[org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20180206125910_0005_m_000000_0: Committed
2018-02-06 12:59:11,072 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 5.0 (TID 5). 1627 bytes result sent to driver
2018-02-06 12:59:11,073 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 5.0 (TID 5) in 90 ms on localhost (executor driver) (1/1)
2018-02-06 12:59:11,073 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2018-02-06 12:59:11,073 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 5 (csv at CreateDataSetFromDatabase.java:38) finished in 0.091 s
2018-02-06 12:59:11,073 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 4 finished: csv at CreateDataSetFromDatabase.java:38, took 0.114328 s
2018-02-06 12:59:11,113 INFO[org.apache.spark.sql.execution.datasources.FileFormatWriter:54] - Job null committed.
2018-02-06 12:59:11,170 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 12:59:11,175 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@644f06a1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 12:59:11,177 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 12:59:11,187 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 12:59:11,210 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 12:59:11,211 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 12:59:11,212 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 12:59:11,215 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 12:59:11,218 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 12:59:11,218 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 12:59:11,219 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-71ef155f-8d75-4491-b41a-529e9241b928
2018-02-06 13:01:29,688 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 13:01:30,376 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 13:01:30,404 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 13:01:30,404 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 13:01:30,405 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 13:01:30,406 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 13:01:30,406 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 13:01:30,788 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 55066.
2018-02-06 13:01:30,807 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 13:01:30,854 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 13:01:30,858 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 13:01:30,859 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 13:01:30,867 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-ace3c5d8-576b-4fbf-be71-8d5edba89885
2018-02-06 13:01:30,891 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 13:01:30,942 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 13:01:31,027 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2644ms
2018-02-06 13:01:31,101 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 13:01:31,116 INFO[org.spark_project.jetty.server.Server:403] - Started @2736ms
2018-02-06 13:01:31,139 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@4d9c0837{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 13:01:31,140 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 13:01:31,166 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,166 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,167 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,168 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,169 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/stages,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,170 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,171 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,173 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ce3db41{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,173 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,174 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,175 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/storage,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,176 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,177 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,178 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,179 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/environment,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,180 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,182 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/executors,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,183 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,184 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,185 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,192 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/static,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,193 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,194 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/api,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,195 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,196 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,198 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 13:01:31,298 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 13:01:31,326 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55079.
2018-02-06 13:01:31,327 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:55079
2018-02-06 13:01:31,328 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 13:01:31,329 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 55079, None)
2018-02-06 13:01:31,332 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:55079 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 55079, None)
2018-02-06 13:01:31,335 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 55079, None)
2018-02-06 13:01:31,336 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 55079, None)
2018-02-06 13:01:31,513 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@420bc288{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,578 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 13:01:31,578 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 13:01:31,586 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@742d4e15{/SQL,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,586 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50b1f030{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,588 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@23aae55{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,589 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@680bddf5{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 13:01:31,590 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53bf7094{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 13:01:32,641 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 13:01:34,622 WARN[org.apache.spark.util.Utils:66] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
2018-02-06 13:01:35,112 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 0
2018-02-06 13:01:35,209 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 292.264093 ms
2018-02-06 13:01:35,230 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 12.634244 ms
2018-02-06 13:01:35,334 INFO[org.apache.spark.SparkContext:54] - Starting job: show at CreateDataSetFromDatabase.java:29
2018-02-06 13:01:35,355 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 2 (show at CreateDataSetFromDatabase.java:29)
2018-02-06 13:01:35,357 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at CreateDataSetFromDatabase.java:29) with 1 output partitions
2018-02-06 13:01:35,358 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at CreateDataSetFromDatabase.java:29)
2018-02-06 13:01:35,358 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 0)
2018-02-06 13:01:35,359 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 0)
2018-02-06 13:01:35,363 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at show at CreateDataSetFromDatabase.java:29), which has no missing parents
2018-02-06 13:01:35,508 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 10.5 KB, free 631.8 MB)
2018-02-06 13:01:35,541 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.5 KB, free 631.8 MB)
2018-02-06 13:01:35,548 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:55079 (size: 5.5 KB, free: 631.8 MB)
2018-02-06 13:01:35,550 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-06 13:01:35,563 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at show at CreateDataSetFromDatabase.java:29) (first 15 tasks are for partitions Vector(0))
2018-02-06 13:01:35,565 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 13:01:35,602 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4638 bytes)
2018-02-06 13:01:35,611 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 13:01:35,702 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 13:01:35,756 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1589 bytes result sent to driver
2018-02-06 13:01:35,764 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 171 ms on localhost (executor driver) (1/1)
2018-02-06 13:01:35,766 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 13:01:35,772 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 0 (show at CreateDataSetFromDatabase.java:29) finished in 0.188 s
2018-02-06 13:01:35,774 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-06 13:01:35,775 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-06 13:01:35,776 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 1)
2018-02-06 13:01:35,777 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-06 13:01:35,781 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at CreateDataSetFromDatabase.java:29), which has no missing parents
2018-02-06 13:01:35,796 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 7.5 KB, free 631.8 MB)
2018-02-06 13:01:35,798 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.0 KB, free 631.8 MB)
2018-02-06 13:01:35,799 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:55079 (size: 4.0 KB, free: 631.8 MB)
2018-02-06 13:01:35,800 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 13:01:35,803 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at CreateDataSetFromDatabase.java:29) (first 15 tasks are for partitions Vector(0))
2018-02-06 13:01:35,803 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 13:01:35,812 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4726 bytes)
2018-02-06 13:01:35,812 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 13:01:35,830 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 1 blocks
2018-02-06 13:01:35,831 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
2018-02-06 13:01:35,853 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1514 bytes result sent to driver
2018-02-06 13:01:35,854 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 48 ms on localhost (executor driver) (1/1)
2018-02-06 13:01:35,854 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 13:01:35,855 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at CreateDataSetFromDatabase.java:29) finished in 0.050 s
2018-02-06 13:01:35,861 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at CreateDataSetFromDatabase.java:29, took 0.525337 s
2018-02-06 13:01:35,883 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 8.719043 ms
2018-02-06 13:01:35,897 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: bill
2018-02-06 13:01:36,057 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select *from bill
2018-02-06 13:01:36,309 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 78.180825 ms
2018-02-06 13:01:36,324 INFO[org.apache.spark.SparkContext:54] - Starting job: show at CreateDataSetFromDatabase.java:32
2018-02-06 13:01:36,329 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at CreateDataSetFromDatabase.java:32) with 1 output partitions
2018-02-06 13:01:36,329 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (show at CreateDataSetFromDatabase.java:32)
2018-02-06 13:01:36,329 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 13:01:36,330 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 13:01:36,330 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[8] at show at CreateDataSetFromDatabase.java:32), which has no missing parents
2018-02-06 13:01:36,361 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 17.9 KB, free 631.8 MB)
2018-02-06 13:01:36,366 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.9 KB, free 631.7 MB)
2018-02-06 13:01:36,371 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:55079 (size: 6.9 KB, free: 631.8 MB)
2018-02-06 13:01:36,373 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-06 13:01:36,374 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at CreateDataSetFromDatabase.java:32) (first 15 tasks are for partitions Vector(0))
2018-02-06 13:01:36,374 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
2018-02-06 13:01:36,376 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 13:01:36,377 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 2)
2018-02-06 13:01:36,384 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:55079 in memory (size: 4.0 KB, free: 631.8 MB)
2018-02-06 13:01:36,390 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_0_piece0 on 192.168.11.26:55079 in memory (size: 5.5 KB, free: 631.8 MB)
2018-02-06 13:01:36,403 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 13:01:36,405 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 2). 5416 bytes result sent to driver
2018-02-06 13:01:36,407 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 2) in 32 ms on localhost (executor driver) (1/1)
2018-02-06 13:01:36,407 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-06 13:01:36,408 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (show at CreateDataSetFromDatabase.java:32) finished in 0.034 s
2018-02-06 13:01:36,409 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at CreateDataSetFromDatabase.java:32, took 0.085218 s
2018-02-06 13:01:36,484 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 60.065939 ms
2018-02-06 13:01:44,189 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 85
2018-02-06 13:01:44,191 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_2_piece0 on 192.168.11.26:55079 in memory (size: 6.9 KB, free: 631.8 MB)
2018-02-06 13:01:44,192 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 86
2018-02-06 13:01:44,227 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 13:01:44,234 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@4d9c0837{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 13:01:44,236 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 13:01:44,246 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 13:01:44,264 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 13:01:44,265 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 13:01:44,266 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 13:01:44,268 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 13:01:44,271 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 13:01:44,272 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 13:01:44,273 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-07413bc7-105b-451a-b630-1349bc970262
2018-02-06 13:02:29,211 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 13:02:29,828 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 13:02:29,852 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 13:02:29,853 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 13:02:29,853 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 13:02:29,854 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 13:02:29,855 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 13:02:30,238 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 55144.
2018-02-06 13:02:30,259 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 13:02:30,314 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 13:02:30,316 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 13:02:30,317 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 13:02:30,327 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-5727a21a-c17f-4abc-8bf0-866e5443a738
2018-02-06 13:02:30,351 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 13:02:30,400 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 13:02:30,487 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2749ms
2018-02-06 13:02:30,561 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 13:02:30,574 INFO[org.spark_project.jetty.server.Server:403] - Started @2838ms
2018-02-06 13:02:30,596 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@1a7a93e3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 13:02:30,597 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 13:02:30,623 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,624 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,625 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,625 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,626 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/stages,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,627 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,628 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,629 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ce3db41{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,630 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,631 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,632 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/storage,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,632 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,633 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,633 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,634 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/environment,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,635 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,635 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/executors,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,636 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,638 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,640 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,649 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/static,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,651 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,654 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/api,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,655 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,656 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 13:02:30,659 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 13:02:30,750 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 13:02:30,778 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55157.
2018-02-06 13:02:30,779 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:55157
2018-02-06 13:02:30,781 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 13:02:30,782 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 55157, None)
2018-02-06 13:02:30,786 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:55157 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 55157, None)
2018-02-06 13:02:30,789 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 55157, None)
2018-02-06 13:02:30,790 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 55157, None)
2018-02-06 13:02:30,978 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@420bc288{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 13:02:31,040 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 13:02:31,041 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 13:02:31,048 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@742d4e15{/SQL,null,AVAILABLE,@Spark}
2018-02-06 13:02:31,049 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50b1f030{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 13:02:31,051 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@23aae55{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 13:02:31,052 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@680bddf5{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 13:02:31,055 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53bf7094{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 13:02:32,096 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 13:02:34,054 WARN[org.apache.spark.util.Utils:66] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
2018-02-06 13:02:34,533 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 0
2018-02-06 13:02:34,639 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 293.806494 ms
2018-02-06 13:02:34,666 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 18.283526 ms
2018-02-06 13:02:34,767 INFO[org.apache.spark.SparkContext:54] - Starting job: show at CreateDataSetFromDatabase.java:29
2018-02-06 13:02:34,787 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 2 (show at CreateDataSetFromDatabase.java:29)
2018-02-06 13:02:34,789 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at CreateDataSetFromDatabase.java:29) with 1 output partitions
2018-02-06 13:02:34,789 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at CreateDataSetFromDatabase.java:29)
2018-02-06 13:02:34,789 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 0)
2018-02-06 13:02:34,790 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 0)
2018-02-06 13:02:34,795 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at show at CreateDataSetFromDatabase.java:29), which has no missing parents
2018-02-06 13:02:34,928 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 10.5 KB, free 631.8 MB)
2018-02-06 13:02:34,962 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.5 KB, free 631.8 MB)
2018-02-06 13:02:34,964 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:55157 (size: 5.5 KB, free: 631.8 MB)
2018-02-06 13:02:34,967 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-06 13:02:34,981 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at show at CreateDataSetFromDatabase.java:29) (first 15 tasks are for partitions Vector(0))
2018-02-06 13:02:34,982 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 13:02:35,020 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4638 bytes)
2018-02-06 13:02:35,029 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 13:02:35,118 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 13:02:35,164 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1589 bytes result sent to driver
2018-02-06 13:02:35,174 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 163 ms on localhost (executor driver) (1/1)
2018-02-06 13:02:35,178 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 13:02:35,186 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 0 (show at CreateDataSetFromDatabase.java:29) finished in 0.185 s
2018-02-06 13:02:35,187 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-06 13:02:35,187 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-06 13:02:35,188 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 1)
2018-02-06 13:02:35,188 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-06 13:02:35,191 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[5] at show at CreateDataSetFromDatabase.java:29), which has no missing parents
2018-02-06 13:02:35,204 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 7.5 KB, free 631.8 MB)
2018-02-06 13:02:35,209 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.0 KB, free 631.8 MB)
2018-02-06 13:02:35,211 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:55157 (size: 4.0 KB, free: 631.8 MB)
2018-02-06 13:02:35,211 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 13:02:35,218 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at show at CreateDataSetFromDatabase.java:29) (first 15 tasks are for partitions Vector(0))
2018-02-06 13:02:35,218 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 13:02:35,222 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 4726 bytes)
2018-02-06 13:02:35,223 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 13:02:35,239 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 1 blocks
2018-02-06 13:02:35,241 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
2018-02-06 13:02:35,260 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1557 bytes result sent to driver
2018-02-06 13:02:35,263 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 43 ms on localhost (executor driver) (1/1)
2018-02-06 13:02:35,263 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 13:02:35,264 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at CreateDataSetFromDatabase.java:29) finished in 0.044 s
2018-02-06 13:02:35,269 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at CreateDataSetFromDatabase.java:29, took 0.500888 s
2018-02-06 13:02:35,291 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 8.835203 ms
2018-02-06 13:02:35,305 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: bill
2018-02-06 13:02:35,459 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select *from bill
2018-02-06 13:02:35,732 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 54.176977 ms
2018-02-06 13:02:35,742 INFO[org.apache.spark.SparkContext:54] - Starting job: show at CreateDataSetFromDatabase.java:32
2018-02-06 13:02:35,746 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at CreateDataSetFromDatabase.java:32) with 1 output partitions
2018-02-06 13:02:35,746 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (show at CreateDataSetFromDatabase.java:32)
2018-02-06 13:02:35,746 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 13:02:35,746 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 13:02:35,747 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[8] at show at CreateDataSetFromDatabase.java:32), which has no missing parents
2018-02-06 13:02:35,757 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 17.9 KB, free 631.8 MB)
2018-02-06 13:02:35,762 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.9 KB, free 631.7 MB)
2018-02-06 13:02:35,765 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:55157 (size: 6.9 KB, free: 631.8 MB)
2018-02-06 13:02:35,766 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-06 13:02:35,767 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at show at CreateDataSetFromDatabase.java:32) (first 15 tasks are for partitions Vector(0))
2018-02-06 13:02:35,767 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
2018-02-06 13:02:35,768 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 13:02:35,768 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 2)
2018-02-06 13:02:35,794 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 13:02:35,796 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 2). 5416 bytes result sent to driver
2018-02-06 13:02:35,797 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 2) in 29 ms on localhost (executor driver) (1/1)
2018-02-06 13:02:35,798 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (show at CreateDataSetFromDatabase.java:32) finished in 0.031 s
2018-02-06 13:02:35,798 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-06 13:02:35,799 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at CreateDataSetFromDatabase.java:32, took 0.054129 s
2018-02-06 13:02:35,872 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 57.496978 ms
2018-02-06 13:02:35,979 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:55157 in memory (size: 4.0 KB, free: 631.8 MB)
2018-02-06 13:02:36,006 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_2_piece0 on 192.168.11.26:55157 in memory (size: 6.9 KB, free: 631.8 MB)
2018-02-06 13:02:36,009 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_0_piece0 on 192.168.11.26:55157 in memory (size: 5.5 KB, free: 631.8 MB)
2018-02-06 13:02:36,011 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 85
2018-02-06 13:02:36,012 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 86
2018-02-06 13:02:43,523 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 13:02:43,523 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 13:02:43,528 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2018-02-06 13:02:43,654 INFO[org.apache.spark.SparkContext:54] - Starting job: json at CreateDataSetFromDatabase.java:36
2018-02-06 13:02:43,656 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 2 (json at CreateDataSetFromDatabase.java:36) with 1 output partitions
2018-02-06 13:02:43,656 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 3 (json at CreateDataSetFromDatabase.java:36)
2018-02-06 13:02:43,656 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 13:02:43,660 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 13:02:43,661 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 3 (MapPartitionsRDD[12] at json at CreateDataSetFromDatabase.java:36), which has no missing parents
2018-02-06 13:02:43,695 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 122.6 KB, free 631.7 MB)
2018-02-06 13:02:43,702 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 44.4 KB, free 631.6 MB)
2018-02-06 13:02:43,703 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:55157 (size: 44.4 KB, free: 631.8 MB)
2018-02-06 13:02:43,703 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 13:02:43,704 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[12] at json at CreateDataSetFromDatabase.java:36) (first 15 tasks are for partitions Vector(0))
2018-02-06 13:02:43,704 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 1 tasks
2018-02-06 13:02:43,706 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 13:02:43,706 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 3)
2018-02-06 13:02:43,869 INFO[org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD:54] - closed connection
2018-02-06 13:02:43,877 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block rdd_11_0 stored as values in memory (estimated size 26.2 KB, free 631.6 MB)
2018-02-06 13:02:43,878 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_11_0 in memory on 192.168.11.26:55157 (size: 26.2 KB, free: 631.7 MB)
2018-02-06 13:02:43,889 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 4.613761 ms
2018-02-06 13:02:43,943 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 42.140814 ms
2018-02-06 13:02:43,950 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 13:02:43,950 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 13:02:43,951 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2018-02-06 13:02:44,213 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:582] - Saved output of task 'attempt_20180206130243_0003_m_000000_0' to hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/_temporary/0/task_20180206130243_0003_m_000000
2018-02-06 13:02:44,214 INFO[org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20180206130243_0003_m_000000_0: Committed
2018-02-06 13:02:44,217 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 3). 2394 bytes result sent to driver
2018-02-06 13:02:44,219 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 3) in 514 ms on localhost (executor driver) (1/1)
2018-02-06 13:02:44,219 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2018-02-06 13:02:44,220 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 3 (json at CreateDataSetFromDatabase.java:36) finished in 0.515 s
2018-02-06 13:02:44,221 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 2 finished: json at CreateDataSetFromDatabase.java:36, took 0.566824 s
2018-02-06 13:02:44,272 INFO[org.apache.spark.sql.execution.datasources.FileFormatWriter:54] - Job null committed.
2018-02-06 13:02:44,346 INFO[org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat:54] - Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 13:02:44,406 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 13:02:44,406 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 13:02:44,408 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 13:02:44,408 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 13:02:44,408 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 13:02:44,409 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 13:02:44,442 INFO[org.apache.spark.SparkContext:54] - Starting job: parquet at CreateDataSetFromDatabase.java:37
2018-02-06 13:02:44,444 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 3 (parquet at CreateDataSetFromDatabase.java:37) with 1 output partitions
2018-02-06 13:02:44,445 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 4 (parquet at CreateDataSetFromDatabase.java:37)
2018-02-06 13:02:44,445 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 13:02:44,445 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 13:02:44,446 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 4 (MapPartitionsRDD[15] at parquet at CreateDataSetFromDatabase.java:37), which has no missing parents
2018-02-06 13:02:44,467 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 125.6 KB, free 631.5 MB)
2018-02-06 13:02:44,471 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 44.4 KB, free 631.4 MB)
2018-02-06 13:02:44,475 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.11.26:55157 (size: 44.4 KB, free: 631.7 MB)
2018-02-06 13:02:44,478 INFO[org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2018-02-06 13:02:44,478 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at parquet at CreateDataSetFromDatabase.java:37) (first 15 tasks are for partitions Vector(0))
2018-02-06 13:02:44,478 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 4.0 with 1 tasks
2018-02-06 13:02:44,481 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 13:02:44,482 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 4.0 (TID 4)
2018-02-06 13:02:44,497 INFO[org.apache.spark.storage.BlockManager:54] - Found block rdd_11_0 locally
2018-02-06 13:02:44,502 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 13:02:44,502 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 13:02:44,503 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 13:02:44,503 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 13:02:44,503 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 13:02:44,503 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-02-06 13:02:44,506 INFO[org.apache.parquet.hadoop.codec.CodecConfig:95] - Compression: SNAPPY
2018-02-06 13:02:44,507 INFO[org.apache.parquet.hadoop.codec.CodecConfig:95] - Compression: SNAPPY
2018-02-06 13:02:44,532 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:329] - Parquet block size to 134217728
2018-02-06 13:02:44,532 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:330] - Parquet page size to 1048576
2018-02-06 13:02:44,532 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:331] - Parquet dictionary page size to 1048576
2018-02-06 13:02:44,532 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:332] - Dictionary is on
2018-02-06 13:02:44,532 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:333] - Validation is off
2018-02-06 13:02:44,532 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:334] - Writer version is: PARQUET_1_0
2018-02-06 13:02:44,532 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:335] - Maximum row group padding size is 0 bytes
2018-02-06 13:02:44,533 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:336] - Page size checking is: estimated
2018-02-06 13:02:44,533 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:337] - Min row count for page size check is: 100
2018-02-06 13:02:44,533 INFO[org.apache.parquet.hadoop.ParquetOutputFormat:338] - Max row count for page size check is: 10000
2018-02-06 13:02:44,568 INFO[org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport:54] - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "bill_id",
    "type" : "integer",
    "nullable" : false,
    "metadata" : {
      "name" : "bill_id",
      "scale" : 0
    }
  }, {
    "name" : "bill_status",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_status",
      "scale" : 0
    }
  }, {
    "name" : "bill_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_type",
      "scale" : 0
    }
  }, {
    "name" : "create_date",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : {
      "name" : "create_date",
      "scale" : 0
    }
  }, {
    "name" : "bill_category_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_category_id",
      "scale" : 0
    }
  }, {
    "name" : "bill_category_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_category_name",
      "scale" : 0
    }
  }, {
    "name" : "bill_trade_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_trade_type",
      "scale" : 0
    }
  }, {
    "name" : "bill_trade_area",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_trade_area",
      "scale" : 0
    }
  }, {
    "name" : "bill_accept_org",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_accept_org",
      "scale" : 0
    }
  }, {
    "name" : "bill_user_name",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_user_name",
      "scale" : 0
    }
  }, {
    "name" : "bill_feature",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_feature",
      "scale" : 0
    }
  }, {
    "name" : "bill_no",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_no",
      "scale" : 0
    }
  }, {
    "name" : "bill_classify",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_classify",
      "scale" : 0
    }
  }, {
    "name" : "bill_sstatus",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_sstatus",
      "scale" : 0
    }
  }, {
    "name" : "bill_company",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_company",
      "scale" : 0
    }
  }, {
    "name" : "bill_bank",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_bank",
      "scale" : 0
    }
  }, {
    "name" : "bill_money",
    "type" : "double",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_money",
      "scale" : 2
    }
  }, {
    "name" : "bill_expire",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_expire",
      "scale" : 0
    }
  }, {
    "name" : "bill_endorse",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_endorse",
      "scale" : 0
    }
  }, {
    "name" : "bill_position",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_position",
      "scale" : 0
    }
  }, {
    "name" : "bill_img",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_img",
      "scale" : 0
    }
  }, {
    "name" : "bill_endorse_img",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_endorse_img",
      "scale" : 0
    }
  }, {
    "name" : "bill_evidence",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_evidence",
      "scale" : 0
    }
  }, {
    "name" : "bill_bg_img",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_bg_img",
      "scale" : 0
    }
  }, {
    "name" : "bill_bg_img2",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_bg_img2",
      "scale" : 0
    }
  }, {
    "name" : "bill_img_health",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_img_health",
      "scale" : 0
    }
  }, {
    "name" : "bill_rate",
    "type" : "double",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_rate",
      "scale" : 2
    }
  }, {
    "name" : "bill_increment",
    "type" : "double",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_increment",
      "scale" : 2
    }
  }, {
    "name" : "bill_discount_amount",
    "type" : "double",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_discount_amount",
      "scale" : 2
    }
  }, {
    "name" : "bill_desc",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_desc",
      "scale" : 0
    }
  }, {
    "name" : "bill_quote_count",
    "type" : "integer",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_quote_count",
      "scale" : 0
    }
  }, {
    "name" : "user_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : {
      "name" : "user_id",
      "scale" : 0
    }
  }, {
    "name" : "bill_visible",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_visible",
      "scale" : 0
    }
  }, {
    "name" : "trade_date",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : {
      "name" : "trade_date",
      "scale" : 0
    }
  }, {
    "name" : "trade_desc",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "trade_desc",
      "scale" : 0
    }
  }, {
    "name" : "bill_return",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_return",
      "scale" : 0
    }
  }, {
    "name" : "bill_quote_type",
    "type" : "string",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_quote_type",
      "scale" : 0
    }
  }, {
    "name" : "bill_fixed_price",
    "type" : "double",
    "nullable" : true,
    "metadata" : {
      "name" : "bill_fixed_price",
      "scale" : 2
    }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 bill_id;
  optional binary bill_status (UTF8);
  optional binary bill_type (UTF8);
  optional int96 create_date;
  optional int32 bill_category_id;
  optional binary bill_category_name (UTF8);
  optional binary bill_trade_type (UTF8);
  optional binary bill_trade_area (UTF8);
  optional binary bill_accept_org (UTF8);
  optional binary bill_user_name (UTF8);
  optional binary bill_feature (UTF8);
  optional binary bill_no (UTF8);
  optional binary bill_classify (UTF8);
  optional binary bill_sstatus (UTF8);
  optional binary bill_company (UTF8);
  optional binary bill_bank (UTF8);
  optional double bill_money;
  optional binary bill_expire (UTF8);
  optional binary bill_endorse (UTF8);
  optional binary bill_position (UTF8);
  optional binary bill_img (UTF8);
  optional binary bill_endorse_img (UTF8);
  optional binary bill_evidence (UTF8);
  optional binary bill_bg_img (UTF8);
  optional binary bill_bg_img2 (UTF8);
  optional binary bill_img_health (UTF8);
  optional double bill_rate;
  optional double bill_increment;
  optional double bill_discount_amount;
  optional binary bill_desc (UTF8);
  optional int32 bill_quote_count;
  optional int32 user_id;
  optional binary bill_visible (UTF8);
  optional int96 trade_date;
  optional binary trade_desc (UTF8);
  optional binary bill_return (UTF8);
  optional binary bill_quote_type (UTF8);
  optional double bill_fixed_price;
}

       
2018-02-06 13:02:44,602 INFO[org.apache.hadoop.io.compress.CodecPool:153] - Got brand-new compressor [.snappy]
2018-02-06 13:02:44,768 INFO[org.apache.parquet.hadoop.InternalParquetRecordWriter:160] - Flushing mem columnStore to file. allocated memory: 20148
2018-02-06 13:02:44,919 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:582] - Saved output of task 'attempt_20180206130244_0004_m_000000_0' to hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.parquet/_temporary/0/task_20180206130244_0004_m_000000
2018-02-06 13:02:44,919 INFO[org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20180206130244_0004_m_000000_0: Committed
2018-02-06 13:02:44,920 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 4.0 (TID 4). 1670 bytes result sent to driver
2018-02-06 13:02:44,921 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 4.0 (TID 4) in 441 ms on localhost (executor driver) (1/1)
2018-02-06 13:02:44,922 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2018-02-06 13:02:44,922 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 4 (parquet at CreateDataSetFromDatabase.java:37) finished in 0.442 s
2018-02-06 13:02:44,923 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 3 finished: parquet at CreateDataSetFromDatabase.java:37, took 0.478932 s
2018-02-06 13:02:44,963 INFO[org.apache.spark.sql.execution.datasources.FileFormatWriter:54] - Job null committed.
2018-02-06 13:02:45,038 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 13:02:45,038 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 13:02:45,038 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2018-02-06 13:02:45,078 INFO[org.apache.spark.SparkContext:54] - Starting job: csv at CreateDataSetFromDatabase.java:38
2018-02-06 13:02:45,080 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 4 (csv at CreateDataSetFromDatabase.java:38) with 1 output partitions
2018-02-06 13:02:45,080 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 5 (csv at CreateDataSetFromDatabase.java:38)
2018-02-06 13:02:45,080 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 13:02:45,081 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 13:02:45,081 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 5 (MapPartitionsRDD[18] at csv at CreateDataSetFromDatabase.java:38), which has no missing parents
2018-02-06 13:02:45,097 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5 stored as values in memory (estimated size 122.9 KB, free 631.3 MB)
2018-02-06 13:02:45,102 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 44.5 KB, free 631.3 MB)
2018-02-06 13:02:45,104 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_5_piece0 in memory on 192.168.11.26:55157 (size: 44.5 KB, free: 631.6 MB)
2018-02-06 13:02:45,105 INFO[org.apache.spark.SparkContext:54] - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
2018-02-06 13:02:45,106 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[18] at csv at CreateDataSetFromDatabase.java:38) (first 15 tasks are for partitions Vector(0))
2018-02-06 13:02:45,106 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 5.0 with 1 tasks
2018-02-06 13:02:45,106 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 4649 bytes)
2018-02-06 13:02:45,107 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 5.0 (TID 5)
2018-02-06 13:02:45,117 INFO[org.apache.spark.storage.BlockManager:54] - Found block rdd_11_0 locally
2018-02-06 13:02:45,120 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:123] - File Output Committer Algorithm version is 1
2018-02-06 13:02:45,121 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:138] - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
2018-02-06 13:02:45,121 INFO[org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol:54] - Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2018-02-06 13:02:45,202 INFO[org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter:582] - Saved output of task 'attempt_20180206130245_0005_m_000000_0' to hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.csv/_temporary/0/task_20180206130245_0005_m_000000
2018-02-06 13:02:45,202 INFO[org.apache.spark.mapred.SparkHadoopMapRedUtil:54] - attempt_20180206130245_0005_m_000000_0: Committed
2018-02-06 13:02:45,204 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 5.0 (TID 5). 1670 bytes result sent to driver
2018-02-06 13:02:45,205 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 5.0 (TID 5) in 99 ms on localhost (executor driver) (1/1)
2018-02-06 13:02:45,205 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 5.0, whose tasks have all completed, from pool 
2018-02-06 13:02:45,205 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 5 (csv at CreateDataSetFromDatabase.java:38) finished in 0.099 s
2018-02-06 13:02:45,207 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 4 finished: csv at CreateDataSetFromDatabase.java:38, took 0.127647 s
2018-02-06 13:02:45,255 INFO[org.apache.spark.sql.execution.datasources.FileFormatWriter:54] - Job null committed.
2018-02-06 13:02:45,337 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 13:02:45,347 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@1a7a93e3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 13:02:45,351 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 13:02:45,388 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 13:02:45,415 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 13:02:45,416 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 13:02:45,416 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 13:02:45,419 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 13:02:45,422 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 13:02:45,422 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 13:02:45,423 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-40fcf958-65c5-44f5-bc39-25e45c1ac29e
2018-02-06 13:03:34,749 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 13:03:35,484 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 13:03:35,517 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 13:03:35,518 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 13:03:35,519 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 13:03:35,520 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 13:03:35,521 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 13:03:35,901 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 55223.
2018-02-06 13:03:35,921 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 13:03:35,969 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 13:03:35,972 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 13:03:35,973 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 13:03:35,981 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-14ac8f6a-3c7d-4e2c-9866-e053c1a4f5fc
2018-02-06 13:03:36,006 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 13:03:36,054 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 13:03:36,141 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2811ms
2018-02-06 13:03:36,215 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 13:03:36,230 INFO[org.spark_project.jetty.server.Server:403] - Started @2901ms
2018-02-06 13:03:36,251 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@2e3708df{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 13:03:36,251 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 13:03:36,276 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,277 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,278 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,279 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,280 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/stages,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,281 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,282 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,283 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ce3db41{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,283 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,284 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,285 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/storage,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,285 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,286 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,286 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,287 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/environment,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,287 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,289 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/executors,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,289 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,290 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,291 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,300 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/static,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,301 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,302 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/api,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,302 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,303 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,307 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 13:03:36,391 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 13:03:36,420 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55236.
2018-02-06 13:03:36,421 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:55236
2018-02-06 13:03:36,426 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 13:03:36,429 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 55236, None)
2018-02-06 13:03:36,432 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:55236 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 55236, None)
2018-02-06 13:03:36,439 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 55236, None)
2018-02-06 13:03:36,440 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 55236, None)
2018-02-06 13:03:36,681 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@420bc288{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,767 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 13:03:36,768 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 13:03:36,774 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@742d4e15{/SQL,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,775 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50b1f030{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,777 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@23aae55{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,779 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@680bddf5{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 13:03:36,782 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53bf7094{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 13:03:37,854 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 13:03:47,259 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 13:03:47,262 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 13:03:47,265 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<value: string>
2018-02-06 13:03:47,273 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 13:03:47,762 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 191.138301 ms
2018-02-06 13:03:47,813 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 322.5 KB, free 631.5 MB)
2018-02-06 13:03:47,873 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.5 MB)
2018-02-06 13:03:47,876 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:55236 (size: 27.5 KB, free: 631.8 MB)
2018-02-06 13:03:47,882 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from json at CreateDataSetFromJSON.java:42
2018-02-06 13:03:47,896 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 13:03:48,078 INFO[org.apache.spark.SparkContext:54] - Starting job: json at CreateDataSetFromJSON.java:42
2018-02-06 13:03:48,095 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at CreateDataSetFromJSON.java:42) with 1 output partitions
2018-02-06 13:03:48,095 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at CreateDataSetFromJSON.java:42)
2018-02-06 13:03:48,096 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 13:03:48,097 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 13:03:48,101 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at CreateDataSetFromJSON.java:42), which has no missing parents
2018-02-06 13:03:48,119 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.5 KB, free 631.4 MB)
2018-02-06 13:03:48,122 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KB, free 631.4 MB)
2018-02-06 13:03:48,123 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:55236 (size: 5.3 KB, free: 631.8 MB)
2018-02-06 13:03:48,123 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 13:03:48,135 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at CreateDataSetFromJSON.java:42) (first 15 tasks are for partitions Vector(0))
2018-02-06 13:03:48,137 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 13:03:48,183 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 13:03:48,191 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 13:03:48,259 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 13:03:48,284 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 11.843204 ms
2018-02-06 13:03:48,532 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 2776 bytes result sent to driver
2018-02-06 13:03:48,540 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 370 ms on localhost (executor driver) (1/1)
2018-02-06 13:03:48,542 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 13:03:48,546 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at CreateDataSetFromJSON.java:42) finished in 0.389 s
2018-02-06 13:03:48,554 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at CreateDataSetFromJSON.java:42, took 0.474725 s
2018-02-06 13:03:48,646 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 13:03:48,647 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 13:03:48,647 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<bill_accept_org: string, bill_category_name: string, bill_classify: string, bill_desc: string, bill_discount_amount: double ... 22 more fields>
2018-02-06 13:03:48,648 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 13:03:48,694 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 631.1 MB)
2018-02-06 13:03:48,713 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.1 MB)
2018-02-06 13:03:48,715 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:55236 (size: 27.5 KB, free: 631.7 MB)
2018-02-06 13:03:48,716 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from show at CreateDataSetFromJSON.java:44
2018-02-06 13:03:48,725 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 13:03:48,758 INFO[org.apache.spark.SparkContext:54] - Starting job: show at CreateDataSetFromJSON.java:44
2018-02-06 13:03:48,759 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at CreateDataSetFromJSON.java:44) with 1 output partitions
2018-02-06 13:03:48,759 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at CreateDataSetFromJSON.java:44)
2018-02-06 13:03:48,759 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 13:03:48,759 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 13:03:48,760 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at CreateDataSetFromJSON.java:44), which has no missing parents
2018-02-06 13:03:48,770 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 631.1 MB)
2018-02-06 13:03:48,773 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KB, free 631.1 MB)
2018-02-06 13:03:48,773 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:55236 (size: 5.8 KB, free: 631.7 MB)
2018-02-06 13:03:48,774 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 13:03:48,774 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at CreateDataSetFromJSON.java:44) (first 15 tasks are for partitions Vector(0))
2018-02-06 13:03:48,774 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 13:03:48,775 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 13:03:48,776 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 13:03:48,785 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 13:03:48,824 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 32.435531 ms
2018-02-06 13:03:48,869 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 5343 bytes result sent to driver
2018-02-06 13:03:48,871 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 96 ms on localhost (executor driver) (1/1)
2018-02-06 13:03:48,871 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 13:03:48,872 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at CreateDataSetFromJSON.java:44) finished in 0.096 s
2018-02-06 13:03:48,872 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at CreateDataSetFromJSON.java:44, took 0.114253 s
2018-02-06 13:03:48,920 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 34.489611 ms
2018-02-06 13:03:48,946 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 13:03:48,951 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@2e3708df{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 13:03:48,952 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 13:03:48,961 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 13:03:48,978 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 13:03:48,979 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 13:03:48,986 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 13:03:48,988 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 13:03:48,991 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 13:03:48,991 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 13:03:48,992 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-8f3398e6-9bb1-454d-bb9a-169d644d4cb8
2018-02-06 13:04:22,634 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 13:04:23,289 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 13:04:23,315 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 13:04:23,316 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 13:04:23,317 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 13:04:23,318 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 13:04:23,318 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 13:04:23,691 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 55284.
2018-02-06 13:04:23,710 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 13:04:23,757 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 13:04:23,760 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 13:04:23,761 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 13:04:23,771 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-9c81b9b5-8298-4a1c-81a9-19e1d3036976
2018-02-06 13:04:23,793 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 13:04:23,847 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 13:04:23,931 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2640ms
2018-02-06 13:04:24,006 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 13:04:24,021 INFO[org.spark_project.jetty.server.Server:403] - Started @2732ms
2018-02-06 13:04:24,044 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@28114b1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 13:04:24,044 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 13:04:24,068 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,068 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,069 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,070 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,071 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/stages,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,072 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,073 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,074 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ce3db41{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,075 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,075 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,076 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/storage,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,078 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,079 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,080 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,081 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/environment,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,082 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,083 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/executors,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,084 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,084 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,085 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,091 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/static,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,092 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,093 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39dcf4b0{/api,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,093 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,094 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,096 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 13:04:24,179 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 13:04:24,206 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55297.
2018-02-06 13:04:24,208 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:55297
2018-02-06 13:04:24,211 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 13:04:24,215 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 55297, None)
2018-02-06 13:04:24,222 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:55297 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 55297, None)
2018-02-06 13:04:24,225 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 55297, None)
2018-02-06 13:04:24,226 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 55297, None)
2018-02-06 13:04:24,450 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@420bc288{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,547 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 13:04:24,547 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 13:04:24,556 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fa05212{/SQL,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,557 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c09d180{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,558 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2d83c5a5{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,559 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7e053511{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 13:04:24,562 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28b576a9{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 13:04:25,658 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 13:04:33,582 INFO[org.apache.spark.SparkContext:54] - Starting job: parquet at CreateDataSetFromParquet.java:22
2018-02-06 13:04:33,599 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (parquet at CreateDataSetFromParquet.java:22) with 1 output partitions
2018-02-06 13:04:33,600 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (parquet at CreateDataSetFromParquet.java:22)
2018-02-06 13:04:33,600 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 13:04:33,601 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 13:04:33,607 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at CreateDataSetFromParquet.java:22), which has no missing parents
2018-02-06 13:04:33,671 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 80.5 KB, free 631.7 MB)
2018-02-06 13:04:33,706 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 28.9 KB, free 631.7 MB)
2018-02-06 13:04:33,708 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:55297 (size: 28.9 KB, free: 631.8 MB)
2018-02-06 13:04:33,710 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-06 13:04:33,727 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at CreateDataSetFromParquet.java:22) (first 15 tasks are for partitions Vector(0))
2018-02-06 13:04:33,728 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 13:04:33,770 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5057 bytes)
2018-02-06 13:04:33,780 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 13:04:34,173 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 4616 bytes result sent to driver
2018-02-06 13:04:34,180 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 427 ms on localhost (executor driver) (1/1)
2018-02-06 13:04:34,183 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 13:04:34,186 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (parquet at CreateDataSetFromParquet.java:22) finished in 0.444 s
2018-02-06 13:04:34,190 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: parquet at CreateDataSetFromParquet.java:22, took 0.607361 s
2018-02-06 13:04:34,656 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_0_piece0 on 192.168.11.26:55297 in memory (size: 28.9 KB, free: 631.8 MB)
2018-02-06 13:04:35,738 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 13:04:35,748 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 13:04:35,751 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<bill_id: int, bill_status: string, bill_type: string, create_date: timestamp, bill_category_id: int ... 36 more fields>
2018-02-06 13:04:35,765 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 13:04:35,819 WARN[org.apache.spark.util.Utils:66] - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
2018-02-06 13:04:36,239 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 280.63337 ms
2018-02-06 13:04:36,289 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 339.3 KB, free 631.5 MB)
2018-02-06 13:04:36,303 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 29.5 KB, free 631.4 MB)
2018-02-06 13:04:36,304 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:55297 (size: 29.5 KB, free: 631.8 MB)
2018-02-06 13:04:36,305 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from show at CreateDataSetFromParquet.java:25
2018-02-06 13:04:36,324 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 13:04:36,378 INFO[org.apache.spark.SparkContext:54] - Starting job: show at CreateDataSetFromParquet.java:25
2018-02-06 13:04:36,381 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at CreateDataSetFromParquet.java:25) with 1 output partitions
2018-02-06 13:04:36,381 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at CreateDataSetFromParquet.java:25)
2018-02-06 13:04:36,381 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 13:04:36,381 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 13:04:36,382 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[4] at show at CreateDataSetFromParquet.java:25), which has no missing parents
2018-02-06 13:04:36,400 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 26.5 KB, free 631.4 MB)
2018-02-06 13:04:36,405 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.1 KB, free 631.4 MB)
2018-02-06 13:04:36,407 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:55297 (size: 8.1 KB, free: 631.8 MB)
2018-02-06 13:04:36,407 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-06 13:04:36,409 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at show at CreateDataSetFromParquet.java:25) (first 15 tasks are for partitions Vector(0))
2018-02-06 13:04:36,409 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 13:04:36,416 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5368 bytes)
2018-02-06 13:04:36,416 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 13:04:36,432 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.parquet/part-00000-d8f6b117-fdb7-4a3b-bf95-0d970b13ba80-c000.snappy.parquet, range: 0-16202, partition values: [empty row]
2018-02-06 13:04:36,564 INFO[org.apache.hadoop.io.compress.CodecPool:181] - Got brand-new decompressor [.snappy]
2018-02-06 13:04:36,701 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 5550 bytes result sent to driver
2018-02-06 13:04:36,702 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 289 ms on localhost (executor driver) (1/1)
2018-02-06 13:04:36,702 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 13:04:36,703 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at CreateDataSetFromParquet.java:25) finished in 0.289 s
2018-02-06 13:04:36,703 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at CreateDataSetFromParquet.java:25, took 0.324221 s
2018-02-06 13:04:36,787 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 68.168342 ms
2018-02-06 13:04:36,821 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 13:04:36,828 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@28114b1{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 13:04:36,830 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 13:04:36,838 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 13:04:36,857 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 13:04:36,858 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 13:04:36,859 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 13:04:36,862 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 13:04:36,866 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 13:04:36,866 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 13:04:36,867 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-7079dc2d-603a-4473-8d77-9b37883e2933
2018-02-06 14:50:52,137 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 14:50:52,783 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 14:50:52,825 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 14:50:52,826 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 14:50:52,827 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 14:50:52,827 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 14:50:52,828 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 14:50:53,293 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 59173.
2018-02-06 14:50:53,319 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 14:50:53,389 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 14:50:53,393 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 14:50:53,394 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 14:50:53,406 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-8caf4979-bd52-4596-a2b2-9ac131c06ef5
2018-02-06 14:50:53,440 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 14:50:53,502 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 14:50:53,617 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3055ms
2018-02-06 14:50:53,722 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 14:50:53,740 INFO[org.spark_project.jetty.server.Server:403] - Started @3180ms
2018-02-06 14:50:53,766 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@33e01298{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 14:50:53,767 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 14:50:53,800 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@773f7880{/jobs,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,802 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bc715b8{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,802 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,803 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,804 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/stages,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,805 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,805 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,806 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@38f57b3d{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,807 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ce3db41{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,807 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,808 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/storage,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,808 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,809 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,811 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,812 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/environment,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,813 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,813 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/executors,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,814 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,814 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,815 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,825 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/static,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,826 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58670130{/,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,827 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/api,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,828 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18518ccf{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,828 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 14:50:53,831 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 14:50:53,939 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 14:50:53,972 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59186.
2018-02-06 14:50:53,974 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:59186
2018-02-06 14:50:53,976 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 14:50:53,981 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 59186, None)
2018-02-06 14:50:53,989 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:59186 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 59186, None)
2018-02-06 14:50:53,992 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 59186, None)
2018-02-06 14:50:53,994 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 59186, None)
2018-02-06 14:50:54,227 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@467f77a5{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 14:50:54,318 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 14:50:54,320 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 14:50:54,328 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c0f7678{/SQL,null,AVAILABLE,@Spark}
2018-02-06 14:50:54,329 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6aa648b9{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 14:50:54,330 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4163f1cd{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 14:50:54,331 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e681bc{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 14:50:54,334 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2d83c5a5{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 14:50:55,607 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 14:50:57,408 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 206.956866 ms
2018-02-06 14:50:58,104 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 9.568643 ms
2018-02-06 14:50:58,306 INFO[org.apache.spark.SparkContext:54] - Starting job: json at BaseSparkSQLTest.java:29
2018-02-06 14:50:58,330 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at BaseSparkSQLTest.java:29) with 1 output partitions
2018-02-06 14:50:58,331 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at BaseSparkSQLTest.java:29)
2018-02-06 14:50:58,331 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 14:50:58,333 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 14:50:58,341 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:29), which has no missing parents
2018-02-06 14:50:58,614 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 5.8 KB, free 631.8 MB)
2018-02-06 14:50:58,658 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.6 KB, free 631.8 MB)
2018-02-06 14:50:58,662 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:59186 (size: 3.6 KB, free: 631.8 MB)
2018-02-06 14:50:58,666 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-06 14:50:58,679 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:29) (first 15 tasks are for partitions Vector(0))
2018-02-06 14:50:58,680 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 14:50:58,732 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5067 bytes)
2018-02-06 14:50:58,743 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 14:50:58,860 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1670 bytes result sent to driver
2018-02-06 14:50:58,871 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 157 ms on localhost (executor driver) (1/1)
2018-02-06 14:50:58,874 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 14:50:58,885 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at BaseSparkSQLTest.java:29) finished in 0.183 s
2018-02-06 14:50:58,893 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at BaseSparkSQLTest.java:29, took 0.587296 s
2018-02-06 14:50:59,038 INFO[org.apache.spark.SparkContext:54] - Starting job: show at BaseSparkSQL.java:22
2018-02-06 14:50:59,039 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at BaseSparkSQL.java:22) with 1 output partitions
2018-02-06 14:50:59,040 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at BaseSparkSQL.java:22)
2018-02-06 14:50:59,040 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 14:50:59,040 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 14:50:59,040 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[10] at show at BaseSparkSQL.java:22), which has no missing parents
2018-02-06 14:50:59,061 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.5 KB, free 631.8 MB)
2018-02-06 14:50:59,064 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.4 KB, free 631.8 MB)
2018-02-06 14:50:59,065 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:59186 (size: 5.4 KB, free: 631.8 MB)
2018-02-06 14:50:59,065 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 14:50:59,068 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at show at BaseSparkSQL.java:22) (first 15 tasks are for partitions Vector(0))
2018-02-06 14:50:59,068 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 14:50:59,072 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5067 bytes)
2018-02-06 14:50:59,073 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 14:50:59,098 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 8.911363 ms
2018-02-06 14:50:59,125 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1098 bytes result sent to driver
2018-02-06 14:50:59,127 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 58 ms on localhost (executor driver) (1/1)
2018-02-06 14:50:59,127 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 14:50:59,127 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at BaseSparkSQL.java:22) finished in 0.058 s
2018-02-06 14:50:59,128 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at BaseSparkSQL.java:22, took 0.089222 s
2018-02-06 14:50:59,156 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 14.329285 ms
2018-02-06 14:50:59,182 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 14:50:59,188 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@33e01298{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 14:50:59,190 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 14:50:59,199 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 14:50:59,211 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 14:50:59,212 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 14:50:59,218 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 14:50:59,220 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 14:50:59,222 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 14:50:59,223 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 14:50:59,224 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-9bbd073c-ab78-4a0a-a2e2-bcfe57d5a770
2018-02-06 14:51:51,916 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 14:51:52,630 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 14:51:52,659 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 14:51:52,660 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 14:51:52,661 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 14:51:52,662 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 14:51:52,663 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 14:51:53,039 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 59235.
2018-02-06 14:51:53,059 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 14:51:53,111 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 14:51:53,114 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 14:51:53,115 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 14:51:53,123 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-47a8e67e-54bf-4ffc-aa0e-324abf45ea56
2018-02-06 14:51:53,147 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 14:51:53,198 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 14:51:53,276 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2772ms
2018-02-06 14:51:53,353 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 14:51:53,365 INFO[org.spark_project.jetty.server.Server:403] - Started @2862ms
2018-02-06 14:51:53,388 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@1c115660{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 14:51:53,388 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 14:51:53,411 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,412 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,413 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,414 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,414 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/stages,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,418 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,419 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51850751{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,420 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64df9a61{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,420 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e260766{/storage,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,421 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a97744{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,422 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c56e013{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,423 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40258c2f{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,424 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6731787b{/environment,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,424 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,425 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/executors,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,426 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,426 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,427 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,432 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/static,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,432 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4bf3798b{/,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74e47444{/api,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,434 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46f699d5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,435 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1991f767{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,437 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 14:51:53,530 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 14:51:53,556 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59248.
2018-02-06 14:51:53,557 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:59248
2018-02-06 14:51:53,559 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 14:51:53,561 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 59248, None)
2018-02-06 14:51:53,566 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:59248 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 59248, None)
2018-02-06 14:51:53,571 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 59248, None)
2018-02-06 14:51:53,572 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 59248, None)
2018-02-06 14:51:53,791 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4201a617{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,860 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 14:51:53,861 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 14:51:53,869 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18ca3c62{/SQL,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,870 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44d70181{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,871 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50b1f030{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,871 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fa05212{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 14:51:53,873 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a9c84a5{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 14:51:55,024 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 14:52:04,538 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 14:52:04,542 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 14:52:04,546 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<value: string>
2018-02-06 14:52:04,571 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 14:52:05,062 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 185.762619 ms
2018-02-06 14:52:05,116 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 322.5 KB, free 631.5 MB)
2018-02-06 14:52:05,178 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.5 MB)
2018-02-06 14:52:05,181 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:59248 (size: 27.5 KB, free: 631.8 MB)
2018-02-06 14:52:05,188 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from json at BaseSparkSQLTest.java:23
2018-02-06 14:52:05,200 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 14:52:05,400 INFO[org.apache.spark.SparkContext:54] - Starting job: json at BaseSparkSQLTest.java:23
2018-02-06 14:52:05,416 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at BaseSparkSQLTest.java:23) with 1 output partitions
2018-02-06 14:52:05,417 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at BaseSparkSQLTest.java:23)
2018-02-06 14:52:05,418 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 14:52:05,419 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 14:52:05,423 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23), which has no missing parents
2018-02-06 14:52:05,441 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.5 KB, free 631.4 MB)
2018-02-06 14:52:05,444 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KB, free 631.4 MB)
2018-02-06 14:52:05,445 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:59248 (size: 5.3 KB, free: 631.8 MB)
2018-02-06 14:52:05,446 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 14:52:05,458 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23) (first 15 tasks are for partitions Vector(0))
2018-02-06 14:52:05,459 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 14:52:05,498 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 14:52:05,506 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 14:52:05,579 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 14:52:05,610 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 18.783046 ms
2018-02-06 14:52:05,884 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 2776 bytes result sent to driver
2018-02-06 14:52:05,894 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 405 ms on localhost (executor driver) (1/1)
2018-02-06 14:52:05,896 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 14:52:05,900 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at BaseSparkSQLTest.java:23) finished in 0.423 s
2018-02-06 14:52:05,906 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at BaseSparkSQLTest.java:23, took 0.505904 s
2018-02-06 14:52:05,998 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 14:52:05,999 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 14:52:05,999 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<bill_accept_org: string, bill_category_name: string, bill_classify: string, bill_desc: string, bill_discount_amount: double ... 22 more fields>
2018-02-06 14:52:06,000 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 14:52:06,039 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 631.1 MB)
2018-02-06 14:52:06,054 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.1 MB)
2018-02-06 14:52:06,056 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:59248 (size: 27.5 KB, free: 631.7 MB)
2018-02-06 14:52:06,057 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from show at BaseSparkSQL.java:22
2018-02-06 14:52:06,062 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 14:52:06,084 INFO[org.apache.spark.SparkContext:54] - Starting job: show at BaseSparkSQL.java:22
2018-02-06 14:52:06,087 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at BaseSparkSQL.java:22) with 1 output partitions
2018-02-06 14:52:06,087 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at BaseSparkSQL.java:22)
2018-02-06 14:52:06,087 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 14:52:06,087 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 14:52:06,090 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at BaseSparkSQL.java:22), which has no missing parents
2018-02-06 14:52:06,096 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 631.1 MB)
2018-02-06 14:52:06,101 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KB, free 631.1 MB)
2018-02-06 14:52:06,105 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:59248 (size: 5.8 KB, free: 631.7 MB)
2018-02-06 14:52:06,106 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 14:52:06,107 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at BaseSparkSQL.java:22) (first 15 tasks are for partitions Vector(0))
2018-02-06 14:52:06,108 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 14:52:06,109 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 14:52:06,110 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 14:52:06,122 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 14:52:06,165 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 35.754891 ms
2018-02-06 14:52:06,217 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 5343 bytes result sent to driver
2018-02-06 14:52:06,219 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 110 ms on localhost (executor driver) (1/1)
2018-02-06 14:52:06,220 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 14:52:06,220 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at BaseSparkSQL.java:22) finished in 0.112 s
2018-02-06 14:52:06,221 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at BaseSparkSQL.java:22, took 0.136368 s
2018-02-06 14:52:06,274 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 37.112332 ms
2018-02-06 14:52:06,300 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 14:52:06,306 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@1c115660{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 14:52:06,307 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 14:52:06,316 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 14:52:06,338 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 14:52:06,339 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 14:52:06,347 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 14:52:06,349 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 14:52:06,354 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 14:52:06,354 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 14:52:06,355 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-5544b111-9352-4f8f-a849-8634459b822c
2018-02-06 14:53:51,999 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 14:53:52,572 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 14:53:52,598 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 14:53:52,599 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 14:53:52,599 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 14:53:52,600 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 14:53:52,601 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 14:53:53,007 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 59427.
2018-02-06 14:53:53,027 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 14:53:53,081 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 14:53:53,084 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 14:53:53,085 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 14:53:53,095 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-caac8def-d2e0-4b2f-b85e-bfec5ac1b5e1
2018-02-06 14:53:53,118 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 14:53:53,174 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 14:53:53,260 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2761ms
2018-02-06 14:53:53,335 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 14:53:53,349 INFO[org.spark_project.jetty.server.Server:403] - Started @2852ms
2018-02-06 14:53:53,373 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@3bc53e30{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 14:53:53,374 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 14:53:53,402 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,402 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,403 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,404 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,406 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/stages,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,407 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,409 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,410 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,411 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51850751{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,412 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64df9a61{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,413 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e260766{/storage,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,414 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a97744{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,414 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c56e013{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,415 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40258c2f{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6731787b{/environment,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,417 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,417 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/executors,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,419 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,420 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,422 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,429 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/static,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,430 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4bf3798b{/,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,431 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74e47444{/api,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,432 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46f699d5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1991f767{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,435 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 14:53:53,536 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 14:53:53,567 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59441.
2018-02-06 14:53:53,568 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:59441
2018-02-06 14:53:53,577 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 14:53:53,583 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 59441, None)
2018-02-06 14:53:53,587 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:59441 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 59441, None)
2018-02-06 14:53:53,595 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 59441, None)
2018-02-06 14:53:53,596 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 59441, None)
2018-02-06 14:53:53,816 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4201a617{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,921 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 14:53:53,923 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 14:53:53,932 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@23c650a3{/SQL,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,932 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@88a8218{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,933 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c09d180{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,934 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5f574cc2{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 14:53:53,936 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@60222fd8{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 14:53:55,033 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 14:54:04,442 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 14:54:04,446 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 14:54:04,449 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<value: string>
2018-02-06 14:54:04,458 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 14:54:05,031 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 199.639104 ms
2018-02-06 14:54:05,091 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 322.5 KB, free 631.5 MB)
2018-02-06 14:54:05,154 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.5 MB)
2018-02-06 14:54:05,158 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:59441 (size: 27.5 KB, free: 631.8 MB)
2018-02-06 14:54:05,163 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from json at BaseSparkSQLTest.java:23
2018-02-06 14:54:05,177 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 14:54:05,428 INFO[org.apache.spark.SparkContext:54] - Starting job: json at BaseSparkSQLTest.java:23
2018-02-06 14:54:05,445 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at BaseSparkSQLTest.java:23) with 1 output partitions
2018-02-06 14:54:05,446 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at BaseSparkSQLTest.java:23)
2018-02-06 14:54:05,446 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 14:54:05,448 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 14:54:05,452 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23), which has no missing parents
2018-02-06 14:54:05,473 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.5 KB, free 631.4 MB)
2018-02-06 14:54:05,476 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KB, free 631.4 MB)
2018-02-06 14:54:05,477 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:59441 (size: 5.3 KB, free: 631.8 MB)
2018-02-06 14:54:05,478 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 14:54:05,492 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23) (first 15 tasks are for partitions Vector(0))
2018-02-06 14:54:05,493 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 14:54:05,534 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 14:54:05,543 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 14:54:05,619 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 14:54:05,653 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 16.627205 ms
2018-02-06 14:54:05,912 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
2018-02-06 14:54:05,919 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 399 ms on localhost (executor driver) (1/1)
2018-02-06 14:54:05,922 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 14:54:05,926 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at BaseSparkSQLTest.java:23) finished in 0.416 s
2018-02-06 14:54:05,932 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at BaseSparkSQLTest.java:23, took 0.503910 s
2018-02-06 14:54:06,095 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:59441 in memory (size: 5.3 KB, free: 631.8 MB)
2018-02-06 14:54:06,096 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 14:54:06,097 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 14:54:06,098 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<>
2018-02-06 14:54:06,098 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 14:54:06,228 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 22.346247 ms
2018-02-06 14:54:06,242 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 9.250563 ms
2018-02-06 14:54:06,248 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 631.1 MB)
2018-02-06 14:54:06,264 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.1 MB)
2018-02-06 14:54:06,266 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:59441 (size: 27.5 KB, free: 631.7 MB)
2018-02-06 14:54:06,267 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from count at BaseSparkSQL.java:26
2018-02-06 14:54:06,273 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 14:54:06,329 INFO[org.apache.spark.SparkContext:54] - Starting job: count at BaseSparkSQL.java:26
2018-02-06 14:54:06,332 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 6 (count at BaseSparkSQL.java:26)
2018-02-06 14:54:06,333 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (count at BaseSparkSQL.java:26) with 1 output partitions
2018-02-06 14:54:06,333 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (count at BaseSparkSQL.java:26)
2018-02-06 14:54:06,333 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-06 14:54:06,334 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-06 14:54:06,334 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at count at BaseSparkSQL.java:26), which has no missing parents
2018-02-06 14:54:06,342 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 11.6 KB, free 631.1 MB)
2018-02-06 14:54:06,345 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KB, free 631.1 MB)
2018-02-06 14:54:06,345 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:59441 (size: 6.3 KB, free: 631.7 MB)
2018-02-06 14:54:06,346 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 14:54:06,349 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at count at BaseSparkSQL.java:26) (first 15 tasks are for partitions Vector(0))
2018-02-06 14:54:06,349 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 14:54:06,351 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5344 bytes)
2018-02-06 14:54:06,351 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 14:54:06,380 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 14:54:06,388 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 6.865922 ms
2018-02-06 14:54:06,461 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1632 bytes result sent to driver
2018-02-06 14:54:06,465 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 116 ms on localhost (executor driver) (1/1)
2018-02-06 14:54:06,466 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 14:54:06,468 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (count at BaseSparkSQL.java:26) finished in 0.119 s
2018-02-06 14:54:06,468 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-06 14:54:06,469 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-06 14:54:06,470 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-06 14:54:06,470 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-06 14:54:06,477 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[9] at count at BaseSparkSQL.java:26), which has no missing parents
2018-02-06 14:54:06,482 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 631.1 MB)
2018-02-06 14:54:06,485 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 631.1 MB)
2018-02-06 14:54:06,486 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.11.26:59441 (size: 3.7 KB, free: 631.7 MB)
2018-02-06 14:54:06,486 INFO[org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2018-02-06 14:54:06,488 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at count at BaseSparkSQL.java:26) (first 15 tasks are for partitions Vector(0))
2018-02-06 14:54:06,488 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
2018-02-06 14:54:06,490 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
2018-02-06 14:54:06,491 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 2)
2018-02-06 14:54:06,509 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 1 blocks
2018-02-06 14:54:06,511 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
2018-02-06 14:54:06,531 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 2). 1538 bytes result sent to driver
2018-02-06 14:54:06,532 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 2) in 43 ms on localhost (executor driver) (1/1)
2018-02-06 14:54:06,533 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-06 14:54:06,533 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (count at BaseSparkSQL.java:26) finished in 0.045 s
2018-02-06 14:54:06,534 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: count at BaseSparkSQL.java:26, took 0.204897 s
2018-02-06 14:54:06,550 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 14:54:06,558 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@3bc53e30{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 14:54:06,562 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 14:54:06,576 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 14:54:20,303 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 14:54:21,010 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 14:54:21,033 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 14:54:21,034 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 14:54:21,034 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 14:54:21,035 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 14:54:21,036 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 14:54:21,427 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 59483.
2018-02-06 14:54:21,447 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 14:54:21,496 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 14:54:21,501 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 14:54:21,501 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 14:54:21,510 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-fa851f63-b6d8-419b-aa61-2d67916b0ada
2018-02-06 14:54:21,536 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 14:54:21,594 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 14:54:21,708 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2827ms
2018-02-06 14:54:21,785 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 14:54:21,801 INFO[org.spark_project.jetty.server.Server:403] - Started @2922ms
2018-02-06 14:54:21,824 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@4789995a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 14:54:21,825 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 14:54:21,848 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,848 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,849 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,850 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,850 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/stages,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,851 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,853 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,854 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,855 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51850751{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,856 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64df9a61{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,857 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e260766{/storage,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,859 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a97744{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,860 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c56e013{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,861 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40258c2f{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,862 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6731787b{/environment,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,862 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,863 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/executors,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,864 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,864 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,865 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,873 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/static,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,875 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4bf3798b{/,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,876 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74e47444{/api,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,877 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46f699d5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,878 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1991f767{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 14:54:21,880 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 14:54:21,977 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 14:54:22,014 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59496.
2018-02-06 14:54:22,015 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:59496
2018-02-06 14:54:22,019 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 14:54:22,025 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 59496, None)
2018-02-06 14:54:22,031 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:59496 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 59496, None)
2018-02-06 14:54:22,037 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 59496, None)
2018-02-06 14:54:22,039 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 59496, None)
2018-02-06 14:54:22,266 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4201a617{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 14:54:22,348 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 14:54:22,348 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 14:54:22,357 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18ca3c62{/SQL,null,AVAILABLE,@Spark}
2018-02-06 14:54:22,358 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44d70181{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 14:54:22,358 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50b1f030{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 14:54:22,359 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fa05212{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 14:54:22,361 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a9c84a5{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 14:54:23,434 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 14:54:32,907 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 14:54:32,913 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 14:54:32,920 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<value: string>
2018-02-06 14:54:32,934 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 14:54:33,847 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 485.960476 ms
2018-02-06 14:54:33,906 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 322.5 KB, free 631.5 MB)
2018-02-06 14:54:33,972 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.5 MB)
2018-02-06 14:54:33,977 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:59496 (size: 27.5 KB, free: 631.8 MB)
2018-02-06 14:54:33,986 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from json at BaseSparkSQLTest.java:23
2018-02-06 14:54:34,006 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 14:54:34,313 INFO[org.apache.spark.SparkContext:54] - Starting job: json at BaseSparkSQLTest.java:23
2018-02-06 14:54:34,335 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at BaseSparkSQLTest.java:23) with 1 output partitions
2018-02-06 14:54:34,336 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at BaseSparkSQLTest.java:23)
2018-02-06 14:54:34,336 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 14:54:34,338 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 14:54:34,346 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23), which has no missing parents
2018-02-06 14:54:34,415 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.5 KB, free 631.4 MB)
2018-02-06 14:54:34,435 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KB, free 631.4 MB)
2018-02-06 14:54:34,438 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:59496 (size: 5.3 KB, free: 631.8 MB)
2018-02-06 14:54:34,439 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 14:54:34,454 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23) (first 15 tasks are for partitions Vector(0))
2018-02-06 14:54:34,456 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 14:54:34,773 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 14:54:34,910 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 14:54:35,092 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 14:54:35,136 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 28.097929 ms
2018-02-06 14:54:35,381 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
2018-02-06 14:54:35,393 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 765 ms on localhost (executor driver) (1/1)
2018-02-06 14:54:35,395 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 14:54:35,408 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at BaseSparkSQLTest.java:23) finished in 0.799 s
2018-02-06 14:54:35,414 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at BaseSparkSQLTest.java:23, took 1.100128 s
2018-02-06 14:54:35,553 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 14:54:35,554 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 14:54:35,554 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<>
2018-02-06 14:54:35,554 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 14:54:35,668 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 26.674569 ms
2018-02-06 14:54:35,688 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 14.216325 ms
2018-02-06 14:54:35,698 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 631.1 MB)
2018-02-06 14:54:35,714 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.1 MB)
2018-02-06 14:54:35,717 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:59496 (size: 27.5 KB, free: 631.7 MB)
2018-02-06 14:54:35,718 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from count at BaseSparkSQL.java:26
2018-02-06 14:54:35,724 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 14:54:35,781 INFO[org.apache.spark.SparkContext:54] - Starting job: count at BaseSparkSQL.java:26
2018-02-06 14:54:35,784 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 6 (count at BaseSparkSQL.java:26)
2018-02-06 14:54:35,785 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (count at BaseSparkSQL.java:26) with 1 output partitions
2018-02-06 14:54:35,785 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (count at BaseSparkSQL.java:26)
2018-02-06 14:54:35,785 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-06 14:54:35,785 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-06 14:54:35,799 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at count at BaseSparkSQL.java:26), which has no missing parents
2018-02-06 14:54:35,810 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 11.6 KB, free 631.1 MB)
2018-02-06 14:54:35,813 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KB, free 631.1 MB)
2018-02-06 14:54:35,814 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:59496 (size: 6.3 KB, free: 631.7 MB)
2018-02-06 14:54:35,815 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 14:54:35,818 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at count at BaseSparkSQL.java:26) (first 15 tasks are for partitions Vector(0))
2018-02-06 14:54:35,818 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 14:54:35,820 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5344 bytes)
2018-02-06 14:54:35,820 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 14:54:35,838 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 14:54:35,847 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 8.080323 ms
2018-02-06 14:54:35,936 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1589 bytes result sent to driver
2018-02-06 14:54:35,939 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 120 ms on localhost (executor driver) (1/1)
2018-02-06 14:54:35,940 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 14:54:35,941 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (count at BaseSparkSQL.java:26) finished in 0.122 s
2018-02-06 14:54:35,942 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-06 14:54:35,944 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-06 14:54:35,944 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-06 14:54:35,945 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-06 14:54:35,950 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[9] at count at BaseSparkSQL.java:26), which has no missing parents
2018-02-06 14:54:35,953 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 631.1 MB)
2018-02-06 14:54:35,958 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 631.1 MB)
2018-02-06 14:54:35,959 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.11.26:59496 (size: 3.7 KB, free: 631.7 MB)
2018-02-06 14:54:35,960 INFO[org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2018-02-06 14:54:35,961 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at count at BaseSparkSQL.java:26) (first 15 tasks are for partitions Vector(0))
2018-02-06 14:54:35,961 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
2018-02-06 14:54:35,963 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
2018-02-06 14:54:35,963 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 2)
2018-02-06 14:54:35,978 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 1 blocks
2018-02-06 14:54:35,981 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 6 ms
2018-02-06 14:54:35,997 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 2). 1581 bytes result sent to driver
2018-02-06 14:54:35,998 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 2) in 36 ms on localhost (executor driver) (1/1)
2018-02-06 14:54:35,999 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-06 14:54:35,999 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (count at BaseSparkSQL.java:26) finished in 0.038 s
2018-02-06 14:54:36,000 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: count at BaseSparkSQL.java:26, took 0.219171 s
2018-02-06 14:54:36,012 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 14:54:36,018 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@4789995a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 14:54:36,020 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 14:54:36,030 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 14:54:36,051 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 14:54:36,052 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 14:54:36,057 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 14:54:36,059 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 14:54:36,064 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 14:54:36,064 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 14:54:36,065 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-1f54a4ba-083d-4558-b738-31df489deb94
2018-02-06 14:55:35,651 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 14:55:36,400 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 14:55:36,426 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 14:55:36,427 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 14:55:36,428 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 14:55:36,429 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 14:55:36,429 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 14:55:36,803 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 59571.
2018-02-06 14:55:36,821 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 14:55:36,871 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 14:55:36,875 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 14:55:36,875 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 14:55:36,883 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-4d494de1-0c7d-4a9d-9e78-b7385bec643b
2018-02-06 14:55:36,906 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 14:55:36,949 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 14:55:37,029 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2762ms
2018-02-06 14:55:37,098 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 14:55:37,112 INFO[org.spark_project.jetty.server.Server:403] - Started @2847ms
2018-02-06 14:55:37,132 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@2b550d79{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 14:55:37,132 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 14:55:37,156 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,157 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,157 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,158 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,159 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/stages,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,159 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,160 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,161 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,162 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51850751{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,162 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64df9a61{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,163 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e260766{/storage,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,163 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a97744{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,164 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c56e013{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,166 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40258c2f{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,167 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6731787b{/environment,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,167 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,168 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/executors,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,169 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,169 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,170 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,176 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/static,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,176 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4bf3798b{/,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,177 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74e47444{/api,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,178 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46f699d5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,179 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1991f767{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,180 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 14:55:37,262 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 14:55:37,297 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59584.
2018-02-06 14:55:37,298 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:59584
2018-02-06 14:55:37,300 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 14:55:37,301 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 59584, None)
2018-02-06 14:55:37,311 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:59584 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 59584, None)
2018-02-06 14:55:37,317 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 59584, None)
2018-02-06 14:55:37,331 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 59584, None)
2018-02-06 14:55:37,601 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4201a617{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,705 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 14:55:37,706 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 14:55:37,715 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@23c650a3{/SQL,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,715 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@88a8218{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,716 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c09d180{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,716 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5f574cc2{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 14:55:37,722 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@60222fd8{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 14:55:38,834 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 14:55:48,612 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 14:55:48,614 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 14:55:48,617 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<value: string>
2018-02-06 14:55:48,627 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 14:55:49,180 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 217.66215 ms
2018-02-06 14:55:49,254 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 322.5 KB, free 631.5 MB)
2018-02-06 14:55:49,315 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.5 MB)
2018-02-06 14:55:49,318 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:59584 (size: 27.5 KB, free: 631.8 MB)
2018-02-06 14:55:49,325 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from json at BaseSparkSQLTest.java:23
2018-02-06 14:55:49,337 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 14:55:49,530 INFO[org.apache.spark.SparkContext:54] - Starting job: json at BaseSparkSQLTest.java:23
2018-02-06 14:55:49,545 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at BaseSparkSQLTest.java:23) with 1 output partitions
2018-02-06 14:55:49,546 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at BaseSparkSQLTest.java:23)
2018-02-06 14:55:49,546 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 14:55:49,547 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 14:55:49,552 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23), which has no missing parents
2018-02-06 14:55:49,571 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.5 KB, free 631.4 MB)
2018-02-06 14:55:49,579 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KB, free 631.4 MB)
2018-02-06 14:55:49,580 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:59584 (size: 5.3 KB, free: 631.8 MB)
2018-02-06 14:55:49,581 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 14:55:49,598 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23) (first 15 tasks are for partitions Vector(0))
2018-02-06 14:55:49,599 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 14:55:49,656 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 14:55:49,666 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 14:55:49,736 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 14:55:49,765 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 14.714565 ms
2018-02-06 14:55:50,011 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
2018-02-06 14:55:50,019 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 381 ms on localhost (executor driver) (1/1)
2018-02-06 14:55:50,021 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 14:55:50,026 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at BaseSparkSQLTest.java:23) finished in 0.402 s
2018-02-06 14:55:50,032 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at BaseSparkSQLTest.java:23, took 0.502174 s
2018-02-06 14:55:50,158 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 14:55:50,159 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 14:55:50,159 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<>
2018-02-06 14:55:50,160 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 14:55:50,263 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 25.211208 ms
2018-02-06 14:55:50,285 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 16.373445 ms
2018-02-06 14:55:50,292 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 631.1 MB)
2018-02-06 14:55:50,308 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.1 MB)
2018-02-06 14:55:50,309 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:59584 (size: 27.5 KB, free: 631.7 MB)
2018-02-06 14:55:50,310 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from count at BaseSparkSQL.java:26
2018-02-06 14:55:50,329 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 14:55:50,380 INFO[org.apache.spark.SparkContext:54] - Starting job: count at BaseSparkSQL.java:26
2018-02-06 14:55:50,382 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 6 (count at BaseSparkSQL.java:26)
2018-02-06 14:55:50,383 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (count at BaseSparkSQL.java:26) with 1 output partitions
2018-02-06 14:55:50,383 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (count at BaseSparkSQL.java:26)
2018-02-06 14:55:50,383 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-06 14:55:50,384 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-06 14:55:50,385 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at count at BaseSparkSQL.java:26), which has no missing parents
2018-02-06 14:55:50,392 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 11.6 KB, free 631.1 MB)
2018-02-06 14:55:50,395 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KB, free 631.1 MB)
2018-02-06 14:55:50,395 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:59584 (size: 6.3 KB, free: 631.7 MB)
2018-02-06 14:55:50,396 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 14:55:50,400 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at count at BaseSparkSQL.java:26) (first 15 tasks are for partitions Vector(0))
2018-02-06 14:55:50,400 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 14:55:50,402 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5344 bytes)
2018-02-06 14:55:50,403 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 14:55:50,421 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 14:55:50,429 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 6.609282 ms
2018-02-06 14:55:50,496 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1589 bytes result sent to driver
2018-02-06 14:55:50,499 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 98 ms on localhost (executor driver) (1/1)
2018-02-06 14:55:50,500 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 14:55:50,501 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (count at BaseSparkSQL.java:26) finished in 0.101 s
2018-02-06 14:55:50,502 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-06 14:55:50,502 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-06 14:55:50,503 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-06 14:55:50,504 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-06 14:55:50,512 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[9] at count at BaseSparkSQL.java:26), which has no missing parents
2018-02-06 14:55:50,516 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 7.0 KB, free 631.1 MB)
2018-02-06 14:55:50,519 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.7 KB, free 631.1 MB)
2018-02-06 14:55:50,521 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.11.26:59584 (size: 3.7 KB, free: 631.7 MB)
2018-02-06 14:55:50,521 INFO[org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2018-02-06 14:55:50,522 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at count at BaseSparkSQL.java:26) (first 15 tasks are for partitions Vector(0))
2018-02-06 14:55:50,522 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
2018-02-06 14:55:50,526 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
2018-02-06 14:55:50,526 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 2)
2018-02-06 14:55:50,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 1 blocks
2018-02-06 14:55:50,545 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
2018-02-06 14:55:50,564 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 2). 1538 bytes result sent to driver
2018-02-06 14:55:50,565 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 2) in 42 ms on localhost (executor driver) (1/1)
2018-02-06 14:55:50,565 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-06 14:55:50,566 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (count at BaseSparkSQL.java:26) finished in 0.043 s
2018-02-06 14:55:50,566 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: count at BaseSparkSQL.java:26, took 0.186477 s
2018-02-06 14:55:50,581 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 14:55:50,587 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@2b550d79{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 14:55:50,589 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 14:55:50,598 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 14:55:50,626 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 14:55:50,626 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 14:55:50,631 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 14:55:50,634 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 14:55:50,637 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 14:55:50,638 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 14:55:50,640 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-d12f5d6b-0eec-4091-bfff-5c7ce6521dd5
2018-02-06 14:58:21,147 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 14:58:21,899 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 14:58:21,936 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 14:58:21,937 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 14:58:21,938 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 14:58:21,939 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 14:58:21,939 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 14:58:22,325 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 59759.
2018-02-06 14:58:22,344 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 14:58:22,395 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 14:58:22,399 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 14:58:22,400 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 14:58:22,409 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-6416b1c5-c8d2-4719-b893-98a798eeacde
2018-02-06 14:58:22,432 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 14:58:22,486 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 14:58:22,604 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2664ms
2018-02-06 14:58:22,687 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 14:58:22,703 INFO[org.spark_project.jetty.server.Server:403] - Started @2765ms
2018-02-06 14:58:22,722 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@33e01298{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 14:58:22,723 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 14:58:22,746 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@773f7880{/jobs,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,747 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bc715b8{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,748 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,749 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,749 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/stages,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,750 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,750 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,752 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@38f57b3d{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,752 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ce3db41{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,753 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,754 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/storage,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,754 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,755 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,755 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,756 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/environment,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,757 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,757 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/executors,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,758 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,759 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,760 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,768 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/static,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,768 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58670130{/,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,769 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/api,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,770 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18518ccf{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,771 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 14:58:22,772 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 14:58:22,865 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 14:58:22,895 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59772.
2018-02-06 14:58:22,897 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:59772
2018-02-06 14:58:22,898 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 14:58:22,900 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 59772, None)
2018-02-06 14:58:22,904 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:59772 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 59772, None)
2018-02-06 14:58:22,908 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 59772, None)
2018-02-06 14:58:22,909 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 59772, None)
2018-02-06 14:58:23,109 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@467f77a5{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 14:58:23,170 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 14:58:23,171 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 14:58:23,178 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c0f7678{/SQL,null,AVAILABLE,@Spark}
2018-02-06 14:58:23,179 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6aa648b9{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 14:58:23,180 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4163f1cd{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 14:58:23,181 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e681bc{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 14:58:23,183 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2d83c5a5{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 14:58:24,444 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 14:58:33,887 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 14:58:33,890 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 14:58:33,892 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<value: string>
2018-02-06 14:58:33,902 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 14:58:34,415 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 191.112061 ms
2018-02-06 14:58:34,465 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 322.5 KB, free 631.5 MB)
2018-02-06 14:58:34,524 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.5 MB)
2018-02-06 14:58:34,531 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:59772 (size: 27.5 KB, free: 631.8 MB)
2018-02-06 14:58:34,536 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from json at BaseSparkSQLTest.java:23
2018-02-06 14:58:34,548 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 14:58:34,757 INFO[org.apache.spark.SparkContext:54] - Starting job: json at BaseSparkSQLTest.java:23
2018-02-06 14:58:34,772 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at BaseSparkSQLTest.java:23) with 1 output partitions
2018-02-06 14:58:34,773 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at BaseSparkSQLTest.java:23)
2018-02-06 14:58:34,773 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 14:58:34,774 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 14:58:34,780 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23), which has no missing parents
2018-02-06 14:58:34,799 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.5 KB, free 631.4 MB)
2018-02-06 14:58:34,806 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KB, free 631.4 MB)
2018-02-06 14:58:34,807 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:59772 (size: 5.3 KB, free: 631.8 MB)
2018-02-06 14:58:34,808 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 14:58:34,824 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23) (first 15 tasks are for partitions Vector(0))
2018-02-06 14:58:34,825 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 14:58:34,866 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 14:58:34,874 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 14:58:34,948 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 14:58:34,976 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 16.937925 ms
2018-02-06 14:58:35,227 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 2776 bytes result sent to driver
2018-02-06 14:58:35,235 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 381 ms on localhost (executor driver) (1/1)
2018-02-06 14:58:35,237 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 14:58:35,240 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at BaseSparkSQLTest.java:23) finished in 0.399 s
2018-02-06 14:58:35,248 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at BaseSparkSQLTest.java:23, took 0.489814 s
2018-02-06 14:58:35,353 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 14:58:35,355 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 14:58:35,355 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<bill_accept_org: string, bill_category_name: string, bill_classify: string, bill_desc: string, bill_discount_amount: double ... 22 more fields>
2018-02-06 14:58:35,356 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 14:58:35,397 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 631.1 MB)
2018-02-06 14:58:35,414 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.1 MB)
2018-02-06 14:58:35,416 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:59772 (size: 27.5 KB, free: 631.7 MB)
2018-02-06 14:58:35,420 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from show at BaseSparkSQL.java:22
2018-02-06 14:58:35,427 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 14:58:35,461 INFO[org.apache.spark.SparkContext:54] - Starting job: show at BaseSparkSQL.java:22
2018-02-06 14:58:35,462 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at BaseSparkSQL.java:22) with 1 output partitions
2018-02-06 14:58:35,463 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at BaseSparkSQL.java:22)
2018-02-06 14:58:35,463 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 14:58:35,463 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 14:58:35,463 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at BaseSparkSQL.java:22), which has no missing parents
2018-02-06 14:58:35,472 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 631.1 MB)
2018-02-06 14:58:35,480 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KB, free 631.1 MB)
2018-02-06 14:58:35,481 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:59772 (size: 5.8 KB, free: 631.7 MB)
2018-02-06 14:58:35,483 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 14:58:35,484 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at BaseSparkSQL.java:22) (first 15 tasks are for partitions Vector(0))
2018-02-06 14:58:35,484 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 14:58:35,486 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 14:58:35,487 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 14:58:35,498 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 14:58:35,548 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 40.382413 ms
2018-02-06 14:58:35,596 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 5386 bytes result sent to driver
2018-02-06 14:58:35,597 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 111 ms on localhost (executor driver) (1/1)
2018-02-06 14:58:35,597 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 14:58:35,598 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at BaseSparkSQL.java:22) finished in 0.113 s
2018-02-06 14:58:35,599 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at BaseSparkSQL.java:22, took 0.137040 s
2018-02-06 14:58:35,656 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 39.331213 ms
2018-02-06 14:58:35,699 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 14:58:35,707 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@33e01298{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 14:58:35,710 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 14:58:35,720 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 14:58:35,737 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 14:58:35,737 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 14:58:35,742 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 14:58:35,744 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 14:58:35,748 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 14:58:35,748 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 14:58:35,749 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-51e72892-e746-488d-921f-7203d2374aa4
2018-02-06 15:00:51,054 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 15:00:51,648 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 15:00:51,677 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 15:00:51,678 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 15:00:51,678 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 15:00:51,679 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 15:00:51,680 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 15:00:52,064 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 59847.
2018-02-06 15:00:52,084 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 15:00:52,132 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 15:00:52,135 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 15:00:52,135 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 15:00:52,144 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-825aae4a-c7cb-4a7e-acba-7d2a951fec86
2018-02-06 15:00:52,168 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 15:00:52,222 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 15:00:52,309 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2629ms
2018-02-06 15:00:52,387 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 15:00:52,404 INFO[org.spark_project.jetty.server.Server:403] - Started @2725ms
2018-02-06 15:00:52,426 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@3bc53e30{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 15:00:52,427 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 15:00:52,453 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,454 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,454 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,456 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,457 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/stages,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,457 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,458 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,460 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,461 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51850751{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,461 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64df9a61{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,462 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e260766{/storage,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,463 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a97744{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,463 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c56e013{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,466 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40258c2f{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,468 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6731787b{/environment,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,469 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,469 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/executors,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,470 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,471 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,472 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,479 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/static,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,480 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4bf3798b{/,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,481 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74e47444{/api,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,482 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46f699d5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,483 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1991f767{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,485 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 15:00:52,586 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 15:00:52,613 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59860.
2018-02-06 15:00:52,614 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:59860
2018-02-06 15:00:52,616 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 15:00:52,618 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 59860, None)
2018-02-06 15:00:52,623 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:59860 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 59860, None)
2018-02-06 15:00:52,627 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 59860, None)
2018-02-06 15:00:52,628 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 59860, None)
2018-02-06 15:00:52,811 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4201a617{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,875 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 15:00:52,876 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 15:00:52,884 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18ca3c62{/SQL,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,885 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44d70181{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,886 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50b1f030{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,886 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fa05212{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 15:00:52,889 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a9c84a5{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 15:00:53,987 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 15:01:03,254 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 15:01:03,256 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 15:01:03,258 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<value: string>
2018-02-06 15:01:03,266 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 15:01:03,733 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 189.44326 ms
2018-02-06 15:01:03,786 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 322.5 KB, free 631.5 MB)
2018-02-06 15:01:03,845 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.5 MB)
2018-02-06 15:01:03,850 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:59860 (size: 27.5 KB, free: 631.8 MB)
2018-02-06 15:01:03,856 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from json at BaseSparkSQLTest.java:23
2018-02-06 15:01:03,865 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 15:01:04,061 INFO[org.apache.spark.SparkContext:54] - Starting job: json at BaseSparkSQLTest.java:23
2018-02-06 15:01:04,076 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at BaseSparkSQLTest.java:23) with 1 output partitions
2018-02-06 15:01:04,076 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at BaseSparkSQLTest.java:23)
2018-02-06 15:01:04,077 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 15:01:04,078 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 15:01:04,083 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23), which has no missing parents
2018-02-06 15:01:04,103 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.5 KB, free 631.4 MB)
2018-02-06 15:01:04,116 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KB, free 631.4 MB)
2018-02-06 15:01:04,119 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:59860 (size: 5.3 KB, free: 631.8 MB)
2018-02-06 15:01:04,119 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 15:01:04,131 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23) (first 15 tasks are for partitions Vector(0))
2018-02-06 15:01:04,132 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 15:01:04,174 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 15:01:04,182 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 15:01:04,246 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 15:01:04,270 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 11.122563 ms
2018-02-06 15:01:04,496 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
2018-02-06 15:01:04,504 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 344 ms on localhost (executor driver) (1/1)
2018-02-06 15:01:04,507 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 15:01:04,511 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at BaseSparkSQLTest.java:23) finished in 0.361 s
2018-02-06 15:01:04,519 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at BaseSparkSQLTest.java:23, took 0.458046 s
2018-02-06 15:01:04,672 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 15:01:04,673 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 15:01:04,674 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<bill_classify: string>
2018-02-06 15:01:04,674 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 15:01:04,803 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 42.069133 ms
2018-02-06 15:01:04,869 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 45.663374 ms
2018-02-06 15:01:04,876 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 631.1 MB)
2018-02-06 15:01:04,893 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.1 MB)
2018-02-06 15:01:04,895 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:59860 (size: 27.5 KB, free: 631.7 MB)
2018-02-06 15:01:04,896 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from show at BaseSparkSQL.java:35
2018-02-06 15:01:04,902 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 15:01:04,973 INFO[org.apache.spark.SparkContext:54] - Starting job: show at BaseSparkSQL.java:35
2018-02-06 15:01:04,976 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 6 (show at BaseSparkSQL.java:35)
2018-02-06 15:01:04,976 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at BaseSparkSQL.java:35) with 1 output partitions
2018-02-06 15:01:04,977 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (show at BaseSparkSQL.java:35)
2018-02-06 15:01:04,977 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-06 15:01:04,977 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-06 15:01:04,978 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at show at BaseSparkSQL.java:35), which has no missing parents
2018-02-06 15:01:04,985 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 24.2 KB, free 631.1 MB)
2018-02-06 15:01:04,989 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.7 KB, free 631.1 MB)
2018-02-06 15:01:04,989 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:59860 (size: 11.7 KB, free: 631.7 MB)
2018-02-06 15:01:04,990 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 15:01:04,992 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at show at BaseSparkSQL.java:35) (first 15 tasks are for partitions Vector(0))
2018-02-06 15:01:04,992 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 15:01:04,994 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5344 bytes)
2018-02-06 15:01:04,994 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 15:01:05,035 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 19.048326 ms
2018-02-06 15:01:05,070 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 15.469445 ms
2018-02-06 15:01:05,092 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 13.525444 ms
2018-02-06 15:01:05,117 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 20.479366 ms
2018-02-06 15:01:05,135 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 15:01:05,258 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 2263 bytes result sent to driver
2018-02-06 15:01:05,261 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 269 ms on localhost (executor driver) (1/1)
2018-02-06 15:01:05,261 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 15:01:05,263 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (show at BaseSparkSQL.java:35) finished in 0.271 s
2018-02-06 15:01:05,264 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-06 15:01:05,265 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-06 15:01:05,266 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-06 15:01:05,266 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-06 15:01:05,269 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[9] at show at BaseSparkSQL.java:35), which has no missing parents
2018-02-06 15:01:05,273 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 23.3 KB, free 631.0 MB)
2018-02-06 15:01:05,275 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 11.6 KB, free 631.0 MB)
2018-02-06 15:01:05,276 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.11.26:59860 (size: 11.6 KB, free: 631.7 MB)
2018-02-06 15:01:05,277 INFO[org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2018-02-06 15:01:05,278 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at show at BaseSparkSQL.java:35) (first 15 tasks are for partitions Vector(0))
2018-02-06 15:01:05,278 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
2018-02-06 15:01:05,280 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,283 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 2)
2018-02-06 15:01:05,303 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,305 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
2018-02-06 15:01:05,327 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 53
2018-02-06 15:01:05,330 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 2). 2550 bytes result sent to driver
2018-02-06 15:01:05,332 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 2) in 53 ms on localhost (executor driver) (1/1)
2018-02-06 15:01:05,332 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-06 15:01:05,333 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (show at BaseSparkSQL.java:35) finished in 0.055 s
2018-02-06 15:01:05,334 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at BaseSparkSQL.java:35, took 0.361233 s
2018-02-06 15:01:05,340 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:59860 in memory (size: 5.3 KB, free: 631.7 MB)
2018-02-06 15:01:05,342 INFO[org.apache.spark.SparkContext:54] - Starting job: show at BaseSparkSQL.java:35
2018-02-06 15:01:05,343 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_3_piece0 on 192.168.11.26:59860 in memory (size: 11.7 KB, free: 631.7 MB)
2018-02-06 15:01:05,345 INFO[org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 0 is 157 bytes
2018-02-06 15:01:05,347 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 2 (show at BaseSparkSQL.java:35) with 4 output partitions
2018-02-06 15:01:05,347 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 4 (show at BaseSparkSQL.java:35)
2018-02-06 15:01:05,347 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 3)
2018-02-06 15:01:05,348 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 15:01:05,349 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 4 (MapPartitionsRDD[9] at show at BaseSparkSQL.java:35), which has no missing parents
2018-02-06 15:01:05,351 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5 stored as values in memory (estimated size 23.3 KB, free 631.1 MB)
2018-02-06 15:01:05,354 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 11.6 KB, free 631.0 MB)
2018-02-06 15:01:05,354 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_5_piece0 in memory on 192.168.11.26:59860 (size: 11.6 KB, free: 631.7 MB)
2018-02-06 15:01:05,355 INFO[org.apache.spark.SparkContext:54] - Created broadcast 5 from broadcast at DAGScheduler.scala:1006
2018-02-06 15:01:05,355 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 4 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at show at BaseSparkSQL.java:35) (first 15 tasks are for partitions Vector(1, 2, 3, 4))
2018-02-06 15:01:05,355 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 4.0 with 4 tasks
2018-02-06 15:01:05,356 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 4.0 (TID 3, localhost, executor driver, partition 1, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,356 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 4.0 (TID 4, localhost, executor driver, partition 2, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,357 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 4.0 (TID 3)
2018-02-06 15:01:05,357 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 4.0 (TID 4)
2018-02-06 15:01:05,365 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,366 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,367 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,367 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,369 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 4.0 (TID 3). 2464 bytes result sent to driver
2018-02-06 15:01:05,373 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 4.0 (TID 5, localhost, executor driver, partition 3, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,373 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 4.0 (TID 4). 2464 bytes result sent to driver
2018-02-06 15:01:05,374 INFO[org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 4.0 (TID 5)
2018-02-06 15:01:05,374 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 4.0 (TID 6, localhost, executor driver, partition 4, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,374 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 4.0 (TID 4) in 18 ms on localhost (executor driver) (1/4)
2018-02-06 15:01:05,375 INFO[org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 4.0 (TID 6)
2018-02-06 15:01:05,377 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,377 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,378 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,378 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,381 INFO[org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 4.0 (TID 6). 2421 bytes result sent to driver
2018-02-06 15:01:05,383 INFO[org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 4.0 (TID 5). 2421 bytes result sent to driver
2018-02-06 15:01:05,383 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 4.0 (TID 6) in 9 ms on localhost (executor driver) (2/4)
2018-02-06 15:01:05,387 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 4.0 (TID 5) in 14 ms on localhost (executor driver) (3/4)
2018-02-06 15:01:05,391 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 4.0 (TID 3) in 35 ms on localhost (executor driver) (4/4)
2018-02-06 15:01:05,391 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 4 (show at BaseSparkSQL.java:35) finished in 0.035 s
2018-02-06 15:01:05,392 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 2 finished: show at BaseSparkSQL.java:35, took 0.050260 s
2018-02-06 15:01:05,392 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2018-02-06 15:01:05,396 INFO[org.apache.spark.SparkContext:54] - Starting job: show at BaseSparkSQL.java:35
2018-02-06 15:01:05,397 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 3 (show at BaseSparkSQL.java:35) with 20 output partitions
2018-02-06 15:01:05,397 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 6 (show at BaseSparkSQL.java:35)
2018-02-06 15:01:05,397 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 5)
2018-02-06 15:01:05,398 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 15:01:05,398 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 6 (MapPartitionsRDD[9] at show at BaseSparkSQL.java:35), which has no missing parents
2018-02-06 15:01:05,401 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_6 stored as values in memory (estimated size 23.3 KB, free 631.0 MB)
2018-02-06 15:01:05,403 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 11.6 KB, free 631.0 MB)
2018-02-06 15:01:05,404 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_6_piece0 in memory on 192.168.11.26:59860 (size: 11.6 KB, free: 631.7 MB)
2018-02-06 15:01:05,404 INFO[org.apache.spark.SparkContext:54] - Created broadcast 6 from broadcast at DAGScheduler.scala:1006
2018-02-06 15:01:05,407 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 20 missing tasks from ResultStage 6 (MapPartitionsRDD[9] at show at BaseSparkSQL.java:35) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19))
2018-02-06 15:01:05,407 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 6.0 with 20 tasks
2018-02-06 15:01:05,408 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 6.0 (TID 7, localhost, executor driver, partition 5, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,408 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 6.0 (TID 8, localhost, executor driver, partition 6, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,408 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 6.0 (TID 8)
2018-02-06 15:01:05,408 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 6.0 (TID 7)
2018-02-06 15:01:05,413 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,415 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
2018-02-06 15:01:05,415 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,415 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,417 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 6.0 (TID 8). 2421 bytes result sent to driver
2018-02-06 15:01:05,420 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 6.0 (TID 9, localhost, executor driver, partition 7, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,421 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 6.0 (TID 8) in 13 ms on localhost (executor driver) (1/20)
2018-02-06 15:01:05,422 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 6.0 (TID 7). 2421 bytes result sent to driver
2018-02-06 15:01:05,422 INFO[org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 6.0 (TID 9)
2018-02-06 15:01:05,423 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 6.0 (TID 10, localhost, executor driver, partition 8, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,424 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 6.0 (TID 7) in 17 ms on localhost (executor driver) (2/20)
2018-02-06 15:01:05,425 INFO[org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 6.0 (TID 10)
2018-02-06 15:01:05,428 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,429 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,429 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,429 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,433 INFO[org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 6.0 (TID 9). 2464 bytes result sent to driver
2018-02-06 15:01:05,436 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 6.0 (TID 11, localhost, executor driver, partition 9, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,437 INFO[org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 6.0 (TID 10). 2464 bytes result sent to driver
2018-02-06 15:01:05,437 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 6.0 (TID 9) in 17 ms on localhost (executor driver) (3/20)
2018-02-06 15:01:05,439 INFO[org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 6.0 (TID 11)
2018-02-06 15:01:05,440 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 6.0 (TID 12, localhost, executor driver, partition 10, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,442 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 6.0 (TID 10) in 19 ms on localhost (executor driver) (4/20)
2018-02-06 15:01:05,443 INFO[org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 6.0 (TID 12)
2018-02-06 15:01:05,445 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,449 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
2018-02-06 15:01:05,449 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,450 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,453 INFO[org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 6.0 (TID 11). 2421 bytes result sent to driver
2018-02-06 15:01:05,454 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 6.0 (TID 13, localhost, executor driver, partition 11, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,454 INFO[org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 6.0 (TID 13)
2018-02-06 15:01:05,454 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 6.0 (TID 11) in 18 ms on localhost (executor driver) (5/20)
2018-02-06 15:01:05,457 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,457 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,460 INFO[org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 6.0 (TID 13). 2421 bytes result sent to driver
2018-02-06 15:01:05,460 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 6.0 (TID 14, localhost, executor driver, partition 12, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,461 INFO[org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 6.0 (TID 14)
2018-02-06 15:01:05,461 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 6.0 (TID 13) in 8 ms on localhost (executor driver) (6/20)
2018-02-06 15:01:05,473 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,473 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,482 INFO[org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 6.0 (TID 12). 2464 bytes result sent to driver
2018-02-06 15:01:05,483 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 6.0 (TID 15, localhost, executor driver, partition 13, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,484 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 6.0 (TID 12) in 44 ms on localhost (executor driver) (7/20)
2018-02-06 15:01:05,491 INFO[org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 6.0 (TID 14). 2464 bytes result sent to driver
2018-02-06 15:01:05,492 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 6.0 (TID 16, localhost, executor driver, partition 14, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,492 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 6.0 (TID 14) in 32 ms on localhost (executor driver) (8/20)
2018-02-06 15:01:05,492 INFO[org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 6.0 (TID 16)
2018-02-06 15:01:05,504 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,504 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,509 INFO[org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 6.0 (TID 15)
2018-02-06 15:01:05,512 INFO[org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 6.0 (TID 16). 2421 bytes result sent to driver
2018-02-06 15:01:05,520 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,520 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,521 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 6.0 (TID 17, localhost, executor driver, partition 15, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,524 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 6.0 (TID 16) in 33 ms on localhost (executor driver) (9/20)
2018-02-06 15:01:05,529 INFO[org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 6.0 (TID 17)
2018-02-06 15:01:05,532 INFO[org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 6.0 (TID 15). 2421 bytes result sent to driver
2018-02-06 15:01:05,534 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 6.0 (TID 18, localhost, executor driver, partition 16, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,540 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 6.0 (TID 15) in 57 ms on localhost (executor driver) (10/20)
2018-02-06 15:01:05,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,545 INFO[org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 6.0 (TID 18)
2018-02-06 15:01:05,550 INFO[org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 6.0 (TID 17). 2464 bytes result sent to driver
2018-02-06 15:01:05,553 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 6.0 (TID 19, localhost, executor driver, partition 17, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,554 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 6.0 (TID 17) in 33 ms on localhost (executor driver) (11/20)
2018-02-06 15:01:05,555 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,555 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,557 INFO[org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 6.0 (TID 19)
2018-02-06 15:01:05,560 INFO[org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 6.0 (TID 18). 2464 bytes result sent to driver
2018-02-06 15:01:05,566 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,566 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 6.0 (TID 20, localhost, executor driver, partition 18, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,566 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,567 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 6.0 (TID 18) in 34 ms on localhost (executor driver) (12/20)
2018-02-06 15:01:05,568 INFO[org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 6.0 (TID 20)
2018-02-06 15:01:05,578 INFO[org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 6.0 (TID 19). 2507 bytes result sent to driver
2018-02-06 15:01:05,585 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,586 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,586 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 6.0 (TID 21, localhost, executor driver, partition 19, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,588 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 6.0 (TID 19) in 36 ms on localhost (executor driver) (13/20)
2018-02-06 15:01:05,589 INFO[org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 6.0 (TID 21)
2018-02-06 15:01:05,591 INFO[org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 6.0 (TID 20). 2464 bytes result sent to driver
2018-02-06 15:01:05,593 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 6.0 (TID 22, localhost, executor driver, partition 20, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,595 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,595 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,599 INFO[org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 6.0 (TID 21). 2421 bytes result sent to driver
2018-02-06 15:01:05,599 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 6.0 (TID 20) in 29 ms on localhost (executor driver) (14/20)
2018-02-06 15:01:05,600 INFO[org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 6.0 (TID 22)
2018-02-06 15:01:05,600 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 16.0 in stage 6.0 (TID 23, localhost, executor driver, partition 21, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,605 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,605 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,605 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 6.0 (TID 21) in 19 ms on localhost (executor driver) (15/20)
2018-02-06 15:01:05,607 INFO[org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 6.0 (TID 22). 2464 bytes result sent to driver
2018-02-06 15:01:05,609 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 17.0 in stage 6.0 (TID 24, localhost, executor driver, partition 22, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,610 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 6.0 (TID 22) in 18 ms on localhost (executor driver) (16/20)
2018-02-06 15:01:05,610 INFO[org.apache.spark.executor.Executor:54] - Running task 17.0 in stage 6.0 (TID 24)
2018-02-06 15:01:05,620 INFO[org.apache.spark.executor.Executor:54] - Running task 16.0 in stage 6.0 (TID 23)
2018-02-06 15:01:05,621 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,621 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,628 INFO[org.apache.spark.executor.Executor:54] - Finished task 17.0 in stage 6.0 (TID 24). 2421 bytes result sent to driver
2018-02-06 15:01:05,628 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 18.0 in stage 6.0 (TID 25, localhost, executor driver, partition 23, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,629 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 17.0 in stage 6.0 (TID 24) in 20 ms on localhost (executor driver) (17/20)
2018-02-06 15:01:05,629 INFO[org.apache.spark.executor.Executor:54] - Running task 18.0 in stage 6.0 (TID 25)
2018-02-06 15:01:05,633 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,633 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,634 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,634 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,635 INFO[org.apache.spark.executor.Executor:54] - Finished task 18.0 in stage 6.0 (TID 25). 2464 bytes result sent to driver
2018-02-06 15:01:05,636 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 19.0 in stage 6.0 (TID 26, localhost, executor driver, partition 24, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,636 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 18.0 in stage 6.0 (TID 25) in 8 ms on localhost (executor driver) (18/20)
2018-02-06 15:01:05,636 INFO[org.apache.spark.executor.Executor:54] - Running task 19.0 in stage 6.0 (TID 26)
2018-02-06 15:01:05,667 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,668 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,670 INFO[org.apache.spark.executor.Executor:54] - Finished task 19.0 in stage 6.0 (TID 26). 2421 bytes result sent to driver
2018-02-06 15:01:05,671 INFO[org.apache.spark.executor.Executor:54] - Finished task 16.0 in stage 6.0 (TID 23). 2464 bytes result sent to driver
2018-02-06 15:01:05,671 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 19.0 in stage 6.0 (TID 26) in 35 ms on localhost (executor driver) (19/20)
2018-02-06 15:01:05,672 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 16.0 in stage 6.0 (TID 23) in 72 ms on localhost (executor driver) (20/20)
2018-02-06 15:01:05,672 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 6.0, whose tasks have all completed, from pool 
2018-02-06 15:01:05,674 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 6 (show at BaseSparkSQL.java:35) finished in 0.266 s
2018-02-06 15:01:05,674 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 3 finished: show at BaseSparkSQL.java:35, took 0.278404 s
2018-02-06 15:01:05,677 INFO[org.apache.spark.SparkContext:54] - Starting job: show at BaseSparkSQL.java:35
2018-02-06 15:01:05,683 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 4 (show at BaseSparkSQL.java:35) with 100 output partitions
2018-02-06 15:01:05,683 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 8 (show at BaseSparkSQL.java:35)
2018-02-06 15:01:05,683 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 7)
2018-02-06 15:01:05,684 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 15:01:05,684 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 8 (MapPartitionsRDD[9] at show at BaseSparkSQL.java:35), which has no missing parents
2018-02-06 15:01:05,704 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_7 stored as values in memory (estimated size 23.3 KB, free 631.0 MB)
2018-02-06 15:01:05,708 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_7_piece0 stored as bytes in memory (estimated size 11.6 KB, free 631.0 MB)
2018-02-06 15:01:05,708 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_7_piece0 in memory on 192.168.11.26:59860 (size: 11.6 KB, free: 631.7 MB)
2018-02-06 15:01:05,709 INFO[org.apache.spark.SparkContext:54] - Created broadcast 7 from broadcast at DAGScheduler.scala:1006
2018-02-06 15:01:05,710 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 100 missing tasks from ResultStage 8 (MapPartitionsRDD[9] at show at BaseSparkSQL.java:35) (first 15 tasks are for partitions Vector(25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39))
2018-02-06 15:01:05,710 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 8.0 with 100 tasks
2018-02-06 15:01:05,718 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 8.0 (TID 27, localhost, executor driver, partition 25, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,719 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 8.0 (TID 28, localhost, executor driver, partition 26, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,722 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 8.0 (TID 27)
2018-02-06 15:01:05,730 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 8.0 (TID 28)
2018-02-06 15:01:05,732 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,732 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,733 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,734 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,736 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 8.0 (TID 28). 2421 bytes result sent to driver
2018-02-06 15:01:05,739 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 8.0 (TID 27). 2464 bytes result sent to driver
2018-02-06 15:01:05,741 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 8.0 (TID 29, localhost, executor driver, partition 27, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,744 INFO[org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 8.0 (TID 29)
2018-02-06 15:01:05,744 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 8.0 (TID 30, localhost, executor driver, partition 28, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,745 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 8.0 (TID 27) in 29 ms on localhost (executor driver) (1/100)
2018-02-06 15:01:05,746 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 8.0 (TID 28) in 27 ms on localhost (executor driver) (2/100)
2018-02-06 15:01:05,746 INFO[org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 8.0 (TID 30)
2018-02-06 15:01:05,750 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,750 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
2018-02-06 15:01:05,754 INFO[org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 8.0 (TID 29). 2421 bytes result sent to driver
2018-02-06 15:01:05,754 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 8.0 (TID 31, localhost, executor driver, partition 29, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,756 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,756 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,756 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 8.0 (TID 29) in 15 ms on localhost (executor driver) (3/100)
2018-02-06 15:01:05,758 INFO[org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 8.0 (TID 30). 2464 bytes result sent to driver
2018-02-06 15:01:05,758 INFO[org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 8.0 (TID 31)
2018-02-06 15:01:05,767 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 8.0 (TID 32, localhost, executor driver, partition 30, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,767 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,768 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,768 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 8.0 (TID 30) in 24 ms on localhost (executor driver) (4/100)
2018-02-06 15:01:05,769 INFO[org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 8.0 (TID 32)
2018-02-06 15:01:05,770 INFO[org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 8.0 (TID 31). 2421 bytes result sent to driver
2018-02-06 15:01:05,774 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,774 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,776 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 8.0 (TID 33, localhost, executor driver, partition 31, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,776 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 8.0 (TID 31) in 22 ms on localhost (executor driver) (5/100)
2018-02-06 15:01:05,777 INFO[org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 8.0 (TID 33)
2018-02-06 15:01:05,778 INFO[org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 8.0 (TID 32). 2421 bytes result sent to driver
2018-02-06 15:01:05,779 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 8.0 (TID 34, localhost, executor driver, partition 32, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,783 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 8.0 (TID 32) in 17 ms on localhost (executor driver) (6/100)
2018-02-06 15:01:05,785 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,786 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,789 INFO[org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 8.0 (TID 34)
2018-02-06 15:01:05,791 INFO[org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 8.0 (TID 33). 2464 bytes result sent to driver
2018-02-06 15:01:05,792 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 8.0 (TID 35, localhost, executor driver, partition 33, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,792 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 8.0 (TID 33) in 16 ms on localhost (executor driver) (7/100)
2018-02-06 15:01:05,794 INFO[org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 8.0 (TID 35)
2018-02-06 15:01:05,795 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,795 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,798 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,799 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,799 INFO[org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 8.0 (TID 34). 2421 bytes result sent to driver
2018-02-06 15:01:05,801 INFO[org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 8.0 (TID 35). 2464 bytes result sent to driver
2018-02-06 15:01:05,801 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 8.0 (TID 36, localhost, executor driver, partition 34, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,802 INFO[org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 8.0 (TID 36)
2018-02-06 15:01:05,803 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 8.0 (TID 37, localhost, executor driver, partition 35, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,803 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 8.0 (TID 34) in 25 ms on localhost (executor driver) (8/100)
2018-02-06 15:01:05,803 INFO[org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 8.0 (TID 37)
2018-02-06 15:01:05,803 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 8.0 (TID 35) in 11 ms on localhost (executor driver) (9/100)
2018-02-06 15:01:05,806 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,806 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,809 INFO[org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 8.0 (TID 37). 2464 bytes result sent to driver
2018-02-06 15:01:05,811 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 8.0 (TID 38, localhost, executor driver, partition 36, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,812 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 8.0 (TID 37) in 9 ms on localhost (executor driver) (10/100)
2018-02-06 15:01:05,815 INFO[org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 8.0 (TID 38)
2018-02-06 15:01:05,818 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,818 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,825 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,825 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,827 INFO[org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 8.0 (TID 38). 2421 bytes result sent to driver
2018-02-06 15:01:05,828 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 8.0 (TID 39, localhost, executor driver, partition 37, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,829 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 8.0 (TID 38) in 18 ms on localhost (executor driver) (11/100)
2018-02-06 15:01:05,829 INFO[org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 8.0 (TID 39)
2018-02-06 15:01:05,831 INFO[org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 8.0 (TID 36). 2421 bytes result sent to driver
2018-02-06 15:01:05,831 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 8.0 (TID 40, localhost, executor driver, partition 38, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,832 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 8.0 (TID 36) in 31 ms on localhost (executor driver) (12/100)
2018-02-06 15:01:05,833 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,834 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,833 INFO[org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 8.0 (TID 40)
2018-02-06 15:01:05,837 INFO[org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 8.0 (TID 39). 2464 bytes result sent to driver
2018-02-06 15:01:05,840 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 8.0 (TID 41, localhost, executor driver, partition 39, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,841 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,841 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,841 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 8.0 (TID 39) in 13 ms on localhost (executor driver) (13/100)
2018-02-06 15:01:05,842 INFO[org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 8.0 (TID 41)
2018-02-06 15:01:05,848 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,848 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,849 INFO[org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 8.0 (TID 40). 2464 bytes result sent to driver
2018-02-06 15:01:05,854 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 8.0 (TID 42, localhost, executor driver, partition 40, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,854 INFO[org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 8.0 (TID 41). 2507 bytes result sent to driver
2018-02-06 15:01:05,854 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 8.0 (TID 40) in 23 ms on localhost (executor driver) (14/100)
2018-02-06 15:01:05,855 INFO[org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 8.0 (TID 42)
2018-02-06 15:01:05,855 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 16.0 in stage 8.0 (TID 43, localhost, executor driver, partition 41, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,856 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 8.0 (TID 41) in 16 ms on localhost (executor driver) (15/100)
2018-02-06 15:01:05,858 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,858 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,858 INFO[org.apache.spark.executor.Executor:54] - Running task 16.0 in stage 8.0 (TID 43)
2018-02-06 15:01:05,861 INFO[org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 8.0 (TID 42). 2421 bytes result sent to driver
2018-02-06 15:01:05,864 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 17.0 in stage 8.0 (TID 44, localhost, executor driver, partition 42, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,865 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,865 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,865 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 8.0 (TID 42) in 11 ms on localhost (executor driver) (16/100)
2018-02-06 15:01:05,866 INFO[org.apache.spark.executor.Executor:54] - Running task 17.0 in stage 8.0 (TID 44)
2018-02-06 15:01:05,868 INFO[org.apache.spark.executor.Executor:54] - Finished task 16.0 in stage 8.0 (TID 43). 2464 bytes result sent to driver
2018-02-06 15:01:05,869 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,869 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,873 INFO[org.apache.spark.executor.Executor:54] - Finished task 17.0 in stage 8.0 (TID 44). 2464 bytes result sent to driver
2018-02-06 15:01:05,873 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 18.0 in stage 8.0 (TID 45, localhost, executor driver, partition 43, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,874 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 19.0 in stage 8.0 (TID 46, localhost, executor driver, partition 44, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,875 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 16.0 in stage 8.0 (TID 43) in 20 ms on localhost (executor driver) (17/100)
2018-02-06 15:01:05,875 INFO[org.apache.spark.executor.Executor:54] - Running task 19.0 in stage 8.0 (TID 46)
2018-02-06 15:01:05,875 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 17.0 in stage 8.0 (TID 44) in 13 ms on localhost (executor driver) (18/100)
2018-02-06 15:01:05,875 INFO[org.apache.spark.executor.Executor:54] - Running task 18.0 in stage 8.0 (TID 45)
2018-02-06 15:01:05,878 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,878 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,878 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,879 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,881 INFO[org.apache.spark.executor.Executor:54] - Finished task 18.0 in stage 8.0 (TID 45). 2421 bytes result sent to driver
2018-02-06 15:01:05,881 INFO[org.apache.spark.executor.Executor:54] - Finished task 19.0 in stage 8.0 (TID 46). 2421 bytes result sent to driver
2018-02-06 15:01:05,882 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 20.0 in stage 8.0 (TID 47, localhost, executor driver, partition 45, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,882 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 21.0 in stage 8.0 (TID 48, localhost, executor driver, partition 46, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,883 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 18.0 in stage 8.0 (TID 45) in 10 ms on localhost (executor driver) (19/100)
2018-02-06 15:01:05,883 INFO[org.apache.spark.executor.Executor:54] - Running task 20.0 in stage 8.0 (TID 47)
2018-02-06 15:01:05,884 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 19.0 in stage 8.0 (TID 46) in 9 ms on localhost (executor driver) (20/100)
2018-02-06 15:01:05,885 INFO[org.apache.spark.executor.Executor:54] - Running task 21.0 in stage 8.0 (TID 48)
2018-02-06 15:01:05,888 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,888 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,890 INFO[org.apache.spark.executor.Executor:54] - Finished task 20.0 in stage 8.0 (TID 47). 2421 bytes result sent to driver
2018-02-06 15:01:05,891 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 22.0 in stage 8.0 (TID 49, localhost, executor driver, partition 47, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,891 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,895 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
2018-02-06 15:01:05,895 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 20.0 in stage 8.0 (TID 47) in 13 ms on localhost (executor driver) (21/100)
2018-02-06 15:01:05,911 INFO[org.apache.spark.executor.Executor:54] - Finished task 21.0 in stage 8.0 (TID 48). 2421 bytes result sent to driver
2018-02-06 15:01:05,911 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 23.0 in stage 8.0 (TID 50, localhost, executor driver, partition 48, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,911 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 21.0 in stage 8.0 (TID 48) in 29 ms on localhost (executor driver) (22/100)
2018-02-06 15:01:05,911 INFO[org.apache.spark.executor.Executor:54] - Running task 23.0 in stage 8.0 (TID 50)
2018-02-06 15:01:05,918 INFO[org.apache.spark.executor.Executor:54] - Running task 22.0 in stage 8.0 (TID 49)
2018-02-06 15:01:05,922 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,923 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,924 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,925 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,927 INFO[org.apache.spark.executor.Executor:54] - Finished task 22.0 in stage 8.0 (TID 49). 2464 bytes result sent to driver
2018-02-06 15:01:05,927 INFO[org.apache.spark.executor.Executor:54] - Finished task 23.0 in stage 8.0 (TID 50). 2421 bytes result sent to driver
2018-02-06 15:01:05,928 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 24.0 in stage 8.0 (TID 51, localhost, executor driver, partition 49, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,928 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 25.0 in stage 8.0 (TID 52, localhost, executor driver, partition 50, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,938 INFO[org.apache.spark.executor.Executor:54] - Running task 24.0 in stage 8.0 (TID 51)
2018-02-06 15:01:05,939 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 22.0 in stage 8.0 (TID 49) in 48 ms on localhost (executor driver) (23/100)
2018-02-06 15:01:05,939 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 23.0 in stage 8.0 (TID 50) in 28 ms on localhost (executor driver) (24/100)
2018-02-06 15:01:05,940 INFO[org.apache.spark.executor.Executor:54] - Running task 25.0 in stage 8.0 (TID 52)
2018-02-06 15:01:05,941 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,941 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,946 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,946 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,951 INFO[org.apache.spark.executor.Executor:54] - Finished task 25.0 in stage 8.0 (TID 52). 2464 bytes result sent to driver
2018-02-06 15:01:05,952 INFO[org.apache.spark.executor.Executor:54] - Finished task 24.0 in stage 8.0 (TID 51). 2421 bytes result sent to driver
2018-02-06 15:01:05,952 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 26.0 in stage 8.0 (TID 53, localhost, executor driver, partition 51, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,953 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 27.0 in stage 8.0 (TID 54, localhost, executor driver, partition 52, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,953 INFO[org.apache.spark.executor.Executor:54] - Running task 26.0 in stage 8.0 (TID 53)
2018-02-06 15:01:05,953 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 25.0 in stage 8.0 (TID 52) in 25 ms on localhost (executor driver) (25/100)
2018-02-06 15:01:05,953 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 24.0 in stage 8.0 (TID 51) in 25 ms on localhost (executor driver) (26/100)
2018-02-06 15:01:05,954 INFO[org.apache.spark.executor.Executor:54] - Running task 27.0 in stage 8.0 (TID 54)
2018-02-06 15:01:05,959 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,959 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:05,961 INFO[org.apache.spark.executor.Executor:54] - Finished task 26.0 in stage 8.0 (TID 53). 2464 bytes result sent to driver
2018-02-06 15:01:05,962 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 28.0 in stage 8.0 (TID 55, localhost, executor driver, partition 53, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,962 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,963 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 26.0 in stage 8.0 (TID 53) in 11 ms on localhost (executor driver) (27/100)
2018-02-06 15:01:05,963 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,963 INFO[org.apache.spark.executor.Executor:54] - Running task 28.0 in stage 8.0 (TID 55)
2018-02-06 15:01:05,967 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,972 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 5 ms
2018-02-06 15:01:05,971 INFO[org.apache.spark.executor.Executor:54] - Finished task 27.0 in stage 8.0 (TID 54). 2464 bytes result sent to driver
2018-02-06 15:01:05,974 INFO[org.apache.spark.executor.Executor:54] - Finished task 28.0 in stage 8.0 (TID 55). 2421 bytes result sent to driver
2018-02-06 15:01:05,975 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 29.0 in stage 8.0 (TID 56, localhost, executor driver, partition 54, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,975 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 30.0 in stage 8.0 (TID 57, localhost, executor driver, partition 55, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,976 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 28.0 in stage 8.0 (TID 55) in 14 ms on localhost (executor driver) (28/100)
2018-02-06 15:01:05,976 INFO[org.apache.spark.executor.Executor:54] - Running task 30.0 in stage 8.0 (TID 57)
2018-02-06 15:01:05,975 INFO[org.apache.spark.executor.Executor:54] - Running task 29.0 in stage 8.0 (TID 56)
2018-02-06 15:01:05,978 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 27.0 in stage 8.0 (TID 54) in 26 ms on localhost (executor driver) (29/100)
2018-02-06 15:01:05,984 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,984 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,984 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,986 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
2018-02-06 15:01:05,988 INFO[org.apache.spark.executor.Executor:54] - Finished task 29.0 in stage 8.0 (TID 56). 2464 bytes result sent to driver
2018-02-06 15:01:05,986 INFO[org.apache.spark.executor.Executor:54] - Finished task 30.0 in stage 8.0 (TID 57). 2464 bytes result sent to driver
2018-02-06 15:01:05,989 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 31.0 in stage 8.0 (TID 58, localhost, executor driver, partition 56, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,989 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 32.0 in stage 8.0 (TID 59, localhost, executor driver, partition 57, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:05,990 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 29.0 in stage 8.0 (TID 56) in 16 ms on localhost (executor driver) (30/100)
2018-02-06 15:01:05,990 INFO[org.apache.spark.executor.Executor:54] - Running task 32.0 in stage 8.0 (TID 59)
2018-02-06 15:01:05,990 INFO[org.apache.spark.executor.Executor:54] - Running task 31.0 in stage 8.0 (TID 58)
2018-02-06 15:01:05,995 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 30.0 in stage 8.0 (TID 57) in 20 ms on localhost (executor driver) (31/100)
2018-02-06 15:01:05,995 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:05,996 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:05,999 INFO[org.apache.spark.executor.Executor:54] - Finished task 31.0 in stage 8.0 (TID 58). 2421 bytes result sent to driver
2018-02-06 15:01:05,999 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 33.0 in stage 8.0 (TID 60, localhost, executor driver, partition 58, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,001 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 31.0 in stage 8.0 (TID 58) in 12 ms on localhost (executor driver) (32/100)
2018-02-06 15:01:06,001 INFO[org.apache.spark.executor.Executor:54] - Running task 33.0 in stage 8.0 (TID 60)
2018-02-06 15:01:06,005 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,006 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,008 INFO[org.apache.spark.executor.Executor:54] - Finished task 33.0 in stage 8.0 (TID 60). 2464 bytes result sent to driver
2018-02-06 15:01:06,008 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 34.0 in stage 8.0 (TID 61, localhost, executor driver, partition 59, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,010 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 33.0 in stage 8.0 (TID 60) in 11 ms on localhost (executor driver) (33/100)
2018-02-06 15:01:06,011 INFO[org.apache.spark.executor.Executor:54] - Running task 34.0 in stage 8.0 (TID 61)
2018-02-06 15:01:06,014 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,015 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,019 INFO[org.apache.spark.executor.Executor:54] - Finished task 32.0 in stage 8.0 (TID 59). 2464 bytes result sent to driver
2018-02-06 15:01:06,020 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 35.0 in stage 8.0 (TID 62, localhost, executor driver, partition 60, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,020 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 32.0 in stage 8.0 (TID 59) in 31 ms on localhost (executor driver) (34/100)
2018-02-06 15:01:06,020 INFO[org.apache.spark.executor.Executor:54] - Running task 35.0 in stage 8.0 (TID 62)
2018-02-06 15:01:06,022 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,022 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,022 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,022 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,025 INFO[org.apache.spark.executor.Executor:54] - Finished task 35.0 in stage 8.0 (TID 62). 2464 bytes result sent to driver
2018-02-06 15:01:06,026 INFO[org.apache.spark.executor.Executor:54] - Finished task 34.0 in stage 8.0 (TID 61). 2464 bytes result sent to driver
2018-02-06 15:01:06,026 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 36.0 in stage 8.0 (TID 63, localhost, executor driver, partition 61, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,027 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 37.0 in stage 8.0 (TID 64, localhost, executor driver, partition 62, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,027 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 35.0 in stage 8.0 (TID 62) in 8 ms on localhost (executor driver) (35/100)
2018-02-06 15:01:06,027 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 34.0 in stage 8.0 (TID 61) in 19 ms on localhost (executor driver) (36/100)
2018-02-06 15:01:06,028 INFO[org.apache.spark.executor.Executor:54] - Running task 36.0 in stage 8.0 (TID 63)
2018-02-06 15:01:06,032 INFO[org.apache.spark.executor.Executor:54] - Running task 37.0 in stage 8.0 (TID 64)
2018-02-06 15:01:06,034 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,034 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,035 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,035 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,039 INFO[org.apache.spark.executor.Executor:54] - Finished task 37.0 in stage 8.0 (TID 64). 2421 bytes result sent to driver
2018-02-06 15:01:06,039 INFO[org.apache.spark.executor.Executor:54] - Finished task 36.0 in stage 8.0 (TID 63). 2421 bytes result sent to driver
2018-02-06 15:01:06,039 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 38.0 in stage 8.0 (TID 65, localhost, executor driver, partition 63, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,040 INFO[org.apache.spark.executor.Executor:54] - Running task 38.0 in stage 8.0 (TID 65)
2018-02-06 15:01:06,040 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 39.0 in stage 8.0 (TID 66, localhost, executor driver, partition 64, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,045 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,045 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,051 INFO[org.apache.spark.executor.Executor:54] - Finished task 38.0 in stage 8.0 (TID 65). 2421 bytes result sent to driver
2018-02-06 15:01:06,052 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 36.0 in stage 8.0 (TID 63) in 26 ms on localhost (executor driver) (37/100)
2018-02-06 15:01:06,052 INFO[org.apache.spark.executor.Executor:54] - Running task 39.0 in stage 8.0 (TID 66)
2018-02-06 15:01:06,053 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 37.0 in stage 8.0 (TID 64) in 26 ms on localhost (executor driver) (38/100)
2018-02-06 15:01:06,054 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 40.0 in stage 8.0 (TID 67, localhost, executor driver, partition 65, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,055 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,055 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,055 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 38.0 in stage 8.0 (TID 65) in 16 ms on localhost (executor driver) (39/100)
2018-02-06 15:01:06,056 INFO[org.apache.spark.executor.Executor:54] - Running task 40.0 in stage 8.0 (TID 67)
2018-02-06 15:01:06,057 INFO[org.apache.spark.executor.Executor:54] - Finished task 39.0 in stage 8.0 (TID 66). 2421 bytes result sent to driver
2018-02-06 15:01:06,059 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 41.0 in stage 8.0 (TID 68, localhost, executor driver, partition 66, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,060 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 39.0 in stage 8.0 (TID 66) in 19 ms on localhost (executor driver) (40/100)
2018-02-06 15:01:06,060 INFO[org.apache.spark.executor.Executor:54] - Running task 41.0 in stage 8.0 (TID 68)
2018-02-06 15:01:06,060 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,063 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,063 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,067 INFO[org.apache.spark.executor.Executor:54] - Finished task 41.0 in stage 8.0 (TID 68). 2421 bytes result sent to driver
2018-02-06 15:01:06,067 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
2018-02-06 15:01:06,068 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 42.0 in stage 8.0 (TID 69, localhost, executor driver, partition 67, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,068 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 41.0 in stage 8.0 (TID 68) in 9 ms on localhost (executor driver) (41/100)
2018-02-06 15:01:06,069 INFO[org.apache.spark.executor.Executor:54] - Finished task 40.0 in stage 8.0 (TID 67). 2421 bytes result sent to driver
2018-02-06 15:01:06,070 INFO[org.apache.spark.executor.Executor:54] - Running task 42.0 in stage 8.0 (TID 69)
2018-02-06 15:01:06,070 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 43.0 in stage 8.0 (TID 70, localhost, executor driver, partition 68, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,072 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 40.0 in stage 8.0 (TID 67) in 18 ms on localhost (executor driver) (42/100)
2018-02-06 15:01:06,074 INFO[org.apache.spark.executor.Executor:54] - Running task 43.0 in stage 8.0 (TID 70)
2018-02-06 15:01:06,074 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,074 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,076 INFO[org.apache.spark.executor.Executor:54] - Finished task 42.0 in stage 8.0 (TID 69). 2464 bytes result sent to driver
2018-02-06 15:01:06,077 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,078 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,082 INFO[org.apache.spark.executor.Executor:54] - Finished task 43.0 in stage 8.0 (TID 70). 2464 bytes result sent to driver
2018-02-06 15:01:06,082 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 44.0 in stage 8.0 (TID 71, localhost, executor driver, partition 69, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,083 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 45.0 in stage 8.0 (TID 72, localhost, executor driver, partition 70, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,084 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 42.0 in stage 8.0 (TID 69) in 16 ms on localhost (executor driver) (43/100)
2018-02-06 15:01:06,084 INFO[org.apache.spark.executor.Executor:54] - Running task 44.0 in stage 8.0 (TID 71)
2018-02-06 15:01:06,084 INFO[org.apache.spark.executor.Executor:54] - Running task 45.0 in stage 8.0 (TID 72)
2018-02-06 15:01:06,087 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,087 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,087 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,087 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,091 INFO[org.apache.spark.executor.Executor:54] - Finished task 45.0 in stage 8.0 (TID 72). 2464 bytes result sent to driver
2018-02-06 15:01:06,084 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 43.0 in stage 8.0 (TID 70) in 14 ms on localhost (executor driver) (44/100)
2018-02-06 15:01:06,092 INFO[org.apache.spark.executor.Executor:54] - Finished task 44.0 in stage 8.0 (TID 71). 2421 bytes result sent to driver
2018-02-06 15:01:06,093 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 46.0 in stage 8.0 (TID 73, localhost, executor driver, partition 71, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,094 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 47.0 in stage 8.0 (TID 74, localhost, executor driver, partition 72, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,094 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 45.0 in stage 8.0 (TID 72) in 11 ms on localhost (executor driver) (45/100)
2018-02-06 15:01:06,094 INFO[org.apache.spark.executor.Executor:54] - Running task 46.0 in stage 8.0 (TID 73)
2018-02-06 15:01:06,094 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 44.0 in stage 8.0 (TID 71) in 12 ms on localhost (executor driver) (46/100)
2018-02-06 15:01:06,095 INFO[org.apache.spark.executor.Executor:54] - Running task 47.0 in stage 8.0 (TID 74)
2018-02-06 15:01:06,097 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,097 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,098 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,098 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,101 INFO[org.apache.spark.executor.Executor:54] - Finished task 47.0 in stage 8.0 (TID 74). 2421 bytes result sent to driver
2018-02-06 15:01:06,101 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 48.0 in stage 8.0 (TID 75, localhost, executor driver, partition 73, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,102 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 47.0 in stage 8.0 (TID 74) in 9 ms on localhost (executor driver) (47/100)
2018-02-06 15:01:06,102 INFO[org.apache.spark.executor.Executor:54] - Running task 48.0 in stage 8.0 (TID 75)
2018-02-06 15:01:06,119 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,119 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,121 INFO[org.apache.spark.executor.Executor:54] - Finished task 48.0 in stage 8.0 (TID 75). 2464 bytes result sent to driver
2018-02-06 15:01:06,122 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 49.0 in stage 8.0 (TID 76, localhost, executor driver, partition 74, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,122 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 48.0 in stage 8.0 (TID 75) in 21 ms on localhost (executor driver) (48/100)
2018-02-06 15:01:06,123 INFO[org.apache.spark.executor.Executor:54] - Running task 49.0 in stage 8.0 (TID 76)
2018-02-06 15:01:06,123 INFO[org.apache.spark.executor.Executor:54] - Finished task 46.0 in stage 8.0 (TID 73). 2421 bytes result sent to driver
2018-02-06 15:01:06,124 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 50.0 in stage 8.0 (TID 77, localhost, executor driver, partition 75, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,128 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,129 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,129 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 46.0 in stage 8.0 (TID 73) in 36 ms on localhost (executor driver) (49/100)
2018-02-06 15:01:06,129 INFO[org.apache.spark.executor.Executor:54] - Running task 50.0 in stage 8.0 (TID 77)
2018-02-06 15:01:06,132 INFO[org.apache.spark.executor.Executor:54] - Finished task 49.0 in stage 8.0 (TID 76). 2421 bytes result sent to driver
2018-02-06 15:01:06,133 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 51.0 in stage 8.0 (TID 78, localhost, executor driver, partition 76, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,133 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 49.0 in stage 8.0 (TID 76) in 11 ms on localhost (executor driver) (50/100)
2018-02-06 15:01:06,134 INFO[org.apache.spark.executor.Executor:54] - Running task 51.0 in stage 8.0 (TID 78)
2018-02-06 15:01:06,136 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,138 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
2018-02-06 15:01:06,142 INFO[org.apache.spark.executor.Executor:54] - Finished task 50.0 in stage 8.0 (TID 77). 2464 bytes result sent to driver
2018-02-06 15:01:06,144 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,144 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,144 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 52.0 in stage 8.0 (TID 79, localhost, executor driver, partition 77, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,144 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 50.0 in stage 8.0 (TID 77) in 20 ms on localhost (executor driver) (51/100)
2018-02-06 15:01:06,145 INFO[org.apache.spark.executor.Executor:54] - Running task 52.0 in stage 8.0 (TID 79)
2018-02-06 15:01:06,146 INFO[org.apache.spark.executor.Executor:54] - Finished task 51.0 in stage 8.0 (TID 78). 2421 bytes result sent to driver
2018-02-06 15:01:06,148 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 53.0 in stage 8.0 (TID 80, localhost, executor driver, partition 78, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,150 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 51.0 in stage 8.0 (TID 78) in 18 ms on localhost (executor driver) (52/100)
2018-02-06 15:01:06,150 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,150 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,154 INFO[org.apache.spark.executor.Executor:54] - Finished task 52.0 in stage 8.0 (TID 79). 2421 bytes result sent to driver
2018-02-06 15:01:06,156 INFO[org.apache.spark.executor.Executor:54] - Running task 53.0 in stage 8.0 (TID 80)
2018-02-06 15:01:06,157 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 54.0 in stage 8.0 (TID 81, localhost, executor driver, partition 79, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,157 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 52.0 in stage 8.0 (TID 79) in 14 ms on localhost (executor driver) (53/100)
2018-02-06 15:01:06,158 INFO[org.apache.spark.executor.Executor:54] - Running task 54.0 in stage 8.0 (TID 81)
2018-02-06 15:01:06,158 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,159 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,161 INFO[org.apache.spark.executor.Executor:54] - Finished task 53.0 in stage 8.0 (TID 80). 2421 bytes result sent to driver
2018-02-06 15:01:06,161 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,162 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,168 INFO[org.apache.spark.executor.Executor:54] - Finished task 54.0 in stage 8.0 (TID 81). 2421 bytes result sent to driver
2018-02-06 15:01:06,161 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 55.0 in stage 8.0 (TID 82, localhost, executor driver, partition 80, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,169 INFO[org.apache.spark.executor.Executor:54] - Running task 55.0 in stage 8.0 (TID 82)
2018-02-06 15:01:06,169 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 56.0 in stage 8.0 (TID 83, localhost, executor driver, partition 81, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,170 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 53.0 in stage 8.0 (TID 80) in 23 ms on localhost (executor driver) (54/100)
2018-02-06 15:01:06,170 INFO[org.apache.spark.executor.Executor:54] - Running task 56.0 in stage 8.0 (TID 83)
2018-02-06 15:01:06,171 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,171 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,173 INFO[org.apache.spark.executor.Executor:54] - Finished task 55.0 in stage 8.0 (TID 82). 2421 bytes result sent to driver
2018-02-06 15:01:06,173 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 54.0 in stage 8.0 (TID 81) in 16 ms on localhost (executor driver) (55/100)
2018-02-06 15:01:06,178 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,178 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
2018-02-06 15:01:06,180 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 57.0 in stage 8.0 (TID 84, localhost, executor driver, partition 82, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,180 INFO[org.apache.spark.executor.Executor:54] - Finished task 56.0 in stage 8.0 (TID 83). 2464 bytes result sent to driver
2018-02-06 15:01:06,182 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 55.0 in stage 8.0 (TID 82) in 21 ms on localhost (executor driver) (56/100)
2018-02-06 15:01:06,182 INFO[org.apache.spark.executor.Executor:54] - Running task 57.0 in stage 8.0 (TID 84)
2018-02-06 15:01:06,183 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 58.0 in stage 8.0 (TID 85, localhost, executor driver, partition 83, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,185 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,185 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,190 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 56.0 in stage 8.0 (TID 83) in 21 ms on localhost (executor driver) (57/100)
2018-02-06 15:01:06,190 INFO[org.apache.spark.executor.Executor:54] - Finished task 57.0 in stage 8.0 (TID 84). 2421 bytes result sent to driver
2018-02-06 15:01:06,190 INFO[org.apache.spark.executor.Executor:54] - Running task 58.0 in stage 8.0 (TID 85)
2018-02-06 15:01:06,192 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 59.0 in stage 8.0 (TID 86, localhost, executor driver, partition 84, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,193 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 57.0 in stage 8.0 (TID 84) in 15 ms on localhost (executor driver) (58/100)
2018-02-06 15:01:06,194 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,194 INFO[org.apache.spark.executor.Executor:54] - Running task 59.0 in stage 8.0 (TID 86)
2018-02-06 15:01:06,194 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,206 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,207 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,217 INFO[org.apache.spark.executor.Executor:54] - Finished task 59.0 in stage 8.0 (TID 86). 2464 bytes result sent to driver
2018-02-06 15:01:06,217 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 60.0 in stage 8.0 (TID 87, localhost, executor driver, partition 85, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,218 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 59.0 in stage 8.0 (TID 86) in 25 ms on localhost (executor driver) (59/100)
2018-02-06 15:01:06,218 INFO[org.apache.spark.executor.Executor:54] - Running task 60.0 in stage 8.0 (TID 87)
2018-02-06 15:01:06,219 INFO[org.apache.spark.executor.Executor:54] - Finished task 58.0 in stage 8.0 (TID 85). 2421 bytes result sent to driver
2018-02-06 15:01:06,220 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 61.0 in stage 8.0 (TID 88, localhost, executor driver, partition 86, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,220 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,221 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,221 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 58.0 in stage 8.0 (TID 85) in 38 ms on localhost (executor driver) (60/100)
2018-02-06 15:01:06,221 INFO[org.apache.spark.executor.Executor:54] - Running task 61.0 in stage 8.0 (TID 88)
2018-02-06 15:01:06,223 INFO[org.apache.spark.executor.Executor:54] - Finished task 60.0 in stage 8.0 (TID 87). 2421 bytes result sent to driver
2018-02-06 15:01:06,223 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 62.0 in stage 8.0 (TID 89, localhost, executor driver, partition 87, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,224 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 60.0 in stage 8.0 (TID 87) in 7 ms on localhost (executor driver) (61/100)
2018-02-06 15:01:06,224 INFO[org.apache.spark.executor.Executor:54] - Running task 62.0 in stage 8.0 (TID 89)
2018-02-06 15:01:06,225 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,225 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,229 INFO[org.apache.spark.executor.Executor:54] - Finished task 61.0 in stage 8.0 (TID 88). 2464 bytes result sent to driver
2018-02-06 15:01:06,229 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,230 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,230 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 63.0 in stage 8.0 (TID 90, localhost, executor driver, partition 88, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,230 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 61.0 in stage 8.0 (TID 88) in 10 ms on localhost (executor driver) (62/100)
2018-02-06 15:01:06,231 INFO[org.apache.spark.executor.Executor:54] - Running task 63.0 in stage 8.0 (TID 90)
2018-02-06 15:01:06,232 INFO[org.apache.spark.executor.Executor:54] - Finished task 62.0 in stage 8.0 (TID 89). 2421 bytes result sent to driver
2018-02-06 15:01:06,233 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 64.0 in stage 8.0 (TID 91, localhost, executor driver, partition 89, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,234 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 62.0 in stage 8.0 (TID 89) in 11 ms on localhost (executor driver) (63/100)
2018-02-06 15:01:06,234 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,234 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,235 INFO[org.apache.spark.executor.Executor:54] - Running task 64.0 in stage 8.0 (TID 91)
2018-02-06 15:01:06,237 INFO[org.apache.spark.executor.Executor:54] - Finished task 63.0 in stage 8.0 (TID 90). 2421 bytes result sent to driver
2018-02-06 15:01:06,237 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,238 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,240 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 65.0 in stage 8.0 (TID 92, localhost, executor driver, partition 90, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,240 INFO[org.apache.spark.executor.Executor:54] - Finished task 64.0 in stage 8.0 (TID 91). 2421 bytes result sent to driver
2018-02-06 15:01:06,240 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 63.0 in stage 8.0 (TID 90) in 10 ms on localhost (executor driver) (64/100)
2018-02-06 15:01:06,240 INFO[org.apache.spark.executor.Executor:54] - Running task 65.0 in stage 8.0 (TID 92)
2018-02-06 15:01:06,240 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 66.0 in stage 8.0 (TID 93, localhost, executor driver, partition 91, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 64.0 in stage 8.0 (TID 91) in 8 ms on localhost (executor driver) (65/100)
2018-02-06 15:01:06,241 INFO[org.apache.spark.executor.Executor:54] - Running task 66.0 in stage 8.0 (TID 93)
2018-02-06 15:01:06,244 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,244 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,246 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,246 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,252 INFO[org.apache.spark.executor.Executor:54] - Finished task 66.0 in stage 8.0 (TID 93). 2421 bytes result sent to driver
2018-02-06 15:01:06,253 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 67.0 in stage 8.0 (TID 94, localhost, executor driver, partition 92, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,254 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 66.0 in stage 8.0 (TID 93) in 14 ms on localhost (executor driver) (66/100)
2018-02-06 15:01:06,255 INFO[org.apache.spark.executor.Executor:54] - Running task 67.0 in stage 8.0 (TID 94)
2018-02-06 15:01:06,255 INFO[org.apache.spark.executor.Executor:54] - Finished task 65.0 in stage 8.0 (TID 92). 2464 bytes result sent to driver
2018-02-06 15:01:06,256 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 68.0 in stage 8.0 (TID 95, localhost, executor driver, partition 93, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,258 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 65.0 in stage 8.0 (TID 92) in 18 ms on localhost (executor driver) (67/100)
2018-02-06 15:01:06,258 INFO[org.apache.spark.executor.Executor:54] - Running task 68.0 in stage 8.0 (TID 95)
2018-02-06 15:01:06,260 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,260 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,260 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,260 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,266 INFO[org.apache.spark.executor.Executor:54] - Finished task 68.0 in stage 8.0 (TID 95). 2421 bytes result sent to driver
2018-02-06 15:01:06,272 INFO[org.apache.spark.executor.Executor:54] - Finished task 67.0 in stage 8.0 (TID 94). 2464 bytes result sent to driver
2018-02-06 15:01:06,273 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 69.0 in stage 8.0 (TID 96, localhost, executor driver, partition 94, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,275 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 68.0 in stage 8.0 (TID 95) in 18 ms on localhost (executor driver) (68/100)
2018-02-06 15:01:06,275 INFO[org.apache.spark.executor.Executor:54] - Running task 69.0 in stage 8.0 (TID 96)
2018-02-06 15:01:06,279 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,279 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,287 INFO[org.apache.spark.executor.Executor:54] - Finished task 69.0 in stage 8.0 (TID 96). 2421 bytes result sent to driver
2018-02-06 15:01:06,287 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 70.0 in stage 8.0 (TID 97, localhost, executor driver, partition 95, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,288 INFO[org.apache.spark.executor.Executor:54] - Running task 70.0 in stage 8.0 (TID 97)
2018-02-06 15:01:06,288 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 69.0 in stage 8.0 (TID 96) in 15 ms on localhost (executor driver) (69/100)
2018-02-06 15:01:06,289 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 71.0 in stage 8.0 (TID 98, localhost, executor driver, partition 96, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,289 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 67.0 in stage 8.0 (TID 94) in 36 ms on localhost (executor driver) (70/100)
2018-02-06 15:01:06,290 INFO[org.apache.spark.executor.Executor:54] - Running task 71.0 in stage 8.0 (TID 98)
2018-02-06 15:01:06,291 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,291 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,293 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,293 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,295 INFO[org.apache.spark.executor.Executor:54] - Finished task 71.0 in stage 8.0 (TID 98). 2421 bytes result sent to driver
2018-02-06 15:01:06,296 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 72.0 in stage 8.0 (TID 99, localhost, executor driver, partition 97, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,296 INFO[org.apache.spark.executor.Executor:54] - Finished task 70.0 in stage 8.0 (TID 97). 2464 bytes result sent to driver
2018-02-06 15:01:06,296 INFO[org.apache.spark.executor.Executor:54] - Running task 72.0 in stage 8.0 (TID 99)
2018-02-06 15:01:06,296 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 71.0 in stage 8.0 (TID 98) in 8 ms on localhost (executor driver) (71/100)
2018-02-06 15:01:06,298 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 73.0 in stage 8.0 (TID 100, localhost, executor driver, partition 98, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,299 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 70.0 in stage 8.0 (TID 97) in 12 ms on localhost (executor driver) (72/100)
2018-02-06 15:01:06,300 INFO[org.apache.spark.executor.Executor:54] - Running task 73.0 in stage 8.0 (TID 100)
2018-02-06 15:01:06,300 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,301 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,303 INFO[org.apache.spark.executor.Executor:54] - Finished task 72.0 in stage 8.0 (TID 99). 2421 bytes result sent to driver
2018-02-06 15:01:06,304 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,304 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,304 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 75.0 in stage 8.0 (TID 101, localhost, executor driver, partition 100, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,305 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 72.0 in stage 8.0 (TID 99) in 9 ms on localhost (executor driver) (73/100)
2018-02-06 15:01:06,307 INFO[org.apache.spark.executor.Executor:54] - Finished task 73.0 in stage 8.0 (TID 100). 2464 bytes result sent to driver
2018-02-06 15:01:06,307 INFO[org.apache.spark.executor.Executor:54] - Running task 75.0 in stage 8.0 (TID 101)
2018-02-06 15:01:06,308 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 76.0 in stage 8.0 (TID 102, localhost, executor driver, partition 101, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,309 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 73.0 in stage 8.0 (TID 100) in 11 ms on localhost (executor driver) (74/100)
2018-02-06 15:01:06,309 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,309 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,309 INFO[org.apache.spark.executor.Executor:54] - Running task 76.0 in stage 8.0 (TID 102)
2018-02-06 15:01:06,315 INFO[org.apache.spark.executor.Executor:54] - Finished task 75.0 in stage 8.0 (TID 101). 2464 bytes result sent to driver
2018-02-06 15:01:06,315 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,315 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,317 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 77.0 in stage 8.0 (TID 103, localhost, executor driver, partition 102, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,317 INFO[org.apache.spark.executor.Executor:54] - Finished task 76.0 in stage 8.0 (TID 102). 2421 bytes result sent to driver
2018-02-06 15:01:06,319 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 75.0 in stage 8.0 (TID 101) in 15 ms on localhost (executor driver) (75/100)
2018-02-06 15:01:06,317 INFO[org.apache.spark.executor.Executor:54] - Running task 77.0 in stage 8.0 (TID 103)
2018-02-06 15:01:06,336 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,337 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,339 INFO[org.apache.spark.executor.Executor:54] - Finished task 77.0 in stage 8.0 (TID 103). 2421 bytes result sent to driver
2018-02-06 15:01:06,340 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 78.0 in stage 8.0 (TID 104, localhost, executor driver, partition 103, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,340 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 79.0 in stage 8.0 (TID 105, localhost, executor driver, partition 104, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,347 INFO[org.apache.spark.executor.Executor:54] - Running task 78.0 in stage 8.0 (TID 104)
2018-02-06 15:01:06,347 INFO[org.apache.spark.executor.Executor:54] - Running task 79.0 in stage 8.0 (TID 105)
2018-02-06 15:01:06,351 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,352 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,347 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 76.0 in stage 8.0 (TID 102) in 39 ms on localhost (executor driver) (76/100)
2018-02-06 15:01:06,352 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,352 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
2018-02-06 15:01:06,357 INFO[org.apache.spark.executor.Executor:54] - Finished task 78.0 in stage 8.0 (TID 104). 2464 bytes result sent to driver
2018-02-06 15:01:06,358 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 77.0 in stage 8.0 (TID 103) in 42 ms on localhost (executor driver) (77/100)
2018-02-06 15:01:06,359 INFO[org.apache.spark.executor.Executor:54] - Finished task 79.0 in stage 8.0 (TID 105). 2464 bytes result sent to driver
2018-02-06 15:01:06,359 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 80.0 in stage 8.0 (TID 106, localhost, executor driver, partition 105, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,359 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 81.0 in stage 8.0 (TID 107, localhost, executor driver, partition 106, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,359 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 78.0 in stage 8.0 (TID 104) in 19 ms on localhost (executor driver) (78/100)
2018-02-06 15:01:06,360 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 79.0 in stage 8.0 (TID 105) in 20 ms on localhost (executor driver) (79/100)
2018-02-06 15:01:06,360 INFO[org.apache.spark.executor.Executor:54] - Running task 80.0 in stage 8.0 (TID 106)
2018-02-06 15:01:06,369 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,369 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,371 INFO[org.apache.spark.executor.Executor:54] - Running task 81.0 in stage 8.0 (TID 107)
2018-02-06 15:01:06,373 INFO[org.apache.spark.executor.Executor:54] - Finished task 80.0 in stage 8.0 (TID 106). 2421 bytes result sent to driver
2018-02-06 15:01:06,376 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,377 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,377 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 82.0 in stage 8.0 (TID 108, localhost, executor driver, partition 107, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,382 INFO[org.apache.spark.executor.Executor:54] - Finished task 81.0 in stage 8.0 (TID 107). 2421 bytes result sent to driver
2018-02-06 15:01:06,385 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 83.0 in stage 8.0 (TID 109, localhost, executor driver, partition 108, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,385 INFO[org.apache.spark.executor.Executor:54] - Running task 82.0 in stage 8.0 (TID 108)
2018-02-06 15:01:06,385 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 81.0 in stage 8.0 (TID 107) in 26 ms on localhost (executor driver) (80/100)
2018-02-06 15:01:06,387 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 80.0 in stage 8.0 (TID 106) in 28 ms on localhost (executor driver) (81/100)
2018-02-06 15:01:06,387 INFO[org.apache.spark.executor.Executor:54] - Running task 83.0 in stage 8.0 (TID 109)
2018-02-06 15:01:06,388 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,390 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 3 ms
2018-02-06 15:01:06,393 INFO[org.apache.spark.executor.Executor:54] - Finished task 82.0 in stage 8.0 (TID 108). 2421 bytes result sent to driver
2018-02-06 15:01:06,394 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 85.0 in stage 8.0 (TID 110, localhost, executor driver, partition 110, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,395 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 82.0 in stage 8.0 (TID 108) in 22 ms on localhost (executor driver) (82/100)
2018-02-06 15:01:06,396 INFO[org.apache.spark.executor.Executor:54] - Running task 85.0 in stage 8.0 (TID 110)
2018-02-06 15:01:06,401 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,404 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 3 ms
2018-02-06 15:01:06,408 INFO[org.apache.spark.executor.Executor:54] - Finished task 85.0 in stage 8.0 (TID 110). 2464 bytes result sent to driver
2018-02-06 15:01:06,409 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 86.0 in stage 8.0 (TID 111, localhost, executor driver, partition 111, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,409 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 85.0 in stage 8.0 (TID 110) in 15 ms on localhost (executor driver) (83/100)
2018-02-06 15:01:06,410 INFO[org.apache.spark.executor.Executor:54] - Running task 86.0 in stage 8.0 (TID 111)
2018-02-06 15:01:06,413 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,413 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,421 INFO[org.apache.spark.executor.Executor:54] - Finished task 86.0 in stage 8.0 (TID 111). 2464 bytes result sent to driver
2018-02-06 15:01:06,422 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 87.0 in stage 8.0 (TID 112, localhost, executor driver, partition 112, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,423 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 86.0 in stage 8.0 (TID 111) in 13 ms on localhost (executor driver) (84/100)
2018-02-06 15:01:06,423 INFO[org.apache.spark.executor.Executor:54] - Running task 87.0 in stage 8.0 (TID 112)
2018-02-06 15:01:06,425 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,426 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,429 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,429 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,434 INFO[org.apache.spark.executor.Executor:54] - Finished task 83.0 in stage 8.0 (TID 109). 2464 bytes result sent to driver
2018-02-06 15:01:06,435 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 88.0 in stage 8.0 (TID 113, localhost, executor driver, partition 113, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,436 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 83.0 in stage 8.0 (TID 109) in 53 ms on localhost (executor driver) (85/100)
2018-02-06 15:01:06,436 INFO[org.apache.spark.executor.Executor:54] - Running task 88.0 in stage 8.0 (TID 113)
2018-02-06 15:01:06,440 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,440 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,442 INFO[org.apache.spark.executor.Executor:54] - Finished task 88.0 in stage 8.0 (TID 113). 2421 bytes result sent to driver
2018-02-06 15:01:06,443 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 89.0 in stage 8.0 (TID 114, localhost, executor driver, partition 114, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,443 INFO[org.apache.spark.executor.Executor:54] - Running task 89.0 in stage 8.0 (TID 114)
2018-02-06 15:01:06,443 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 88.0 in stage 8.0 (TID 113) in 8 ms on localhost (executor driver) (86/100)
2018-02-06 15:01:06,446 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,447 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
2018-02-06 15:01:06,450 INFO[org.apache.spark.executor.Executor:54] - Finished task 89.0 in stage 8.0 (TID 114). 2507 bytes result sent to driver
2018-02-06 15:01:06,453 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 90.0 in stage 8.0 (TID 115, localhost, executor driver, partition 115, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,453 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 89.0 in stage 8.0 (TID 114) in 10 ms on localhost (executor driver) (87/100)
2018-02-06 15:01:06,454 INFO[org.apache.spark.executor.Executor:54] - Running task 90.0 in stage 8.0 (TID 115)
2018-02-06 15:01:06,456 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,457 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,459 INFO[org.apache.spark.executor.Executor:54] - Finished task 90.0 in stage 8.0 (TID 115). 2421 bytes result sent to driver
2018-02-06 15:01:06,460 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 91.0 in stage 8.0 (TID 116, localhost, executor driver, partition 116, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,460 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 90.0 in stage 8.0 (TID 115) in 7 ms on localhost (executor driver) (88/100)
2018-02-06 15:01:06,461 INFO[org.apache.spark.executor.Executor:54] - Running task 91.0 in stage 8.0 (TID 116)
2018-02-06 15:01:06,463 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,463 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,468 INFO[org.apache.spark.executor.Executor:54] - Finished task 91.0 in stage 8.0 (TID 116). 2421 bytes result sent to driver
2018-02-06 15:01:06,469 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 92.0 in stage 8.0 (TID 117, localhost, executor driver, partition 117, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,469 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 91.0 in stage 8.0 (TID 116) in 9 ms on localhost (executor driver) (89/100)
2018-02-06 15:01:06,469 INFO[org.apache.spark.executor.Executor:54] - Running task 92.0 in stage 8.0 (TID 117)
2018-02-06 15:01:06,472 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,472 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,474 INFO[org.apache.spark.executor.Executor:54] - Finished task 92.0 in stage 8.0 (TID 117). 2421 bytes result sent to driver
2018-02-06 15:01:06,474 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 93.0 in stage 8.0 (TID 118, localhost, executor driver, partition 118, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,475 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 92.0 in stage 8.0 (TID 117) in 5 ms on localhost (executor driver) (90/100)
2018-02-06 15:01:06,475 INFO[org.apache.spark.executor.Executor:54] - Running task 93.0 in stage 8.0 (TID 118)
2018-02-06 15:01:06,479 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,479 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,483 INFO[org.apache.spark.executor.Executor:54] - Finished task 93.0 in stage 8.0 (TID 118). 2464 bytes result sent to driver
2018-02-06 15:01:06,484 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 94.0 in stage 8.0 (TID 119, localhost, executor driver, partition 119, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,484 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 93.0 in stage 8.0 (TID 118) in 10 ms on localhost (executor driver) (91/100)
2018-02-06 15:01:06,484 INFO[org.apache.spark.executor.Executor:54] - Running task 94.0 in stage 8.0 (TID 119)
2018-02-06 15:01:06,488 INFO[org.apache.spark.executor.Executor:54] - Finished task 87.0 in stage 8.0 (TID 112). 2421 bytes result sent to driver
2018-02-06 15:01:06,489 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 95.0 in stage 8.0 (TID 120, localhost, executor driver, partition 120, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,489 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 87.0 in stage 8.0 (TID 112) in 67 ms on localhost (executor driver) (92/100)
2018-02-06 15:01:06,489 INFO[org.apache.spark.executor.Executor:54] - Running task 95.0 in stage 8.0 (TID 120)
2018-02-06 15:01:06,494 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,494 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,496 INFO[org.apache.spark.executor.Executor:54] - Finished task 95.0 in stage 8.0 (TID 120). 2464 bytes result sent to driver
2018-02-06 15:01:06,498 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 96.0 in stage 8.0 (TID 121, localhost, executor driver, partition 121, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,498 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 95.0 in stage 8.0 (TID 120) in 9 ms on localhost (executor driver) (93/100)
2018-02-06 15:01:06,499 INFO[org.apache.spark.executor.Executor:54] - Running task 96.0 in stage 8.0 (TID 121)
2018-02-06 15:01:06,501 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,501 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,503 INFO[org.apache.spark.executor.Executor:54] - Finished task 96.0 in stage 8.0 (TID 121). 2421 bytes result sent to driver
2018-02-06 15:01:06,504 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 97.0 in stage 8.0 (TID 122, localhost, executor driver, partition 122, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,504 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 96.0 in stage 8.0 (TID 121) in 8 ms on localhost (executor driver) (94/100)
2018-02-06 15:01:06,505 INFO[org.apache.spark.executor.Executor:54] - Running task 97.0 in stage 8.0 (TID 122)
2018-02-06 15:01:06,507 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,507 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,509 INFO[org.apache.spark.executor.Executor:54] - Finished task 97.0 in stage 8.0 (TID 122). 2464 bytes result sent to driver
2018-02-06 15:01:06,509 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 98.0 in stage 8.0 (TID 123, localhost, executor driver, partition 123, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,510 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 97.0 in stage 8.0 (TID 122) in 6 ms on localhost (executor driver) (95/100)
2018-02-06 15:01:06,510 INFO[org.apache.spark.executor.Executor:54] - Running task 98.0 in stage 8.0 (TID 123)
2018-02-06 15:01:06,516 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,517 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,518 INFO[org.apache.spark.executor.Executor:54] - Finished task 98.0 in stage 8.0 (TID 123). 2421 bytes result sent to driver
2018-02-06 15:01:06,519 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 99.0 in stage 8.0 (TID 124, localhost, executor driver, partition 124, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,519 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 98.0 in stage 8.0 (TID 123) in 10 ms on localhost (executor driver) (96/100)
2018-02-06 15:01:06,519 INFO[org.apache.spark.executor.Executor:54] - Running task 99.0 in stage 8.0 (TID 124)
2018-02-06 15:01:06,521 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,522 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,523 INFO[org.apache.spark.executor.Executor:54] - Finished task 99.0 in stage 8.0 (TID 124). 2421 bytes result sent to driver
2018-02-06 15:01:06,524 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 74.0 in stage 8.0 (TID 125, localhost, executor driver, partition 99, ANY, 4726 bytes)
2018-02-06 15:01:06,524 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 99.0 in stage 8.0 (TID 124) in 5 ms on localhost (executor driver) (97/100)
2018-02-06 15:01:06,525 INFO[org.apache.spark.executor.Executor:54] - Running task 74.0 in stage 8.0 (TID 125)
2018-02-06 15:01:06,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,528 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,543 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,544 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,545 INFO[org.apache.spark.executor.Executor:54] - Finished task 94.0 in stage 8.0 (TID 119). 2421 bytes result sent to driver
2018-02-06 15:01:06,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 84.0 in stage 8.0 (TID 126, localhost, executor driver, partition 109, ANY, 4726 bytes)
2018-02-06 15:01:06,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 94.0 in stage 8.0 (TID 119) in 62 ms on localhost (executor driver) (98/100)
2018-02-06 15:01:06,546 INFO[org.apache.spark.executor.Executor:54] - Running task 84.0 in stage 8.0 (TID 126)
2018-02-06 15:01:06,555 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,555 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,606 INFO[org.apache.spark.executor.Executor:54] - Finished task 74.0 in stage 8.0 (TID 125). 2536 bytes result sent to driver
2018-02-06 15:01:06,609 INFO[org.apache.spark.executor.Executor:54] - Finished task 84.0 in stage 8.0 (TID 126). 2536 bytes result sent to driver
2018-02-06 15:01:06,611 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_6_piece0 on 192.168.11.26:59860 in memory (size: 11.6 KB, free: 631.7 MB)
2018-02-06 15:01:06,612 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 74.0 in stage 8.0 (TID 125) in 88 ms on localhost (executor driver) (99/100)
2018-02-06 15:01:06,612 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 84.0 in stage 8.0 (TID 126) in 66 ms on localhost (executor driver) (100/100)
2018-02-06 15:01:06,612 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 8.0, whose tasks have all completed, from pool 
2018-02-06 15:01:06,615 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 8 (show at BaseSparkSQL.java:35) finished in 0.903 s
2018-02-06 15:01:06,616 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 4 finished: show at BaseSparkSQL.java:35, took 0.938562 s
2018-02-06 15:01:06,621 INFO[org.apache.spark.SparkContext:54] - Starting job: show at BaseSparkSQL.java:35
2018-02-06 15:01:06,625 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_5_piece0 on 192.168.11.26:59860 in memory (size: 11.6 KB, free: 631.7 MB)
2018-02-06 15:01:06,627 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 5 (show at BaseSparkSQL.java:35) with 75 output partitions
2018-02-06 15:01:06,627 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 10 (show at BaseSparkSQL.java:35)
2018-02-06 15:01:06,627 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 9)
2018-02-06 15:01:06,627 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 15:01:06,627 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_4_piece0 on 192.168.11.26:59860 in memory (size: 11.6 KB, free: 631.7 MB)
2018-02-06 15:01:06,627 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 10 (MapPartitionsRDD[9] at show at BaseSparkSQL.java:35), which has no missing parents
2018-02-06 15:01:06,633 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_8 stored as values in memory (estimated size 23.3 KB, free 631.1 MB)
2018-02-06 15:01:06,645 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_8_piece0 stored as bytes in memory (estimated size 11.6 KB, free 631.0 MB)
2018-02-06 15:01:06,646 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_8_piece0 in memory on 192.168.11.26:59860 (size: 11.6 KB, free: 631.7 MB)
2018-02-06 15:01:06,648 INFO[org.apache.spark.SparkContext:54] - Created broadcast 8 from broadcast at DAGScheduler.scala:1006
2018-02-06 15:01:06,649 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 75 missing tasks from ResultStage 10 (MapPartitionsRDD[9] at show at BaseSparkSQL.java:35) (first 15 tasks are for partitions Vector(125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139))
2018-02-06 15:01:06,649 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 10.0 with 75 tasks
2018-02-06 15:01:06,653 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 10.0 (TID 127, localhost, executor driver, partition 125, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,654 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 10.0 (TID 128, localhost, executor driver, partition 126, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,655 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 10.0 (TID 128)
2018-02-06 15:01:06,656 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 10.0 (TID 127)
2018-02-06 15:01:06,665 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,665 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
2018-02-06 15:01:06,659 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,666 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
2018-02-06 15:01:06,669 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 10.0 (TID 127). 2421 bytes result sent to driver
2018-02-06 15:01:06,669 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 10.0 (TID 128). 2421 bytes result sent to driver
2018-02-06 15:01:06,670 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 10.0 (TID 129, localhost, executor driver, partition 127, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,670 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 10.0 (TID 130, localhost, executor driver, partition 128, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,670 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 10.0 (TID 127) in 18 ms on localhost (executor driver) (1/75)
2018-02-06 15:01:06,671 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 10.0 (TID 128) in 18 ms on localhost (executor driver) (2/75)
2018-02-06 15:01:06,671 INFO[org.apache.spark.executor.Executor:54] - Running task 2.0 in stage 10.0 (TID 129)
2018-02-06 15:01:06,672 INFO[org.apache.spark.executor.Executor:54] - Running task 3.0 in stage 10.0 (TID 130)
2018-02-06 15:01:06,673 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,673 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,677 INFO[org.apache.spark.executor.Executor:54] - Finished task 2.0 in stage 10.0 (TID 129). 2421 bytes result sent to driver
2018-02-06 15:01:06,677 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,677 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,678 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 10.0 (TID 131, localhost, executor driver, partition 129, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,678 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 10.0 (TID 129) in 8 ms on localhost (executor driver) (3/75)
2018-02-06 15:01:06,679 INFO[org.apache.spark.executor.Executor:54] - Running task 4.0 in stage 10.0 (TID 131)
2018-02-06 15:01:06,679 INFO[org.apache.spark.executor.Executor:54] - Finished task 3.0 in stage 10.0 (TID 130). 2421 bytes result sent to driver
2018-02-06 15:01:06,679 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 10.0 (TID 132, localhost, executor driver, partition 130, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,680 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 10.0 (TID 130) in 10 ms on localhost (executor driver) (4/75)
2018-02-06 15:01:06,682 INFO[org.apache.spark.executor.Executor:54] - Running task 5.0 in stage 10.0 (TID 132)
2018-02-06 15:01:06,682 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,683 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,684 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,685 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,686 INFO[org.apache.spark.executor.Executor:54] - Finished task 4.0 in stage 10.0 (TID 131). 2464 bytes result sent to driver
2018-02-06 15:01:06,688 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 10.0 (TID 133, localhost, executor driver, partition 131, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,688 INFO[org.apache.spark.executor.Executor:54] - Finished task 5.0 in stage 10.0 (TID 132). 2421 bytes result sent to driver
2018-02-06 15:01:06,689 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 10.0 (TID 134, localhost, executor driver, partition 132, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,689 INFO[org.apache.spark.executor.Executor:54] - Running task 6.0 in stage 10.0 (TID 133)
2018-02-06 15:01:06,689 INFO[org.apache.spark.executor.Executor:54] - Running task 7.0 in stage 10.0 (TID 134)
2018-02-06 15:01:06,691 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,691 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,693 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,693 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,689 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 10.0 (TID 131) in 11 ms on localhost (executor driver) (5/75)
2018-02-06 15:01:06,693 INFO[org.apache.spark.executor.Executor:54] - Finished task 6.0 in stage 10.0 (TID 133). 2421 bytes result sent to driver
2018-02-06 15:01:06,695 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 10.0 (TID 135, localhost, executor driver, partition 133, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,695 INFO[org.apache.spark.executor.Executor:54] - Finished task 7.0 in stage 10.0 (TID 134). 2464 bytes result sent to driver
2018-02-06 15:01:06,695 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 10.0 (TID 132) in 16 ms on localhost (executor driver) (6/75)
2018-02-06 15:01:06,696 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 10.0 (TID 133) in 9 ms on localhost (executor driver) (7/75)
2018-02-06 15:01:06,696 INFO[org.apache.spark.executor.Executor:54] - Running task 8.0 in stage 10.0 (TID 135)
2018-02-06 15:01:06,697 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 10.0 (TID 136, localhost, executor driver, partition 134, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,698 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 10.0 (TID 134) in 9 ms on localhost (executor driver) (8/75)
2018-02-06 15:01:06,698 INFO[org.apache.spark.executor.Executor:54] - Running task 9.0 in stage 10.0 (TID 136)
2018-02-06 15:01:06,699 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,699 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,700 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,700 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,702 INFO[org.apache.spark.executor.Executor:54] - Finished task 9.0 in stage 10.0 (TID 136). 2421 bytes result sent to driver
2018-02-06 15:01:06,703 INFO[org.apache.spark.executor.Executor:54] - Finished task 8.0 in stage 10.0 (TID 135). 2421 bytes result sent to driver
2018-02-06 15:01:06,703 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 10.0 (TID 137, localhost, executor driver, partition 135, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,703 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 10.0 (TID 138, localhost, executor driver, partition 136, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,703 INFO[org.apache.spark.executor.Executor:54] - Running task 10.0 in stage 10.0 (TID 137)
2018-02-06 15:01:06,703 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 10.0 (TID 135) in 9 ms on localhost (executor driver) (9/75)
2018-02-06 15:01:06,704 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 10.0 (TID 136) in 7 ms on localhost (executor driver) (10/75)
2018-02-06 15:01:06,704 INFO[org.apache.spark.executor.Executor:54] - Running task 11.0 in stage 10.0 (TID 138)
2018-02-06 15:01:06,705 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,706 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,706 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,707 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,707 INFO[org.apache.spark.executor.Executor:54] - Finished task 10.0 in stage 10.0 (TID 137). 2421 bytes result sent to driver
2018-02-06 15:01:06,708 INFO[org.apache.spark.executor.Executor:54] - Finished task 11.0 in stage 10.0 (TID 138). 2378 bytes result sent to driver
2018-02-06 15:01:06,709 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 10.0 (TID 139, localhost, executor driver, partition 137, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,709 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 10.0 (TID 140, localhost, executor driver, partition 138, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,709 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 10.0 (TID 137) in 6 ms on localhost (executor driver) (11/75)
2018-02-06 15:01:06,710 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 10.0 (TID 138) in 7 ms on localhost (executor driver) (12/75)
2018-02-06 15:01:06,710 INFO[org.apache.spark.executor.Executor:54] - Running task 12.0 in stage 10.0 (TID 139)
2018-02-06 15:01:06,713 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,713 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,716 INFO[org.apache.spark.executor.Executor:54] - Finished task 12.0 in stage 10.0 (TID 139). 2464 bytes result sent to driver
2018-02-06 15:01:06,713 INFO[org.apache.spark.executor.Executor:54] - Running task 13.0 in stage 10.0 (TID 140)
2018-02-06 15:01:06,718 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 10.0 (TID 141, localhost, executor driver, partition 139, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,718 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 10.0 (TID 139) in 10 ms on localhost (executor driver) (13/75)
2018-02-06 15:01:06,719 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,719 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,719 INFO[org.apache.spark.executor.Executor:54] - Running task 14.0 in stage 10.0 (TID 141)
2018-02-06 15:01:06,720 INFO[org.apache.spark.executor.Executor:54] - Finished task 13.0 in stage 10.0 (TID 140). 2421 bytes result sent to driver
2018-02-06 15:01:06,721 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,721 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,721 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 10.0 (TID 142, localhost, executor driver, partition 140, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,722 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 10.0 (TID 140) in 13 ms on localhost (executor driver) (14/75)
2018-02-06 15:01:06,722 INFO[org.apache.spark.executor.Executor:54] - Running task 15.0 in stage 10.0 (TID 142)
2018-02-06 15:01:06,723 INFO[org.apache.spark.executor.Executor:54] - Finished task 14.0 in stage 10.0 (TID 141). 2421 bytes result sent to driver
2018-02-06 15:01:06,724 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 16.0 in stage 10.0 (TID 143, localhost, executor driver, partition 141, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,725 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 10.0 (TID 141) in 8 ms on localhost (executor driver) (15/75)
2018-02-06 15:01:06,725 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,725 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,727 INFO[org.apache.spark.executor.Executor:54] - Running task 16.0 in stage 10.0 (TID 143)
2018-02-06 15:01:06,731 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,731 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,733 INFO[org.apache.spark.executor.Executor:54] - Finished task 15.0 in stage 10.0 (TID 142). 2421 bytes result sent to driver
2018-02-06 15:01:06,733 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 17.0 in stage 10.0 (TID 144, localhost, executor driver, partition 142, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,734 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 10.0 (TID 142) in 13 ms on localhost (executor driver) (16/75)
2018-02-06 15:01:06,734 INFO[org.apache.spark.executor.Executor:54] - Running task 17.0 in stage 10.0 (TID 144)
2018-02-06 15:01:06,736 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,737 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,738 INFO[org.apache.spark.executor.Executor:54] - Finished task 17.0 in stage 10.0 (TID 144). 2421 bytes result sent to driver
2018-02-06 15:01:06,739 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 18.0 in stage 10.0 (TID 145, localhost, executor driver, partition 143, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,740 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 17.0 in stage 10.0 (TID 144) in 7 ms on localhost (executor driver) (17/75)
2018-02-06 15:01:06,740 INFO[org.apache.spark.executor.Executor:54] - Running task 18.0 in stage 10.0 (TID 145)
2018-02-06 15:01:06,743 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,743 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,750 INFO[org.apache.spark.executor.Executor:54] - Finished task 18.0 in stage 10.0 (TID 145). 2464 bytes result sent to driver
2018-02-06 15:01:06,751 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 19.0 in stage 10.0 (TID 146, localhost, executor driver, partition 144, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,751 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 18.0 in stage 10.0 (TID 145) in 12 ms on localhost (executor driver) (18/75)
2018-02-06 15:01:06,751 INFO[org.apache.spark.executor.Executor:54] - Running task 19.0 in stage 10.0 (TID 146)
2018-02-06 15:01:06,754 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,754 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,758 INFO[org.apache.spark.executor.Executor:54] - Finished task 19.0 in stage 10.0 (TID 146). 2421 bytes result sent to driver
2018-02-06 15:01:06,758 INFO[org.apache.spark.executor.Executor:54] - Finished task 16.0 in stage 10.0 (TID 143). 2421 bytes result sent to driver
2018-02-06 15:01:06,759 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 20.0 in stage 10.0 (TID 147, localhost, executor driver, partition 145, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,760 INFO[org.apache.spark.executor.Executor:54] - Running task 20.0 in stage 10.0 (TID 147)
2018-02-06 15:01:06,760 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 21.0 in stage 10.0 (TID 148, localhost, executor driver, partition 146, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,760 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 19.0 in stage 10.0 (TID 146) in 9 ms on localhost (executor driver) (19/75)
2018-02-06 15:01:06,761 INFO[org.apache.spark.executor.Executor:54] - Running task 21.0 in stage 10.0 (TID 148)
2018-02-06 15:01:06,763 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,765 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
2018-02-06 15:01:06,765 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 16.0 in stage 10.0 (TID 143) in 39 ms on localhost (executor driver) (20/75)
2018-02-06 15:01:06,765 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,765 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,766 INFO[org.apache.spark.executor.Executor:54] - Finished task 20.0 in stage 10.0 (TID 147). 2421 bytes result sent to driver
2018-02-06 15:01:06,767 INFO[org.apache.spark.executor.Executor:54] - Finished task 21.0 in stage 10.0 (TID 148). 2421 bytes result sent to driver
2018-02-06 15:01:06,767 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 22.0 in stage 10.0 (TID 149, localhost, executor driver, partition 147, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,767 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 23.0 in stage 10.0 (TID 150, localhost, executor driver, partition 148, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,767 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 20.0 in stage 10.0 (TID 147) in 8 ms on localhost (executor driver) (21/75)
2018-02-06 15:01:06,767 INFO[org.apache.spark.executor.Executor:54] - Running task 22.0 in stage 10.0 (TID 149)
2018-02-06 15:01:06,770 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,770 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,772 INFO[org.apache.spark.executor.Executor:54] - Running task 23.0 in stage 10.0 (TID 150)
2018-02-06 15:01:06,772 INFO[org.apache.spark.executor.Executor:54] - Finished task 22.0 in stage 10.0 (TID 149). 2421 bytes result sent to driver
2018-02-06 15:01:06,768 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 21.0 in stage 10.0 (TID 148) in 8 ms on localhost (executor driver) (22/75)
2018-02-06 15:01:06,773 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 24.0 in stage 10.0 (TID 151, localhost, executor driver, partition 149, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,774 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 22.0 in stage 10.0 (TID 149) in 7 ms on localhost (executor driver) (23/75)
2018-02-06 15:01:06,774 INFO[org.apache.spark.executor.Executor:54] - Running task 24.0 in stage 10.0 (TID 151)
2018-02-06 15:01:06,774 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,775 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,777 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,777 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,777 INFO[org.apache.spark.executor.Executor:54] - Finished task 23.0 in stage 10.0 (TID 150). 2464 bytes result sent to driver
2018-02-06 15:01:06,777 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 25.0 in stage 10.0 (TID 152, localhost, executor driver, partition 150, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,779 INFO[org.apache.spark.executor.Executor:54] - Finished task 24.0 in stage 10.0 (TID 151). 2421 bytes result sent to driver
2018-02-06 15:01:06,779 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 26.0 in stage 10.0 (TID 153, localhost, executor driver, partition 151, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,780 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 23.0 in stage 10.0 (TID 150) in 13 ms on localhost (executor driver) (24/75)
2018-02-06 15:01:06,782 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 24.0 in stage 10.0 (TID 151) in 8 ms on localhost (executor driver) (25/75)
2018-02-06 15:01:06,782 INFO[org.apache.spark.executor.Executor:54] - Running task 25.0 in stage 10.0 (TID 152)
2018-02-06 15:01:06,784 INFO[org.apache.spark.executor.Executor:54] - Running task 26.0 in stage 10.0 (TID 153)
2018-02-06 15:01:06,785 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,785 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,786 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,787 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,787 INFO[org.apache.spark.executor.Executor:54] - Finished task 25.0 in stage 10.0 (TID 152). 2421 bytes result sent to driver
2018-02-06 15:01:06,787 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 27.0 in stage 10.0 (TID 154, localhost, executor driver, partition 152, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,788 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 25.0 in stage 10.0 (TID 152) in 11 ms on localhost (executor driver) (26/75)
2018-02-06 15:01:06,788 INFO[org.apache.spark.executor.Executor:54] - Finished task 26.0 in stage 10.0 (TID 153). 2421 bytes result sent to driver
2018-02-06 15:01:06,788 INFO[org.apache.spark.executor.Executor:54] - Running task 27.0 in stage 10.0 (TID 154)
2018-02-06 15:01:06,788 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 28.0 in stage 10.0 (TID 155, localhost, executor driver, partition 153, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,789 INFO[org.apache.spark.executor.Executor:54] - Running task 28.0 in stage 10.0 (TID 155)
2018-02-06 15:01:06,789 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 26.0 in stage 10.0 (TID 153) in 10 ms on localhost (executor driver) (27/75)
2018-02-06 15:01:06,793 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,794 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,796 INFO[org.apache.spark.executor.Executor:54] - Finished task 28.0 in stage 10.0 (TID 155). 2421 bytes result sent to driver
2018-02-06 15:01:06,797 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 29.0 in stage 10.0 (TID 156, localhost, executor driver, partition 154, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,798 INFO[org.apache.spark.executor.Executor:54] - Running task 29.0 in stage 10.0 (TID 156)
2018-02-06 15:01:06,798 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 28.0 in stage 10.0 (TID 155) in 10 ms on localhost (executor driver) (28/75)
2018-02-06 15:01:06,799 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,799 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,800 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,800 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,801 INFO[org.apache.spark.executor.Executor:54] - Finished task 29.0 in stage 10.0 (TID 156). 2421 bytes result sent to driver
2018-02-06 15:01:06,802 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 30.0 in stage 10.0 (TID 157, localhost, executor driver, partition 155, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,802 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 29.0 in stage 10.0 (TID 156) in 5 ms on localhost (executor driver) (29/75)
2018-02-06 15:01:06,803 INFO[org.apache.spark.executor.Executor:54] - Running task 30.0 in stage 10.0 (TID 157)
2018-02-06 15:01:06,808 INFO[org.apache.spark.executor.Executor:54] - Finished task 27.0 in stage 10.0 (TID 154). 2464 bytes result sent to driver
2018-02-06 15:01:06,808 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,809 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,809 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 31.0 in stage 10.0 (TID 158, localhost, executor driver, partition 156, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,809 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 27.0 in stage 10.0 (TID 154) in 22 ms on localhost (executor driver) (30/75)
2018-02-06 15:01:06,809 INFO[org.apache.spark.executor.Executor:54] - Running task 31.0 in stage 10.0 (TID 158)
2018-02-06 15:01:06,811 INFO[org.apache.spark.executor.Executor:54] - Finished task 30.0 in stage 10.0 (TID 157). 2464 bytes result sent to driver
2018-02-06 15:01:06,815 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,815 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,821 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 32.0 in stage 10.0 (TID 159, localhost, executor driver, partition 157, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,822 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 30.0 in stage 10.0 (TID 157) in 20 ms on localhost (executor driver) (31/75)
2018-02-06 15:01:06,822 INFO[org.apache.spark.executor.Executor:54] - Running task 32.0 in stage 10.0 (TID 159)
2018-02-06 15:01:06,822 INFO[org.apache.spark.executor.Executor:54] - Finished task 31.0 in stage 10.0 (TID 158). 2421 bytes result sent to driver
2018-02-06 15:01:06,823 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 33.0 in stage 10.0 (TID 160, localhost, executor driver, partition 158, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,823 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 31.0 in stage 10.0 (TID 158) in 15 ms on localhost (executor driver) (32/75)
2018-02-06 15:01:06,823 INFO[org.apache.spark.executor.Executor:54] - Running task 33.0 in stage 10.0 (TID 160)
2018-02-06 15:01:06,824 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,824 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,825 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,826 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,826 INFO[org.apache.spark.executor.Executor:54] - Finished task 32.0 in stage 10.0 (TID 159). 2421 bytes result sent to driver
2018-02-06 15:01:06,826 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 34.0 in stage 10.0 (TID 161, localhost, executor driver, partition 159, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,827 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 32.0 in stage 10.0 (TID 159) in 7 ms on localhost (executor driver) (33/75)
2018-02-06 15:01:06,827 INFO[org.apache.spark.executor.Executor:54] - Running task 34.0 in stage 10.0 (TID 161)
2018-02-06 15:01:06,827 INFO[org.apache.spark.executor.Executor:54] - Finished task 33.0 in stage 10.0 (TID 160). 2421 bytes result sent to driver
2018-02-06 15:01:06,827 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 35.0 in stage 10.0 (TID 162, localhost, executor driver, partition 160, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,832 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,832 INFO[org.apache.spark.executor.Executor:54] - Running task 35.0 in stage 10.0 (TID 162)
2018-02-06 15:01:06,832 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 33.0 in stage 10.0 (TID 160) in 10 ms on localhost (executor driver) (34/75)
2018-02-06 15:01:06,832 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,834 INFO[org.apache.spark.executor.Executor:54] - Finished task 34.0 in stage 10.0 (TID 161). 2421 bytes result sent to driver
2018-02-06 15:01:06,834 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 36.0 in stage 10.0 (TID 163, localhost, executor driver, partition 161, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,834 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,835 INFO[org.apache.spark.executor.Executor:54] - Running task 36.0 in stage 10.0 (TID 163)
2018-02-06 15:01:06,835 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 34.0 in stage 10.0 (TID 161) in 9 ms on localhost (executor driver) (35/75)
2018-02-06 15:01:06,835 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,836 INFO[org.apache.spark.executor.Executor:54] - Finished task 35.0 in stage 10.0 (TID 162). 2464 bytes result sent to driver
2018-02-06 15:01:06,837 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,837 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,837 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 37.0 in stage 10.0 (TID 164, localhost, executor driver, partition 162, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,838 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 35.0 in stage 10.0 (TID 162) in 11 ms on localhost (executor driver) (36/75)
2018-02-06 15:01:06,838 INFO[org.apache.spark.executor.Executor:54] - Running task 37.0 in stage 10.0 (TID 164)
2018-02-06 15:01:06,839 INFO[org.apache.spark.executor.Executor:54] - Finished task 36.0 in stage 10.0 (TID 163). 2464 bytes result sent to driver
2018-02-06 15:01:06,839 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 38.0 in stage 10.0 (TID 165, localhost, executor driver, partition 163, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,840 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 36.0 in stage 10.0 (TID 163) in 6 ms on localhost (executor driver) (37/75)
2018-02-06 15:01:06,844 INFO[org.apache.spark.executor.Executor:54] - Running task 38.0 in stage 10.0 (TID 165)
2018-02-06 15:01:06,844 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,845 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,846 INFO[org.apache.spark.executor.Executor:54] - Finished task 37.0 in stage 10.0 (TID 164). 2421 bytes result sent to driver
2018-02-06 15:01:06,846 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,847 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,848 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 39.0 in stage 10.0 (TID 166, localhost, executor driver, partition 164, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,848 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 37.0 in stage 10.0 (TID 164) in 11 ms on localhost (executor driver) (38/75)
2018-02-06 15:01:06,848 INFO[org.apache.spark.executor.Executor:54] - Running task 39.0 in stage 10.0 (TID 166)
2018-02-06 15:01:06,849 INFO[org.apache.spark.executor.Executor:54] - Finished task 38.0 in stage 10.0 (TID 165). 2421 bytes result sent to driver
2018-02-06 15:01:06,850 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 40.0 in stage 10.0 (TID 167, localhost, executor driver, partition 165, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,850 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 38.0 in stage 10.0 (TID 165) in 11 ms on localhost (executor driver) (39/75)
2018-02-06 15:01:06,851 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,851 INFO[org.apache.spark.executor.Executor:54] - Running task 40.0 in stage 10.0 (TID 167)
2018-02-06 15:01:06,851 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,853 INFO[org.apache.spark.executor.Executor:54] - Finished task 39.0 in stage 10.0 (TID 166). 2464 bytes result sent to driver
2018-02-06 15:01:06,853 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 41.0 in stage 10.0 (TID 168, localhost, executor driver, partition 166, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,853 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,855 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
2018-02-06 15:01:06,858 INFO[org.apache.spark.executor.Executor:54] - Finished task 40.0 in stage 10.0 (TID 167). 2421 bytes result sent to driver
2018-02-06 15:01:06,858 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 39.0 in stage 10.0 (TID 166) in 11 ms on localhost (executor driver) (40/75)
2018-02-06 15:01:06,858 INFO[org.apache.spark.executor.Executor:54] - Running task 41.0 in stage 10.0 (TID 168)
2018-02-06 15:01:06,858 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 42.0 in stage 10.0 (TID 169, localhost, executor driver, partition 167, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,859 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 40.0 in stage 10.0 (TID 167) in 10 ms on localhost (executor driver) (41/75)
2018-02-06 15:01:06,859 INFO[org.apache.spark.executor.Executor:54] - Running task 42.0 in stage 10.0 (TID 169)
2018-02-06 15:01:06,861 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,861 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,862 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,861 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,863 INFO[org.apache.spark.executor.Executor:54] - Finished task 41.0 in stage 10.0 (TID 168). 2421 bytes result sent to driver
2018-02-06 15:01:06,865 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 43.0 in stage 10.0 (TID 170, localhost, executor driver, partition 168, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,865 INFO[org.apache.spark.executor.Executor:54] - Finished task 42.0 in stage 10.0 (TID 169). 2421 bytes result sent to driver
2018-02-06 15:01:06,866 INFO[org.apache.spark.executor.Executor:54] - Running task 43.0 in stage 10.0 (TID 170)
2018-02-06 15:01:06,865 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 41.0 in stage 10.0 (TID 168) in 12 ms on localhost (executor driver) (42/75)
2018-02-06 15:01:06,868 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 44.0 in stage 10.0 (TID 171, localhost, executor driver, partition 169, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,869 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,869 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,869 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 42.0 in stage 10.0 (TID 169) in 11 ms on localhost (executor driver) (43/75)
2018-02-06 15:01:06,869 INFO[org.apache.spark.executor.Executor:54] - Running task 44.0 in stage 10.0 (TID 171)
2018-02-06 15:01:06,870 INFO[org.apache.spark.executor.Executor:54] - Finished task 43.0 in stage 10.0 (TID 170). 2464 bytes result sent to driver
2018-02-06 15:01:06,872 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,873 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,876 INFO[org.apache.spark.executor.Executor:54] - Finished task 44.0 in stage 10.0 (TID 171). 2421 bytes result sent to driver
2018-02-06 15:01:06,877 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 45.0 in stage 10.0 (TID 172, localhost, executor driver, partition 170, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,877 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 46.0 in stage 10.0 (TID 173, localhost, executor driver, partition 171, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,878 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 43.0 in stage 10.0 (TID 170) in 14 ms on localhost (executor driver) (44/75)
2018-02-06 15:01:06,878 INFO[org.apache.spark.executor.Executor:54] - Running task 45.0 in stage 10.0 (TID 172)
2018-02-06 15:01:06,878 INFO[org.apache.spark.executor.Executor:54] - Running task 46.0 in stage 10.0 (TID 173)
2018-02-06 15:01:06,878 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 44.0 in stage 10.0 (TID 171) in 10 ms on localhost (executor driver) (45/75)
2018-02-06 15:01:06,881 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,882 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,883 INFO[org.apache.spark.executor.Executor:54] - Finished task 45.0 in stage 10.0 (TID 172). 2464 bytes result sent to driver
2018-02-06 15:01:06,883 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,883 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,885 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 47.0 in stage 10.0 (TID 174, localhost, executor driver, partition 172, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,885 INFO[org.apache.spark.executor.Executor:54] - Finished task 46.0 in stage 10.0 (TID 173). 2464 bytes result sent to driver
2018-02-06 15:01:06,886 INFO[org.apache.spark.executor.Executor:54] - Running task 47.0 in stage 10.0 (TID 174)
2018-02-06 15:01:06,885 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 45.0 in stage 10.0 (TID 172) in 8 ms on localhost (executor driver) (46/75)
2018-02-06 15:01:06,886 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 48.0 in stage 10.0 (TID 175, localhost, executor driver, partition 173, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,886 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 46.0 in stage 10.0 (TID 173) in 9 ms on localhost (executor driver) (47/75)
2018-02-06 15:01:06,886 INFO[org.apache.spark.executor.Executor:54] - Running task 48.0 in stage 10.0 (TID 175)
2018-02-06 15:01:06,889 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,889 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,889 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,889 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,890 INFO[org.apache.spark.executor.Executor:54] - Finished task 47.0 in stage 10.0 (TID 174). 2421 bytes result sent to driver
2018-02-06 15:01:06,890 INFO[org.apache.spark.executor.Executor:54] - Finished task 48.0 in stage 10.0 (TID 175). 2421 bytes result sent to driver
2018-02-06 15:01:06,891 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 49.0 in stage 10.0 (TID 176, localhost, executor driver, partition 174, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,891 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 50.0 in stage 10.0 (TID 177, localhost, executor driver, partition 175, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,891 INFO[org.apache.spark.executor.Executor:54] - Running task 49.0 in stage 10.0 (TID 176)
2018-02-06 15:01:06,891 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 48.0 in stage 10.0 (TID 175) in 5 ms on localhost (executor driver) (48/75)
2018-02-06 15:01:06,892 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 47.0 in stage 10.0 (TID 174) in 8 ms on localhost (executor driver) (49/75)
2018-02-06 15:01:06,892 INFO[org.apache.spark.executor.Executor:54] - Running task 50.0 in stage 10.0 (TID 177)
2018-02-06 15:01:06,893 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,893 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,895 INFO[org.apache.spark.executor.Executor:54] - Finished task 49.0 in stage 10.0 (TID 176). 2421 bytes result sent to driver
2018-02-06 15:01:06,896 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 51.0 in stage 10.0 (TID 178, localhost, executor driver, partition 176, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,893 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,896 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 3 ms
2018-02-06 15:01:06,897 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 49.0 in stage 10.0 (TID 176) in 6 ms on localhost (executor driver) (50/75)
2018-02-06 15:01:06,898 INFO[org.apache.spark.executor.Executor:54] - Running task 51.0 in stage 10.0 (TID 178)
2018-02-06 15:01:06,899 INFO[org.apache.spark.executor.Executor:54] - Finished task 50.0 in stage 10.0 (TID 177). 2464 bytes result sent to driver
2018-02-06 15:01:06,900 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 52.0 in stage 10.0 (TID 179, localhost, executor driver, partition 177, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,901 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 50.0 in stage 10.0 (TID 177) in 10 ms on localhost (executor driver) (51/75)
2018-02-06 15:01:06,901 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,901 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,901 INFO[org.apache.spark.executor.Executor:54] - Running task 52.0 in stage 10.0 (TID 179)
2018-02-06 15:01:06,903 INFO[org.apache.spark.executor.Executor:54] - Finished task 51.0 in stage 10.0 (TID 178). 2421 bytes result sent to driver
2018-02-06 15:01:06,906 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,906 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,907 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 53.0 in stage 10.0 (TID 180, localhost, executor driver, partition 178, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,907 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 51.0 in stage 10.0 (TID 178) in 11 ms on localhost (executor driver) (52/75)
2018-02-06 15:01:06,907 INFO[org.apache.spark.executor.Executor:54] - Finished task 52.0 in stage 10.0 (TID 179). 2421 bytes result sent to driver
2018-02-06 15:01:06,907 INFO[org.apache.spark.executor.Executor:54] - Running task 53.0 in stage 10.0 (TID 180)
2018-02-06 15:01:06,908 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 54.0 in stage 10.0 (TID 181, localhost, executor driver, partition 179, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,908 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 52.0 in stage 10.0 (TID 179) in 8 ms on localhost (executor driver) (53/75)
2018-02-06 15:01:06,908 INFO[org.apache.spark.executor.Executor:54] - Running task 54.0 in stage 10.0 (TID 181)
2018-02-06 15:01:06,910 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,910 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,912 INFO[org.apache.spark.executor.Executor:54] - Finished task 53.0 in stage 10.0 (TID 180). 2421 bytes result sent to driver
2018-02-06 15:01:06,914 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 55.0 in stage 10.0 (TID 182, localhost, executor driver, partition 180, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,915 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 53.0 in stage 10.0 (TID 180) in 9 ms on localhost (executor driver) (54/75)
2018-02-06 15:01:06,915 INFO[org.apache.spark.executor.Executor:54] - Running task 55.0 in stage 10.0 (TID 182)
2018-02-06 15:01:06,915 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,915 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,920 INFO[org.apache.spark.executor.Executor:54] - Finished task 54.0 in stage 10.0 (TID 181). 2464 bytes result sent to driver
2018-02-06 15:01:06,920 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,920 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,920 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 56.0 in stage 10.0 (TID 183, localhost, executor driver, partition 181, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,920 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 54.0 in stage 10.0 (TID 181) in 12 ms on localhost (executor driver) (55/75)
2018-02-06 15:01:06,921 INFO[org.apache.spark.executor.Executor:54] - Running task 56.0 in stage 10.0 (TID 183)
2018-02-06 15:01:06,921 INFO[org.apache.spark.executor.Executor:54] - Finished task 55.0 in stage 10.0 (TID 182). 2421 bytes result sent to driver
2018-02-06 15:01:06,922 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 57.0 in stage 10.0 (TID 184, localhost, executor driver, partition 182, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,922 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 55.0 in stage 10.0 (TID 182) in 8 ms on localhost (executor driver) (56/75)
2018-02-06 15:01:06,923 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,924 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,925 INFO[org.apache.spark.executor.Executor:54] - Running task 57.0 in stage 10.0 (TID 184)
2018-02-06 15:01:06,925 INFO[org.apache.spark.executor.Executor:54] - Finished task 56.0 in stage 10.0 (TID 183). 2421 bytes result sent to driver
2018-02-06 15:01:06,926 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 58.0 in stage 10.0 (TID 185, localhost, executor driver, partition 183, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,926 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 56.0 in stage 10.0 (TID 183) in 6 ms on localhost (executor driver) (57/75)
2018-02-06 15:01:06,926 INFO[org.apache.spark.executor.Executor:54] - Running task 58.0 in stage 10.0 (TID 185)
2018-02-06 15:01:06,927 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,928 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,928 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,928 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,930 INFO[org.apache.spark.executor.Executor:54] - Finished task 58.0 in stage 10.0 (TID 185). 2464 bytes result sent to driver
2018-02-06 15:01:06,931 INFO[org.apache.spark.executor.Executor:54] - Finished task 57.0 in stage 10.0 (TID 184). 2464 bytes result sent to driver
2018-02-06 15:01:06,931 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 59.0 in stage 10.0 (TID 186, localhost, executor driver, partition 184, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,932 INFO[org.apache.spark.executor.Executor:54] - Running task 59.0 in stage 10.0 (TID 186)
2018-02-06 15:01:06,932 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 60.0 in stage 10.0 (TID 187, localhost, executor driver, partition 185, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,932 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 57.0 in stage 10.0 (TID 184) in 10 ms on localhost (executor driver) (58/75)
2018-02-06 15:01:06,932 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 58.0 in stage 10.0 (TID 185) in 6 ms on localhost (executor driver) (59/75)
2018-02-06 15:01:06,933 INFO[org.apache.spark.executor.Executor:54] - Running task 60.0 in stage 10.0 (TID 187)
2018-02-06 15:01:06,934 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,934 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,934 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,934 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,935 INFO[org.apache.spark.executor.Executor:54] - Finished task 59.0 in stage 10.0 (TID 186). 2421 bytes result sent to driver
2018-02-06 15:01:06,937 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 61.0 in stage 10.0 (TID 188, localhost, executor driver, partition 186, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,937 INFO[org.apache.spark.executor.Executor:54] - Finished task 60.0 in stage 10.0 (TID 187). 2421 bytes result sent to driver
2018-02-06 15:01:06,938 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 62.0 in stage 10.0 (TID 189, localhost, executor driver, partition 187, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,938 INFO[org.apache.spark.executor.Executor:54] - Running task 61.0 in stage 10.0 (TID 188)
2018-02-06 15:01:06,943 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,943 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,949 INFO[org.apache.spark.executor.Executor:54] - Finished task 61.0 in stage 10.0 (TID 188). 2421 bytes result sent to driver
2018-02-06 15:01:06,950 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 60.0 in stage 10.0 (TID 187) in 18 ms on localhost (executor driver) (60/75)
2018-02-06 15:01:06,951 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 59.0 in stage 10.0 (TID 186) in 20 ms on localhost (executor driver) (61/75)
2018-02-06 15:01:06,951 INFO[org.apache.spark.executor.Executor:54] - Running task 62.0 in stage 10.0 (TID 189)
2018-02-06 15:01:06,953 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 63.0 in stage 10.0 (TID 190, localhost, executor driver, partition 188, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,954 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,954 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,962 INFO[org.apache.spark.executor.Executor:54] - Finished task 62.0 in stage 10.0 (TID 189). 2464 bytes result sent to driver
2018-02-06 15:01:06,962 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 64.0 in stage 10.0 (TID 191, localhost, executor driver, partition 189, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,964 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 61.0 in stage 10.0 (TID 188) in 26 ms on localhost (executor driver) (62/75)
2018-02-06 15:01:06,965 INFO[org.apache.spark.executor.Executor:54] - Running task 63.0 in stage 10.0 (TID 190)
2018-02-06 15:01:06,969 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 62.0 in stage 10.0 (TID 189) in 27 ms on localhost (executor driver) (63/75)
2018-02-06 15:01:06,971 INFO[org.apache.spark.executor.Executor:54] - Running task 64.0 in stage 10.0 (TID 191)
2018-02-06 15:01:06,976 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,977 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,979 INFO[org.apache.spark.executor.Executor:54] - Finished task 64.0 in stage 10.0 (TID 191). 2464 bytes result sent to driver
2018-02-06 15:01:06,982 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 65.0 in stage 10.0 (TID 192, localhost, executor driver, partition 190, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,983 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 64.0 in stage 10.0 (TID 191) in 21 ms on localhost (executor driver) (64/75)
2018-02-06 15:01:06,984 INFO[org.apache.spark.executor.Executor:54] - Running task 65.0 in stage 10.0 (TID 192)
2018-02-06 15:01:06,986 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,987 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,987 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:06,987 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,989 INFO[org.apache.spark.executor.Executor:54] - Finished task 65.0 in stage 10.0 (TID 192). 2421 bytes result sent to driver
2018-02-06 15:01:06,990 INFO[org.apache.spark.executor.Executor:54] - Finished task 63.0 in stage 10.0 (TID 190). 2421 bytes result sent to driver
2018-02-06 15:01:06,991 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 66.0 in stage 10.0 (TID 193, localhost, executor driver, partition 191, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,991 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 67.0 in stage 10.0 (TID 194, localhost, executor driver, partition 192, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:06,993 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 65.0 in stage 10.0 (TID 192) in 11 ms on localhost (executor driver) (65/75)
2018-02-06 15:01:06,993 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 63.0 in stage 10.0 (TID 190) in 40 ms on localhost (executor driver) (66/75)
2018-02-06 15:01:06,994 INFO[org.apache.spark.executor.Executor:54] - Running task 66.0 in stage 10.0 (TID 193)
2018-02-06 15:01:06,994 INFO[org.apache.spark.executor.Executor:54] - Running task 67.0 in stage 10.0 (TID 194)
2018-02-06 15:01:06,997 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,998 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:06,997 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:06,999 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 2 ms
2018-02-06 15:01:07,001 INFO[org.apache.spark.executor.Executor:54] - Finished task 66.0 in stage 10.0 (TID 193). 2421 bytes result sent to driver
2018-02-06 15:01:07,001 INFO[org.apache.spark.executor.Executor:54] - Finished task 67.0 in stage 10.0 (TID 194). 2421 bytes result sent to driver
2018-02-06 15:01:07,002 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 68.0 in stage 10.0 (TID 195, localhost, executor driver, partition 193, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:07,003 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 69.0 in stage 10.0 (TID 196, localhost, executor driver, partition 194, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:07,003 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 66.0 in stage 10.0 (TID 193) in 13 ms on localhost (executor driver) (67/75)
2018-02-06 15:01:07,004 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 67.0 in stage 10.0 (TID 194) in 13 ms on localhost (executor driver) (68/75)
2018-02-06 15:01:07,007 INFO[org.apache.spark.executor.Executor:54] - Running task 68.0 in stage 10.0 (TID 195)
2018-02-06 15:01:07,007 INFO[org.apache.spark.executor.Executor:54] - Running task 69.0 in stage 10.0 (TID 196)
2018-02-06 15:01:07,009 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:07,009 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:07,010 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:07,010 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:07,011 INFO[org.apache.spark.executor.Executor:54] - Finished task 68.0 in stage 10.0 (TID 195). 2464 bytes result sent to driver
2018-02-06 15:01:07,011 INFO[org.apache.spark.executor.Executor:54] - Finished task 69.0 in stage 10.0 (TID 196). 2464 bytes result sent to driver
2018-02-06 15:01:07,011 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 70.0 in stage 10.0 (TID 197, localhost, executor driver, partition 195, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:07,012 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 71.0 in stage 10.0 (TID 198, localhost, executor driver, partition 196, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:07,012 INFO[org.apache.spark.executor.Executor:54] - Running task 70.0 in stage 10.0 (TID 197)
2018-02-06 15:01:07,012 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 69.0 in stage 10.0 (TID 196) in 9 ms on localhost (executor driver) (69/75)
2018-02-06 15:01:07,012 INFO[org.apache.spark.executor.Executor:54] - Running task 71.0 in stage 10.0 (TID 198)
2018-02-06 15:01:07,012 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 68.0 in stage 10.0 (TID 195) in 10 ms on localhost (executor driver) (70/75)
2018-02-06 15:01:07,016 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:07,016 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:07,017 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:07,017 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 1 ms
2018-02-06 15:01:07,019 INFO[org.apache.spark.executor.Executor:54] - Finished task 70.0 in stage 10.0 (TID 197). 2421 bytes result sent to driver
2018-02-06 15:01:07,019 INFO[org.apache.spark.executor.Executor:54] - Finished task 71.0 in stage 10.0 (TID 198). 2421 bytes result sent to driver
2018-02-06 15:01:07,020 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 72.0 in stage 10.0 (TID 199, localhost, executor driver, partition 197, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:07,020 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 70.0 in stage 10.0 (TID 197) in 9 ms on localhost (executor driver) (71/75)
2018-02-06 15:01:07,021 INFO[org.apache.spark.executor.Executor:54] - Running task 72.0 in stage 10.0 (TID 199)
2018-02-06 15:01:07,021 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 71.0 in stage 10.0 (TID 198) in 9 ms on localhost (executor driver) (72/75)
2018-02-06 15:01:07,022 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 73.0 in stage 10.0 (TID 200, localhost, executor driver, partition 198, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:07,023 INFO[org.apache.spark.executor.Executor:54] - Running task 73.0 in stage 10.0 (TID 200)
2018-02-06 15:01:07,023 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:07,023 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:07,025 INFO[org.apache.spark.executor.Executor:54] - Finished task 72.0 in stage 10.0 (TID 199). 2464 bytes result sent to driver
2018-02-06 15:01:07,026 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 74.0 in stage 10.0 (TID 201, localhost, executor driver, partition 199, PROCESS_LOCAL, 4726 bytes)
2018-02-06 15:01:07,026 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 72.0 in stage 10.0 (TID 199) in 6 ms on localhost (executor driver) (73/75)
2018-02-06 15:01:07,027 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:07,027 INFO[org.apache.spark.executor.Executor:54] - Running task 74.0 in stage 10.0 (TID 201)
2018-02-06 15:01:07,027 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:07,029 INFO[org.apache.spark.executor.Executor:54] - Finished task 73.0 in stage 10.0 (TID 200). 2464 bytes result sent to driver
2018-02-06 15:01:07,029 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 0 non-empty blocks out of 1 blocks
2018-02-06 15:01:07,029 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-06 15:01:07,036 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 73.0 in stage 10.0 (TID 200) in 14 ms on localhost (executor driver) (74/75)
2018-02-06 15:01:07,036 INFO[org.apache.spark.executor.Executor:54] - Finished task 74.0 in stage 10.0 (TID 201). 2421 bytes result sent to driver
2018-02-06 15:01:07,037 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 74.0 in stage 10.0 (TID 201) in 11 ms on localhost (executor driver) (75/75)
2018-02-06 15:01:07,037 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2018-02-06 15:01:07,037 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 10 (show at BaseSparkSQL.java:35) finished in 0.386 s
2018-02-06 15:01:07,038 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 5 finished: show at BaseSparkSQL.java:35, took 0.415578 s
2018-02-06 15:01:07,067 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 9.173443 ms
2018-02-06 15:01:07,104 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 15:01:07,108 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@3bc53e30{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 15:01:07,111 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 15:01:07,120 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 15:01:07,230 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 15:01:07,230 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 15:01:07,232 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 15:01:07,234 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 15:01:07,240 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 15:01:07,240 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 15:01:07,242 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-f2794789-6fae-45e2-bee7-a964faf26769
2018-02-06 15:04:46,131 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 15:04:46,921 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 15:04:46,945 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 15:04:46,946 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 15:04:46,948 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 15:04:46,949 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 15:04:46,950 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 15:04:47,324 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 59970.
2018-02-06 15:04:47,345 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 15:04:47,395 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 15:04:47,398 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 15:04:47,399 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 15:04:47,408 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-1288ee01-107f-4726-b903-196e12b0980b
2018-02-06 15:04:47,431 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 15:04:47,480 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 15:04:47,557 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2712ms
2018-02-06 15:04:47,632 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 15:04:47,648 INFO[org.spark_project.jetty.server.Server:403] - Started @2803ms
2018-02-06 15:04:47,675 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@196ecc71{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 15:04:47,675 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 15:04:47,701 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,702 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,703 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,704 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,704 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/stages,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,705 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,705 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,706 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,707 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51850751{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,707 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64df9a61{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,708 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e260766{/storage,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,709 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a97744{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,709 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c56e013{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,710 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40258c2f{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,711 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6731787b{/environment,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,711 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,712 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/executors,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,712 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,713 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,714 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,720 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/static,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,720 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4bf3798b{/,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,721 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74e47444{/api,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,722 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46f699d5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,723 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1991f767{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 15:04:47,725 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 15:04:47,814 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 15:04:47,844 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59985.
2018-02-06 15:04:47,845 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:59985
2018-02-06 15:04:47,848 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 15:04:47,852 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 59985, None)
2018-02-06 15:04:47,863 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:59985 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 59985, None)
2018-02-06 15:04:47,866 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 59985, None)
2018-02-06 15:04:47,867 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 59985, None)
2018-02-06 15:04:48,101 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4201a617{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 15:04:48,172 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 15:04:48,173 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 15:04:48,180 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18ca3c62{/SQL,null,AVAILABLE,@Spark}
2018-02-06 15:04:48,181 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44d70181{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 15:04:48,184 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50b1f030{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 15:04:48,184 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fa05212{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 15:04:48,187 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a9c84a5{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 15:04:49,255 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 15:04:58,644 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 15:04:58,647 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 15:04:58,650 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<value: string>
2018-02-06 15:04:58,665 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 15:04:59,150 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 196.690943 ms
2018-02-06 15:04:59,202 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 322.5 KB, free 631.5 MB)
2018-02-06 15:04:59,270 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.5 MB)
2018-02-06 15:04:59,273 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:59985 (size: 27.5 KB, free: 631.8 MB)
2018-02-06 15:04:59,277 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from json at BaseSparkSQLTest.java:23
2018-02-06 15:04:59,288 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 15:04:59,471 INFO[org.apache.spark.SparkContext:54] - Starting job: json at BaseSparkSQLTest.java:23
2018-02-06 15:04:59,488 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at BaseSparkSQLTest.java:23) with 1 output partitions
2018-02-06 15:04:59,488 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at BaseSparkSQLTest.java:23)
2018-02-06 15:04:59,488 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 15:04:59,489 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 15:04:59,493 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23), which has no missing parents
2018-02-06 15:04:59,512 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.5 KB, free 631.4 MB)
2018-02-06 15:04:59,515 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KB, free 631.4 MB)
2018-02-06 15:04:59,516 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:59985 (size: 5.3 KB, free: 631.8 MB)
2018-02-06 15:04:59,517 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 15:04:59,530 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23) (first 15 tasks are for partitions Vector(0))
2018-02-06 15:04:59,531 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 15:04:59,571 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 15:04:59,582 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 15:04:59,641 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 15:04:59,671 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 16.424645 ms
2018-02-06 15:04:59,947 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 2776 bytes result sent to driver
2018-02-06 15:04:59,954 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 397 ms on localhost (executor driver) (1/1)
2018-02-06 15:04:59,956 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 15:04:59,960 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at BaseSparkSQLTest.java:23) finished in 0.412 s
2018-02-06 15:04:59,966 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at BaseSparkSQLTest.java:23, took 0.495064 s
2018-02-06 15:05:00,004 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 15:05:00,009 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@196ecc71{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 15:05:00,012 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 15:05:00,021 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 15:05:00,032 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 15:05:00,032 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 15:05:00,033 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 15:05:00,034 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 15:05:00,039 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 15:05:00,041 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 15:05:00,042 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-6276821f-c324-4837-8a29-b36156ac9f68
2018-02-06 15:08:04,600 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 15:08:05,495 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 15:08:05,533 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 15:08:05,534 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 15:08:05,534 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 15:08:05,535 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 15:08:05,536 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 15:08:05,924 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 60080.
2018-02-06 15:08:05,942 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 15:08:05,997 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 15:08:06,000 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 15:08:06,000 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 15:08:06,016 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-e070c319-2da0-4908-b5d5-3dec14de7215
2018-02-06 15:08:06,039 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 15:08:06,092 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 15:08:06,179 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3019ms
2018-02-06 15:08:06,254 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 15:08:06,268 INFO[org.spark_project.jetty.server.Server:403] - Started @3110ms
2018-02-06 15:08:06,293 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@33e01298{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 15:08:06,294 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 15:08:06,321 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@773f7880{/jobs,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,323 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bc715b8{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,324 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,326 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,326 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/stages,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,327 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,328 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,329 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@38f57b3d{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,329 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ce3db41{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,330 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,331 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/storage,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,331 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,332 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,333 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,334 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/environment,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,334 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,335 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/executors,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,337 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,337 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,338 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,345 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/static,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,346 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58670130{/,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,347 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/api,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,348 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18518ccf{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,348 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,350 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 15:08:06,442 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 15:08:06,477 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60093.
2018-02-06 15:08:06,477 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:60093
2018-02-06 15:08:06,481 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 15:08:06,484 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 60093, None)
2018-02-06 15:08:06,487 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:60093 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 60093, None)
2018-02-06 15:08:06,489 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 60093, None)
2018-02-06 15:08:06,491 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 60093, None)
2018-02-06 15:08:06,693 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@467f77a5{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,767 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 15:08:06,769 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 15:08:06,779 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c0f7678{/SQL,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,780 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6aa648b9{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,780 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4163f1cd{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,781 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e681bc{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:06,784 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2d83c5a5{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 15:08:07,975 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 15:08:17,426 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 15:08:17,428 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 15:08:17,431 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<value: string>
2018-02-06 15:08:17,439 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 15:08:17,925 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 184.761659 ms
2018-02-06 15:08:17,976 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 322.5 KB, free 631.5 MB)
2018-02-06 15:08:18,036 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.5 MB)
2018-02-06 15:08:18,039 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:60093 (size: 27.5 KB, free: 631.8 MB)
2018-02-06 15:08:18,045 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from json at BaseSparkSQLTest.java:23
2018-02-06 15:08:18,060 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 15:08:18,270 INFO[org.apache.spark.SparkContext:54] - Starting job: json at BaseSparkSQLTest.java:23
2018-02-06 15:08:18,286 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at BaseSparkSQLTest.java:23) with 1 output partitions
2018-02-06 15:08:18,286 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at BaseSparkSQLTest.java:23)
2018-02-06 15:08:18,287 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 15:08:18,288 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 15:08:18,294 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23), which has no missing parents
2018-02-06 15:08:18,317 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.5 KB, free 631.4 MB)
2018-02-06 15:08:18,322 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KB, free 631.4 MB)
2018-02-06 15:08:18,324 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:60093 (size: 5.3 KB, free: 631.8 MB)
2018-02-06 15:08:18,326 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 15:08:18,338 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23) (first 15 tasks are for partitions Vector(0))
2018-02-06 15:08:18,340 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 15:08:18,385 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 15:08:18,392 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 15:08:18,462 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 15:08:18,503 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 25.552328 ms
2018-02-06 15:08:18,764 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 2776 bytes result sent to driver
2018-02-06 15:08:18,774 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 401 ms on localhost (executor driver) (1/1)
2018-02-06 15:08:18,777 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 15:08:18,782 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at BaseSparkSQLTest.java:23) finished in 0.422 s
2018-02-06 15:08:18,789 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at BaseSparkSQLTest.java:23, took 0.517683 s
2018-02-06 15:08:18,834 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: bill
2018-02-06 15:08:19,021 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select *from bill
2018-02-06 15:08:19,122 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 15:08:19,131 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@33e01298{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 15:08:19,138 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 15:08:19,150 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 15:08:19,169 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 15:08:19,170 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 15:08:19,178 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 15:08:19,186 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 15:08:19,190 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 15:08:19,192 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 15:08:19,193 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-242d9e8b-fd7d-462e-b43e-428b2f9b8ae6
2018-02-06 15:08:33,813 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 15:08:34,482 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 15:08:34,505 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 15:08:34,506 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 15:08:34,507 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 15:08:34,508 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 15:08:34,508 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 15:08:34,923 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 60136.
2018-02-06 15:08:34,942 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 15:08:34,992 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 15:08:34,997 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 15:08:34,997 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 15:08:35,015 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-fd18eb99-ec7f-417d-93f5-14e926247bd7
2018-02-06 15:08:35,040 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 15:08:35,090 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 15:08:35,179 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3050ms
2018-02-06 15:08:35,282 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 15:08:35,301 INFO[org.spark_project.jetty.server.Server:403] - Started @3173ms
2018-02-06 15:08:35,330 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@2828d2ef{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 15:08:35,331 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 15:08:35,357 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,358 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,360 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,361 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,362 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/stages,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,363 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,364 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,365 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,366 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51850751{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,367 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64df9a61{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,367 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e260766{/storage,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,368 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a97744{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,370 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c56e013{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,373 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40258c2f{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,375 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6731787b{/environment,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,376 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,377 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/executors,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,378 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,378 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,379 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,386 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/static,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,387 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4bf3798b{/,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,389 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74e47444{/api,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,390 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46f699d5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,391 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1991f767{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,393 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 15:08:35,502 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 15:08:35,532 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60149.
2018-02-06 15:08:35,533 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:60149
2018-02-06 15:08:35,535 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 15:08:35,537 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 60149, None)
2018-02-06 15:08:35,543 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:60149 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 60149, None)
2018-02-06 15:08:35,547 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 60149, None)
2018-02-06 15:08:35,548 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 60149, None)
2018-02-06 15:08:35,844 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4201a617{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,923 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 15:08:35,925 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 15:08:35,936 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@23c650a3{/SQL,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,937 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@88a8218{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,938 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5c09d180{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,938 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5f574cc2{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 15:08:35,941 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@60222fd8{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 15:08:37,056 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 15:08:46,521 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 15:08:46,523 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 15:08:46,528 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<value: string>
2018-02-06 15:08:46,538 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 15:08:47,056 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 202.008705 ms
2018-02-06 15:08:47,111 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 322.5 KB, free 631.5 MB)
2018-02-06 15:08:47,168 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.5 MB)
2018-02-06 15:08:47,172 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:60149 (size: 27.5 KB, free: 631.8 MB)
2018-02-06 15:08:47,179 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from json at BaseSparkSQLTest.java:23
2018-02-06 15:08:47,191 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 15:08:47,394 INFO[org.apache.spark.SparkContext:54] - Starting job: json at BaseSparkSQLTest.java:23
2018-02-06 15:08:47,410 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at BaseSparkSQLTest.java:23) with 1 output partitions
2018-02-06 15:08:47,411 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at BaseSparkSQLTest.java:23)
2018-02-06 15:08:47,412 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 15:08:47,413 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 15:08:47,418 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23), which has no missing parents
2018-02-06 15:08:47,438 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.5 KB, free 631.4 MB)
2018-02-06 15:08:47,455 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KB, free 631.4 MB)
2018-02-06 15:08:47,456 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:60149 (size: 5.3 KB, free: 631.8 MB)
2018-02-06 15:08:47,456 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 15:08:47,473 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23) (first 15 tasks are for partitions Vector(0))
2018-02-06 15:08:47,474 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 15:08:47,519 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 15:08:47,529 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 15:08:47,592 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 15:08:47,622 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 15.922565 ms
2018-02-06 15:08:47,849 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
2018-02-06 15:08:47,857 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 350 ms on localhost (executor driver) (1/1)
2018-02-06 15:08:47,861 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 15:08:47,867 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at BaseSparkSQLTest.java:23) finished in 0.372 s
2018-02-06 15:08:47,874 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at BaseSparkSQLTest.java:23, took 0.479271 s
2018-02-06 15:08:47,919 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: bill
2018-02-06 15:08:48,085 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select *from bill
2018-02-06 15:08:48,226 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 15:08:48,226 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 15:08:48,227 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<bill_accept_org: string, bill_category_name: string, bill_classify: string, bill_desc: string, bill_discount_amount: double ... 22 more fields>
2018-02-06 15:08:48,227 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 15:08:48,300 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 631.1 MB)
2018-02-06 15:08:48,324 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.1 MB)
2018-02-06 15:08:48,327 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:60149 (size: 27.5 KB, free: 631.7 MB)
2018-02-06 15:08:48,332 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from show at BaseSparkSQL.java:47
2018-02-06 15:08:48,341 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 15:08:48,367 INFO[org.apache.spark.SparkContext:54] - Starting job: show at BaseSparkSQL.java:47
2018-02-06 15:08:48,368 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at BaseSparkSQL.java:47) with 1 output partitions
2018-02-06 15:08:48,369 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at BaseSparkSQL.java:47)
2018-02-06 15:08:48,369 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 15:08:48,369 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 15:08:48,371 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at BaseSparkSQL.java:47), which has no missing parents
2018-02-06 15:08:48,378 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 631.1 MB)
2018-02-06 15:08:48,381 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.9 KB, free 631.1 MB)
2018-02-06 15:08:48,382 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:60149 (size: 5.9 KB, free: 631.7 MB)
2018-02-06 15:08:48,383 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 15:08:48,384 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at BaseSparkSQL.java:47) (first 15 tasks are for partitions Vector(0))
2018-02-06 15:08:48,384 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 15:08:48,385 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 15:08:48,385 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 15:08:48,395 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 15:08:48,441 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 37.872652 ms
2018-02-06 15:08:48,486 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 5343 bytes result sent to driver
2018-02-06 15:08:48,487 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 103 ms on localhost (executor driver) (1/1)
2018-02-06 15:08:48,488 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 15:08:48,488 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at BaseSparkSQL.java:47) finished in 0.104 s
2018-02-06 15:08:48,488 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at BaseSparkSQL.java:47, took 0.120876 s
2018-02-06 15:08:48,555 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 43.714574 ms
2018-02-06 15:08:48,595 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 15:08:48,604 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@2828d2ef{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 15:08:48,609 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 15:08:48,621 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 15:08:48,638 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 15:08:48,639 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 15:08:48,640 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 15:08:48,643 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 15:08:48,645 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 15:08:48,646 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 15:08:48,647 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-af38b184-4ce5-4233-8458-2114ce5d1940
2018-02-06 15:17:27,102 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 15:17:27,717 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 15:17:27,741 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 15:17:27,743 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 15:17:27,744 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 15:17:27,745 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 15:17:27,746 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 15:17:28,131 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 60485.
2018-02-06 15:17:28,152 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 15:17:28,206 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 15:17:28,209 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 15:17:28,210 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 15:17:28,220 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-dd00c99c-ef74-491a-9b77-6beb6b069fdc
2018-02-06 15:17:28,243 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 15:17:28,297 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 15:17:28,387 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2685ms
2018-02-06 15:17:28,475 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 15:17:28,490 INFO[org.spark_project.jetty.server.Server:403] - Started @2790ms
2018-02-06 15:17:28,515 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@4b3c354a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 15:17:28,515 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 15:17:28,542 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c51bb7{/jobs,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,543 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,544 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,545 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,546 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/stages,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,547 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,548 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,550 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64df9a61{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,550 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e260766{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,551 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a97744{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,552 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c56e013{/storage,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,552 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40258c2f{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,554 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6731787b{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,555 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,558 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/environment,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,559 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,560 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/executors,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,560 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,561 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,561 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5860f3d7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,567 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/static,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,567 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@59d2103b{/,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,568 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e4de19b{/api,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,569 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c6daf0{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,570 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@659eef7{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,571 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 15:17:28,672 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 15:17:28,695 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60499.
2018-02-06 15:17:28,696 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:60499
2018-02-06 15:17:28,697 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 15:17:28,699 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 60499, None)
2018-02-06 15:17:28,703 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:60499 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 60499, None)
2018-02-06 15:17:28,706 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 60499, None)
2018-02-06 15:17:28,707 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 60499, None)
2018-02-06 15:17:28,887 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@df5f5c0{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,956 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 15:17:28,957 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 15:17:28,967 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@88a8218{/SQL,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,968 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4163f1cd{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,968 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5f574cc2{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,971 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a9c84a5{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 15:17:28,973 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@26f1249d{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 15:17:30,093 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 15:17:31,853 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 189.895741 ms
2018-02-06 15:17:32,294 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 12.314884 ms
2018-02-06 15:17:32,314 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 9.690883 ms
2018-02-06 15:17:32,336 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 15:17:32,343 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@4b3c354a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 15:17:32,345 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 15:17:32,354 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 15:17:32,362 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 15:17:32,362 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 15:17:32,367 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 15:17:32,370 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 15:17:32,374 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 15:17:32,375 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 15:17:32,376 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-c63522f1-6701-4462-a247-b378027ccfd8
2018-02-06 15:36:26,691 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 15:36:27,350 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 15:36:27,394 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 15:36:27,395 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 15:36:27,397 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 15:36:27,397 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 15:36:27,398 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 15:36:27,932 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 62060.
2018-02-06 15:36:27,955 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 15:36:28,010 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 15:36:28,015 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 15:36:28,015 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 15:36:28,029 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-19ddbd07-8710-4b27-acee-01f8213b068b
2018-02-06 15:36:28,065 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 15:36:28,125 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 15:36:28,252 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3219ms
2018-02-06 15:36:28,346 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 15:36:28,361 INFO[org.spark_project.jetty.server.Server:403] - Started @3330ms
2018-02-06 15:36:28,389 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@2828d2ef{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 15:36:28,389 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 15:36:28,417 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,418 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,419 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,420 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,420 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/stages,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,421 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,421 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,422 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,423 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51850751{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,423 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64df9a61{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,424 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e260766{/storage,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,424 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a97744{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,425 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c56e013{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,425 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40258c2f{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,426 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6731787b{/environment,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,427 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,428 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/executors,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,428 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,429 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,430 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,439 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/static,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,440 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4bf3798b{/,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,441 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74e47444{/api,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,442 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46f699d5{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,442 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1991f767{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 15:36:28,445 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 15:36:28,583 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 15:36:28,617 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62075.
2018-02-06 15:36:28,619 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:62075
2018-02-06 15:36:28,621 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 15:36:28,624 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 62075, None)
2018-02-06 15:36:28,629 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:62075 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 62075, None)
2018-02-06 15:36:28,634 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 62075, None)
2018-02-06 15:36:28,635 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 62075, None)
2018-02-06 15:36:28,958 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4201a617{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 15:36:29,115 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 15:36:29,116 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 15:36:29,130 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18ca3c62{/SQL,null,AVAILABLE,@Spark}
2018-02-06 15:36:29,131 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44d70181{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 15:36:29,132 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50b1f030{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 15:36:29,132 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5fa05212{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 15:36:29,138 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a9c84a5{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 15:36:30,464 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 15:36:40,399 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 15:36:40,403 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 15:36:40,408 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<value: string>
2018-02-06 15:36:40,420 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 15:36:41,092 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 263.132244 ms
2018-02-06 15:36:41,158 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 322.5 KB, free 631.5 MB)
2018-02-06 15:36:41,253 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.5 MB)
2018-02-06 15:36:41,259 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:62075 (size: 27.5 KB, free: 631.8 MB)
2018-02-06 15:36:41,270 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from json at BaseSparkSQLTest.java:23
2018-02-06 15:36:41,298 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 15:36:41,574 INFO[org.apache.spark.SparkContext:54] - Starting job: json at BaseSparkSQLTest.java:23
2018-02-06 15:36:41,603 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at BaseSparkSQLTest.java:23) with 1 output partitions
2018-02-06 15:36:41,603 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at BaseSparkSQLTest.java:23)
2018-02-06 15:36:41,605 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 15:36:41,607 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 15:36:41,616 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23), which has no missing parents
2018-02-06 15:36:41,666 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.5 KB, free 631.4 MB)
2018-02-06 15:36:41,674 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KB, free 631.4 MB)
2018-02-06 15:36:41,676 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:62075 (size: 5.3 KB, free: 631.8 MB)
2018-02-06 15:36:41,677 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 15:36:41,701 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23) (first 15 tasks are for partitions Vector(0))
2018-02-06 15:36:41,703 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 15:36:41,771 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 15:36:41,789 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 15:36:41,915 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 15:36:41,943 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 12.299524 ms
2018-02-06 15:36:42,248 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
2018-02-06 15:36:42,259 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 512 ms on localhost (executor driver) (1/1)
2018-02-06 15:36:42,266 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 15:36:42,271 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at BaseSparkSQLTest.java:23) finished in 0.541 s
2018-02-06 15:36:42,280 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at BaseSparkSQLTest.java:23, took 0.703252 s
2018-02-06 15:36:42,356 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 15:36:42,365 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@2828d2ef{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 15:36:42,367 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 15:36:42,377 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 15:36:42,390 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 15:36:42,391 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 15:36:42,396 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 15:36:42,401 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 15:36:42,404 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 15:36:42,404 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 15:36:42,405 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-42c3ce9d-2ffd-4282-879c-1167ca03388f
2018-02-06 15:37:27,434 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 15:37:28,114 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 15:37:28,140 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 15:37:28,141 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 15:37:28,142 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 15:37:28,143 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 15:37:28,144 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 15:37:28,543 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 62204.
2018-02-06 15:37:28,563 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 15:37:28,619 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 15:37:28,623 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 15:37:28,623 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 15:37:28,631 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-605b597d-f2f6-4740-aade-b8ad0d3424b4
2018-02-06 15:37:28,656 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 15:37:28,710 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 15:37:28,805 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2674ms
2018-02-06 15:37:28,887 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 15:37:28,902 INFO[org.spark_project.jetty.server.Server:403] - Started @2772ms
2018-02-06 15:37:28,926 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@33e01298{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 15:37:28,926 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 15:37:28,955 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@773f7880{/jobs,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,956 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bc715b8{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,956 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,957 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,958 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/stages,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,958 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,959 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,960 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@38f57b3d{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,961 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3ce3db41{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,962 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@77602954{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,962 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c3dec30{/storage,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,963 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4275c20c{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,964 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3fc9dfc5{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,965 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cac4385{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,966 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/environment,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,967 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,968 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@73c60324{/executors,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,968 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,969 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,969 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,975 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a66a204{/static,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,976 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58670130{/,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,976 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/api,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,977 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18518ccf{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,979 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 15:37:28,981 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 15:37:29,070 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 15:37:29,098 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62219.
2018-02-06 15:37:29,099 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:62219
2018-02-06 15:37:29,101 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 15:37:29,108 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 62219, None)
2018-02-06 15:37:29,111 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:62219 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 62219, None)
2018-02-06 15:37:29,117 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 62219, None)
2018-02-06 15:37:29,118 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 62219, None)
2018-02-06 15:37:29,374 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@467f77a5{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 15:37:29,455 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 15:37:29,456 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 15:37:29,468 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@742d4e15{/SQL,null,AVAILABLE,@Spark}
2018-02-06 15:37:29,468 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50b1f030{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 15:37:29,469 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@23aae55{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 15:37:29,470 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@680bddf5{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 15:37:29,472 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53bf7094{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 15:37:30,627 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 15:37:40,321 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 15:37:40,326 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 15:37:40,329 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<value: string>
2018-02-06 15:37:40,343 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 15:37:40,899 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 216.94983 ms
2018-02-06 15:37:40,960 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 322.5 KB, free 631.5 MB)
2018-02-06 15:37:41,025 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.5 MB)
2018-02-06 15:37:41,028 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:62219 (size: 27.5 KB, free: 631.8 MB)
2018-02-06 15:37:41,033 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from json at BaseSparkSQLTest.java:23
2018-02-06 15:37:41,045 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 15:37:41,276 INFO[org.apache.spark.SparkContext:54] - Starting job: json at BaseSparkSQLTest.java:23
2018-02-06 15:37:41,296 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at BaseSparkSQLTest.java:23) with 1 output partitions
2018-02-06 15:37:41,297 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at BaseSparkSQLTest.java:23)
2018-02-06 15:37:41,297 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 15:37:41,300 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 15:37:41,305 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23), which has no missing parents
2018-02-06 15:37:41,328 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.5 KB, free 631.4 MB)
2018-02-06 15:37:41,333 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KB, free 631.4 MB)
2018-02-06 15:37:41,334 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:62219 (size: 5.3 KB, free: 631.8 MB)
2018-02-06 15:37:41,335 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 15:37:41,347 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23) (first 15 tasks are for partitions Vector(0))
2018-02-06 15:37:41,348 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 15:37:41,399 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 15:37:41,409 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 15:37:41,486 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 15:37:41,518 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 14.160964 ms
2018-02-06 15:37:41,778 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 2776 bytes result sent to driver
2018-02-06 15:37:41,786 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 403 ms on localhost (executor driver) (1/1)
2018-02-06 15:37:41,788 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 15:37:41,793 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at BaseSparkSQLTest.java:23) finished in 0.424 s
2018-02-06 15:37:41,799 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at BaseSparkSQLTest.java:23, took 0.523469 s
2018-02-06 15:37:41,844 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: bill
2018-02-06 15:37:42,009 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select myAverage(bill_money) from bill
2018-02-06 15:37:42,280 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 15:37:42,281 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 15:37:42,282 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<bill_money: double>
2018-02-06 15:37:42,282 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 15:37:42,363 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 631.1 MB)
2018-02-06 15:37:42,383 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.1 MB)
2018-02-06 15:37:42,385 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:62219 (size: 27.5 KB, free: 631.7 MB)
2018-02-06 15:37:42,389 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from show at BaseSparkSQL.java:54
2018-02-06 15:37:42,395 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 15:37:42,549 INFO[org.apache.spark.SparkContext:54] - Starting job: show at BaseSparkSQL.java:54
2018-02-06 15:37:42,553 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 7 (show at BaseSparkSQL.java:54)
2018-02-06 15:37:42,554 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at BaseSparkSQL.java:54) with 1 output partitions
2018-02-06 15:37:42,554 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (show at BaseSparkSQL.java:54)
2018-02-06 15:37:42,555 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-06 15:37:42,555 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-06 15:37:42,556 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at show at BaseSparkSQL.java:54), which has no missing parents
2018-02-06 15:37:42,564 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 16.4 KB, free 631.1 MB)
2018-02-06 15:37:42,568 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.5 KB, free 631.1 MB)
2018-02-06 15:37:42,569 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:62219 (size: 8.5 KB, free: 631.7 MB)
2018-02-06 15:37:42,570 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 15:37:42,575 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at show at BaseSparkSQL.java:54) (first 15 tasks are for partitions Vector(0))
2018-02-06 15:37:42,575 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 15:37:42,577 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5344 bytes)
2018-02-06 15:37:42,578 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 15:37:42,597 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 15:37:42,608 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 10.473283 ms
2018-02-06 15:37:42,683 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 12.201603 ms
2018-02-06 15:37:42,702 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 7.841282 ms
2018-02-06 15:37:42,722 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 12.532804 ms
2018-02-06 15:37:42,739 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 12.611524 ms
2018-02-06 15:37:42,791 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 7.628802 ms
2018-02-06 15:37:42,859 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 2061 bytes result sent to driver
2018-02-06 15:37:42,862 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 285 ms on localhost (executor driver) (1/1)
2018-02-06 15:37:42,862 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 15:37:42,863 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (show at BaseSparkSQL.java:54) finished in 0.287 s
2018-02-06 15:37:42,863 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-06 15:37:42,864 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-06 15:37:42,864 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-06 15:37:42,865 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-06 15:37:42,869 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (MapPartitionsRDD[10] at show at BaseSparkSQL.java:54), which has no missing parents
2018-02-06 15:37:42,876 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4 stored as values in memory (estimated size 16.7 KB, free 631.1 MB)
2018-02-06 15:37:42,880 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 8.5 KB, free 631.1 MB)
2018-02-06 15:37:42,881 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_4_piece0 in memory on 192.168.11.26:62219 (size: 8.5 KB, free: 631.7 MB)
2018-02-06 15:37:42,882 INFO[org.apache.spark.SparkContext:54] - Created broadcast 4 from broadcast at DAGScheduler.scala:1006
2018-02-06 15:37:42,882 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[10] at show at BaseSparkSQL.java:54) (first 15 tasks are for partitions Vector(0))
2018-02-06 15:37:42,883 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 1 tasks
2018-02-06 15:37:42,885 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
2018-02-06 15:37:42,885 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 2)
2018-02-06 15:37:42,902 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 1 non-empty blocks out of 1 blocks
2018-02-06 15:37:42,903 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
2018-02-06 15:37:42,932 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 6.443842 ms
2018-02-06 15:37:42,942 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 2). 2391 bytes result sent to driver
2018-02-06 15:37:42,943 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 2) in 60 ms on localhost (executor driver) (1/1)
2018-02-06 15:37:42,943 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-06 15:37:42,943 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (show at BaseSparkSQL.java:54) finished in 0.060 s
2018-02-06 15:37:42,944 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at BaseSparkSQL.java:54, took 0.395223 s
2018-02-06 15:37:42,960 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 6.328322 ms
2018-02-06 15:37:42,977 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 15:37:42,982 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@33e01298{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 15:37:42,984 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 15:37:42,992 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 15:37:43,017 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 15:37:43,018 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 15:37:43,019 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 15:37:43,022 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 15:37:43,042 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 15:37:43,043 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 15:37:43,046 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-e2afde68-e3aa-4350-bca9-defa2bca21a1
2018-02-06 16:04:28,692 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 16:04:29,174 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 16:04:29,211 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 16:04:29,212 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 16:04:29,214 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 16:04:29,215 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 16:04:29,216 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 16:04:29,726 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 65401.
2018-02-06 16:04:29,752 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 16:04:29,777 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 16:04:29,781 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 16:04:29,782 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 16:04:29,796 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-fe0547ff-0b9c-4596-9a41-21b6de3c4eea
2018-02-06 16:04:29,829 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 16:04:29,890 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 16:04:30,007 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3114ms
2018-02-06 16:04:30,080 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 16:04:30,095 INFO[org.spark_project.jetty.server.Server:403] - Started @3204ms
2018-02-06 16:04:30,121 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@1a7e5aa2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:04:30,121 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 16:04:30,152 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c177f9e{/jobs,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,153 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@460f76a6{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,153 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@11acdc30{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,154 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1922e6d{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,155 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6bab2585{/stages,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,155 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@644c78d4{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,156 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@611f8234{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,157 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@62923ee6{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,157 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f19c9d2{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,158 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@b91d8c4{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,159 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a77614d{/storage,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,159 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4a067c25{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,160 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3bde62ff{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,161 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2baa8d82{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,161 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@791cbf87{/environment,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,162 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@754777cd{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,162 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@372ea2bc{/executors,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,163 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f08c4b{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,163 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7de0c6ae{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,164 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@cdc3aae{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,174 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5dcbb60{/static,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,175 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@114a85c2{/,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,176 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@cf65451{/api,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,177 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46074492{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,177 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c715e84{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 16:04:30,180 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 16:04:30,313 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 16:04:30,349 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65415.
2018-02-06 16:04:30,350 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:65415
2018-02-06 16:04:30,352 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 16:04:30,355 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 65415, None)
2018-02-06 16:04:30,358 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:65415 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 65415, None)
2018-02-06 16:04:30,362 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 65415, None)
2018-02-06 16:04:30,363 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 65415, None)
2018-02-06 16:04:30,560 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@240139e1{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:38,449 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 16:04:38,893 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 16:04:38,917 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 16:04:38,918 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 16:04:38,919 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 16:04:38,919 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 16:04:38,920 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 16:04:39,390 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 65453.
2018-02-06 16:04:39,410 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 16:04:39,428 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 16:04:39,432 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 16:04:39,432 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 16:04:39,443 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-588c84b7-5dcd-4249-a193-832c97542339
2018-02-06 16:04:39,463 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 16:04:39,519 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 16:04:39,610 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3051ms
2018-02-06 16:04:39,670 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 16:04:39,684 INFO[org.spark_project.jetty.server.Server:403] - Started @3125ms
2018-02-06 16:04:39,702 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@4be712af{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:04:39,703 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 16:04:39,725 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c177f9e{/jobs,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,726 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@460f76a6{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,726 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@11acdc30{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,727 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1922e6d{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,727 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6bab2585{/stages,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,728 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@644c78d4{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,729 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@611f8234{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,731 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@62923ee6{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,731 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f19c9d2{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,732 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@b91d8c4{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,733 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a77614d{/storage,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,733 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4a067c25{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,734 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3bde62ff{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,735 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2baa8d82{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,736 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@791cbf87{/environment,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,737 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@754777cd{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,738 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@372ea2bc{/executors,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,738 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f08c4b{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,739 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7de0c6ae{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,739 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@cdc3aae{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,746 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5dcbb60{/static,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,747 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@114a85c2{/,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,748 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@cf65451{/api,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,749 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46074492{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,749 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c715e84{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 16:04:39,751 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 16:04:39,834 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 16:04:39,857 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65469.
2018-02-06 16:04:39,858 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:65469
2018-02-06 16:04:39,860 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 16:04:39,862 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 65469, None)
2018-02-06 16:04:39,866 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:65469 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 65469, None)
2018-02-06 16:04:39,870 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 65469, None)
2018-02-06 16:04:39,872 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 65469, None)
2018-02-06 16:04:40,057 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@240139e1{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:40,095 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 16:04:40,096 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 16:04:40,109 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@208e9ef6{/SQL,null,AVAILABLE,@Spark}
2018-02-06 16:04:40,110 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@261d8190{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:40,110 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@537b32ef{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 16:04:40,111 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4b61d0c6{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 16:04:40,113 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4aeaadc1{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 16:05:04,240 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 16:05:04,690 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 16:05:04,711 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 16:05:04,712 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 16:05:04,713 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 16:05:04,714 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 16:05:04,715 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 16:05:05,175 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 49160.
2018-02-06 16:05:05,194 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 16:05:05,214 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 16:05:05,217 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 16:05:05,218 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 16:05:05,228 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-d9af7c3e-abd8-4857-b64f-c41f9302009e
2018-02-06 16:05:05,250 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 16:05:05,303 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 16:05:05,393 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3354ms
2018-02-06 16:05:05,452 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 16:05:05,466 INFO[org.spark_project.jetty.server.Server:403] - Started @3429ms
2018-02-06 16:05:05,483 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@3ffb97f2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:05:05,484 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 16:05:05,504 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c177f9e{/jobs,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,505 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@460f76a6{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,506 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@11acdc30{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,506 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1922e6d{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,507 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6bab2585{/stages,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,508 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@644c78d4{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,508 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@611f8234{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,509 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@62923ee6{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,510 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f19c9d2{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,511 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@b91d8c4{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,511 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a77614d{/storage,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,512 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4a067c25{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,512 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3bde62ff{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,513 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2baa8d82{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,514 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@791cbf87{/environment,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,514 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@754777cd{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,515 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@372ea2bc{/executors,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,516 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f08c4b{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,517 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7de0c6ae{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,517 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@cdc3aae{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,524 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5dcbb60{/static,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,525 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@114a85c2{/,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,526 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@cf65451{/api,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,527 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46074492{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,528 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c715e84{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,530 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 16:05:05,616 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 16:05:05,658 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49173.
2018-02-06 16:05:05,659 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:49173
2018-02-06 16:05:05,660 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 16:05:05,663 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 49173, None)
2018-02-06 16:05:05,668 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:49173 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 49173, None)
2018-02-06 16:05:05,677 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 49173, None)
2018-02-06 16:05:05,678 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 49173, None)
2018-02-06 16:05:05,895 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@240139e1{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,930 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 16:05:05,931 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 16:05:05,944 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@208e9ef6{/SQL,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,945 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@261d8190{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,947 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@537b32ef{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,947 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4b61d0c6{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:05,950 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4aeaadc1{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 16:05:07,142 INFO[org.apache.spark.sql.hive.HiveUtils:54] - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2018-02-06 16:05:07,731 INFO[org.apache.hadoop.hive.metastore.HiveMetaStore:589] - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2018-02-06 16:05:07,763 INFO[org.apache.hadoop.hive.metastore.ObjectStore:289] - ObjectStore, initialize called
2018-02-06 16:05:07,938 INFO[DataNucleus.Persistence:77] - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2018-02-06 16:05:07,939 INFO[DataNucleus.Persistence:77] - Property datanucleus.cache.level2 unknown - will be ignored
2018-02-06 16:05:47,353 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 16:05:47,885 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 16:05:47,925 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 16:05:47,926 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 16:05:47,927 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 16:05:47,928 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 16:05:47,928 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 16:05:48,378 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 49275.
2018-02-06 16:05:48,403 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 16:05:48,457 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 16:05:48,461 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 16:05:48,462 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 16:05:48,474 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-7e7387e8-a170-4833-a095-3ee7e238475c
2018-02-06 16:05:48,507 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 16:05:48,567 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 16:05:48,681 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3411ms
2018-02-06 16:05:48,766 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 16:05:48,783 INFO[org.spark_project.jetty.server.Server:403] - Started @3515ms
2018-02-06 16:05:48,808 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@165b2f7f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:05:48,810 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 16:05:48,834 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/jobs,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,834 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74e47444{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,835 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@59d2103b{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,837 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f6c03cb{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,838 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18518ccf{/stages,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,839 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@768ccdc5{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,839 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10650953{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,841 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c9f0a20{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,842 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1cd201a8{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,843 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1992eaf4{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,843 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3276732{/storage,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,844 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@31e3250d{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,845 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@21d8bcbe{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,846 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7383eae2{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,847 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c7d3c46{/environment,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,848 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@48c35007{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,848 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6722db6e{/executors,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,849 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ae33a11{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,850 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@b40bb6e{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,851 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5049d8b2{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,860 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@631e06ab{/static,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,861 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@741b3bc3{/,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,862 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63648ee9{/api,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,862 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5b58ed3c{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,863 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3a320ade{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 16:05:48,865 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 16:05:48,980 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 16:05:49,012 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49290.
2018-02-06 16:05:49,013 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:49290
2018-02-06 16:05:49,016 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 16:05:49,019 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 49290, None)
2018-02-06 16:05:49,023 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:49290 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 49290, None)
2018-02-06 16:05:49,027 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 49290, None)
2018-02-06 16:05:49,028 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 49290, None)
2018-02-06 16:05:49,238 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3b718392{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:49,330 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 16:05:49,331 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 16:05:49,339 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7f4d9395{/SQL,null,AVAILABLE,@Spark}
2018-02-06 16:05:49,340 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@344b8190{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:49,341 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@27f3b6d6{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 16:05:49,342 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2617f816{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 16:05:49,345 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a53bb6f{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 16:05:50,665 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 16:06:00,442 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 16:06:00,445 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 16:06:00,448 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<value: string>
2018-02-06 16:06:00,458 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 16:06:01,036 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 204.176385 ms
2018-02-06 16:06:01,102 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 322.5 KB, free 631.5 MB)
2018-02-06 16:06:01,168 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.5 MB)
2018-02-06 16:06:01,173 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:49290 (size: 27.5 KB, free: 631.8 MB)
2018-02-06 16:06:01,178 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from json at BaseSparkSQLTest.java:23
2018-02-06 16:06:01,192 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 16:06:01,402 INFO[org.apache.spark.SparkContext:54] - Starting job: json at BaseSparkSQLTest.java:23
2018-02-06 16:06:01,424 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at BaseSparkSQLTest.java:23) with 1 output partitions
2018-02-06 16:06:01,424 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at BaseSparkSQLTest.java:23)
2018-02-06 16:06:01,425 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 16:06:01,427 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 16:06:01,435 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23), which has no missing parents
2018-02-06 16:06:01,475 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.5 KB, free 631.4 MB)
2018-02-06 16:06:01,478 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KB, free 631.4 MB)
2018-02-06 16:06:01,480 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:49290 (size: 5.3 KB, free: 631.8 MB)
2018-02-06 16:06:01,481 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 16:06:01,494 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23) (first 15 tasks are for partitions Vector(0))
2018-02-06 16:06:01,496 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 16:06:01,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 16:06:01,554 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 16:06:01,637 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 16:06:01,671 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 17.165766 ms
2018-02-06 16:06:01,915 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
2018-02-06 16:06:01,928 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 397 ms on localhost (executor driver) (1/1)
2018-02-06 16:06:01,930 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 16:06:01,936 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at BaseSparkSQLTest.java:23) finished in 0.418 s
2018-02-06 16:06:01,942 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at BaseSparkSQLTest.java:23, took 0.539581 s
2018-02-06 16:06:02,029 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 16:06:02,030 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 16:06:02,031 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<bill_accept_org: string, bill_category_name: string, bill_classify: string, bill_desc: string, bill_discount_amount: double ... 22 more fields>
2018-02-06 16:06:02,032 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 16:06:02,075 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 322.5 KB, free 631.1 MB)
2018-02-06 16:06:02,090 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KB, free 631.1 MB)
2018-02-06 16:06:02,092 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:49290 (size: 27.5 KB, free: 631.7 MB)
2018-02-06 16:06:02,093 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from show at BaseSparkSQL.java:23
2018-02-06 16:06:02,098 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 16:06:02,118 INFO[org.apache.spark.SparkContext:54] - Starting job: show at BaseSparkSQL.java:23
2018-02-06 16:06:02,123 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at BaseSparkSQL.java:23) with 1 output partitions
2018-02-06 16:06:02,123 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at BaseSparkSQL.java:23)
2018-02-06 16:06:02,124 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 16:06:02,124 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 16:06:02,124 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at BaseSparkSQL.java:23), which has no missing parents
2018-02-06 16:06:02,140 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 631.1 MB)
2018-02-06 16:06:02,144 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KB, free 631.1 MB)
2018-02-06 16:06:02,145 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:49290 (size: 5.8 KB, free: 631.7 MB)
2018-02-06 16:06:02,146 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 16:06:02,147 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at BaseSparkSQL.java:23) (first 15 tasks are for partitions Vector(0))
2018-02-06 16:06:02,147 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 16:06:02,148 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 16:06:02,148 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 16:06:02,158 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 16:06:02,201 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 35.257291 ms
2018-02-06 16:06:02,254 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 5429 bytes result sent to driver
2018-02-06 16:06:02,256 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 108 ms on localhost (executor driver) (1/1)
2018-02-06 16:06:02,256 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 16:06:02,256 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at BaseSparkSQL.java:23) finished in 0.109 s
2018-02-06 16:06:02,257 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at BaseSparkSQL.java:23, took 0.137177 s
2018-02-06 16:06:02,312 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 34.194571 ms
2018-02-06 16:06:02,344 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 16:06:02,349 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@165b2f7f{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:06:02,351 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 16:06:02,361 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 16:06:02,382 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 16:06:02,382 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 16:06:02,388 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 16:06:02,392 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 16:06:02,396 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 16:06:02,397 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 16:06:02,398 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-fe663890-8641-443e-a880-8134343f996d
2018-02-06 16:06:25,941 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 16:06:26,419 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 16:06:26,441 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 16:06:26,441 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 16:06:26,442 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 16:06:26,442 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 16:06:26,443 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 16:06:26,891 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 49382.
2018-02-06 16:06:26,911 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 16:06:26,930 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 16:06:26,935 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 16:06:26,936 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 16:06:26,945 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-7cfd5e30-acfd-4d16-827f-169c2da96313
2018-02-06 16:06:26,967 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 16:06:27,023 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 16:06:27,114 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2959ms
2018-02-06 16:06:27,176 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 16:06:27,190 INFO[org.spark_project.jetty.server.Server:403] - Started @3036ms
2018-02-06 16:06:27,210 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@308bea9b{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:06:27,210 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 16:06:27,234 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18e7143f{/jobs,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,235 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@770d4269{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,236 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1922e6d{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,237 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74bdc168{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,238 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@532a02d9{/stages,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,238 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7bb3a9fe{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,239 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7f811d00{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,240 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7807ac2c{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,241 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4b6166aa{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,242 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4fd4cae3{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,244 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a1217f9{/storage,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,245 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@523424b5{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,246 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@319dead1{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,247 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a7e2d9d{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,247 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b52c0d6{/environment,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,248 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4cc76301{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,248 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f19b8b3{/executors,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,249 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a486d78{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,250 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7ef2d7a6{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,250 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c36250e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,256 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@49f5c307{/static,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,257 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@724f138e{/,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,259 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@32fe9d0a{/api,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,259 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@47428937{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,260 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7caa550{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,262 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 16:06:27,351 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 16:06:27,394 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49395.
2018-02-06 16:06:27,395 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:49395
2018-02-06 16:06:27,400 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 16:06:27,402 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 49395, None)
2018-02-06 16:06:27,413 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:49395 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 49395, None)
2018-02-06 16:06:27,416 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 49395, None)
2018-02-06 16:06:27,417 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 49395, None)
2018-02-06 16:06:27,694 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@253c1256{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,733 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-06 16:06:27,734 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-06 16:06:27,740 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4b61d0c6{/SQL,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,741 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65e7f52a{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,742 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@263558c9{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,742 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7daa61f3{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 16:06:27,747 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@b0964b2{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 16:11:59,540 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 16:12:00,099 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 16:12:00,124 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 16:12:00,124 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 16:12:00,125 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 16:12:00,125 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 16:12:00,126 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 16:12:00,527 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 50060.
2018-02-06 16:12:00,547 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 16:12:00,592 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 16:12:00,596 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 16:12:00,597 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 16:12:00,606 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-4666e9e8-1a20-451c-8038-f25b6998da5e
2018-02-06 16:12:00,633 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 16:12:00,694 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 16:12:00,782 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2816ms
2018-02-06 16:12:00,855 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 16:12:00,870 INFO[org.spark_project.jetty.server.Server:403] - Started @2906ms
2018-02-06 16:12:00,890 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@4efb356{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:12:00,890 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 16:12:00,916 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/jobs,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,916 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58670130{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,917 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,918 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e4de19b{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,918 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46f699d5{/stages,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,919 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1991f767{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,919 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c6daf0{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,921 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2488b073{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,921 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55787112{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,922 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7db82169{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,922 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f74e835{/storage,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,923 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f28bd56{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,924 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@19fe4644{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,924 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5be067de{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,925 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18245eb0{/environment,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,925 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@24fb6a80{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,926 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@72a85671{/executors,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,927 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18f20260{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,928 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a48e6e2{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,929 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3a94964{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,937 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d0b5baf{/static,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,937 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@675d8c96{/,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,938 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2ed3b1f5{/api,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,939 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,939 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@24faea88{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 16:12:00,941 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 16:12:01,034 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 16:12:01,064 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50073.
2018-02-06 16:12:01,065 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:50073
2018-02-06 16:12:01,066 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 16:12:01,068 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 50073, None)
2018-02-06 16:12:01,075 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:50073 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 50073, None)
2018-02-06 16:12:01,079 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 50073, None)
2018-02-06 16:12:01,080 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 50073, None)
2018-02-06 16:12:01,330 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2bffa76d{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:01,423 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('E:\IntelliJWorkspaceMumu\mumu-spark/spark-warehouse').
2018-02-06 16:12:01,425 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'E:\IntelliJWorkspaceMumu\mumu-spark/spark-warehouse'.
2018-02-06 16:12:01,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@cda4919{/SQL,null,AVAILABLE,@Spark}
2018-02-06 16:12:01,435 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f19f2aa{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:01,436 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@718607eb{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 16:12:01,437 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@757f675c{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:01,439 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55b62629{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 16:12:52,240 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 16:12:52,820 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 16:12:52,845 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 16:12:52,846 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 16:12:52,847 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 16:12:52,848 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 16:12:52,849 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 16:12:53,239 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 50197.
2018-02-06 16:12:53,262 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 16:12:53,311 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 16:12:53,314 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 16:12:53,315 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 16:12:53,325 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-aa41879f-c9bf-44f4-8c04-e726159d039e
2018-02-06 16:12:53,350 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 16:12:53,409 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 16:12:53,499 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2658ms
2018-02-06 16:12:53,578 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 16:12:53,594 INFO[org.spark_project.jetty.server.Server:403] - Started @2754ms
2018-02-06 16:12:53,617 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@2213fe1a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:12:53,617 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 16:12:53,642 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/jobs,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,643 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58670130{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,644 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,645 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e4de19b{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,646 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46f699d5{/stages,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,648 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1991f767{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,649 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c6daf0{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,652 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2488b073{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,654 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55787112{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,655 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7db82169{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,657 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f74e835{/storage,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,658 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f28bd56{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,660 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@19fe4644{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,662 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5be067de{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,663 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18245eb0{/environment,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,664 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@24fb6a80{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,665 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@72a85671{/executors,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,667 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18f20260{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,668 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a48e6e2{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,669 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3a94964{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,676 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d0b5baf{/static,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,677 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@675d8c96{/,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,680 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2ed3b1f5{/api,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,680 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,682 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@24faea88{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 16:12:53,684 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 16:12:53,802 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 16:12:53,829 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50210.
2018-02-06 16:12:53,830 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:50210
2018-02-06 16:12:53,832 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 16:12:53,833 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 50210, None)
2018-02-06 16:12:53,837 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:50210 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 50210, None)
2018-02-06 16:12:53,840 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 50210, None)
2018-02-06 16:12:53,840 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 50210, None)
2018-02-06 16:12:54,017 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2bffa76d{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:54,086 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('E:\IntelliJWorkspaceMumu\mumu-spark\spark-warehouse').
2018-02-06 16:12:54,087 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'E:\IntelliJWorkspaceMumu\mumu-spark\spark-warehouse'.
2018-02-06 16:12:54,094 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@cda4919{/SQL,null,AVAILABLE,@Spark}
2018-02-06 16:12:54,094 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f19f2aa{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:54,095 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@718607eb{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 16:12:54,095 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@757f675c{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 16:12:54,097 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55b62629{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 16:12:55,252 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 16:14:17,405 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 16:14:17,937 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 16:14:17,962 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 16:14:17,963 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 16:14:17,964 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 16:14:17,965 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 16:14:17,965 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 16:14:18,422 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 50385.
2018-02-06 16:14:18,443 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 16:14:18,494 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 16:14:18,498 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 16:14:18,498 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 16:14:18,508 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-010047c7-bc4d-41c5-8042-324427862ea9
2018-02-06 16:14:18,536 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 16:14:18,597 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 16:14:18,698 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2912ms
2018-02-06 16:14:18,775 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 16:14:18,790 INFO[org.spark_project.jetty.server.Server:403] - Started @3005ms
2018-02-06 16:14:18,813 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@56ce85d4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:14:18,814 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 16:14:18,838 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/jobs,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,838 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58670130{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,839 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,840 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e4de19b{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,841 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46f699d5{/stages,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,842 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1991f767{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,842 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c6daf0{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,843 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2488b073{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,844 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55787112{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,845 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7db82169{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,845 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f74e835{/storage,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,846 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f28bd56{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,847 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@19fe4644{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,849 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5be067de{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,849 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18245eb0{/environment,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,850 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@24fb6a80{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,851 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@72a85671{/executors,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,852 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18f20260{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,852 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a48e6e2{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,853 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3a94964{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,859 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d0b5baf{/static,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,860 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@675d8c96{/,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,861 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2ed3b1f5{/api,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,862 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,863 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@24faea88{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 16:14:18,864 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 16:14:18,969 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 16:14:18,995 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50401.
2018-02-06 16:14:18,996 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:50401
2018-02-06 16:14:18,999 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 16:14:19,002 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 50401, None)
2018-02-06 16:14:19,006 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:50401 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 50401, None)
2018-02-06 16:14:19,010 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 50401, None)
2018-02-06 16:14:19,011 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 50401, None)
2018-02-06 16:14:19,213 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2bffa76d{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 16:14:19,281 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('E:\IntelliJWorkspaceMumu\mumu-spark\spark-warehouse').
2018-02-06 16:14:19,283 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'E:\IntelliJWorkspaceMumu\mumu-spark\spark-warehouse'.
2018-02-06 16:14:19,290 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f19f2aa{/SQL,null,AVAILABLE,@Spark}
2018-02-06 16:14:19,291 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a078481{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 16:14:19,292 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@757f675c{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 16:14:19,293 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@676f0a60{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 16:14:19,296 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5e63cad{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 16:14:20,611 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 16:14:28,717 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 16:14:28,722 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@56ce85d4{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:14:28,724 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 16:14:28,732 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 16:14:28,741 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 16:14:28,742 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 16:14:28,747 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 16:14:28,750 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 16:14:28,752 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 16:14:28,753 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 16:14:28,754 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-c067c003-cbbb-4835-a137-b6a532350935
2018-02-06 16:15:11,766 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 16:15:12,513 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 16:15:12,541 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 16:15:12,542 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 16:15:12,543 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 16:15:12,543 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 16:15:12,544 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 16:15:12,978 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 50545.
2018-02-06 16:15:13,000 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 16:15:13,049 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 16:15:13,054 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 16:15:13,054 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 16:15:13,063 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-b610e9b9-e2c2-4603-9c3d-7946dfedb86f
2018-02-06 16:15:13,092 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 16:15:13,157 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 16:15:13,250 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3169ms
2018-02-06 16:15:13,337 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 16:15:13,351 INFO[org.spark_project.jetty.server.Server:403] - Started @3271ms
2018-02-06 16:15:13,373 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@45161c9d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:15:13,374 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 16:15:13,399 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/jobs,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,401 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58670130{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,402 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,403 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e4de19b{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,403 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46f699d5{/stages,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,404 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1991f767{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,404 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c6daf0{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,405 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2488b073{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,406 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55787112{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,407 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7db82169{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,407 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f74e835{/storage,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,408 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f28bd56{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,409 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@19fe4644{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,411 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5be067de{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,412 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18245eb0{/environment,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,413 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@24fb6a80{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,413 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@72a85671{/executors,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,414 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18f20260{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,415 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a48e6e2{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3a94964{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,423 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d0b5baf{/static,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,424 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@675d8c96{/,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,425 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2ed3b1f5{/api,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,425 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,426 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@24faea88{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,428 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 16:15:13,521 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 16:15:13,551 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50558.
2018-02-06 16:15:13,551 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:50558
2018-02-06 16:15:13,552 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 16:15:13,554 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 50558, None)
2018-02-06 16:15:13,557 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:50558 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 50558, None)
2018-02-06 16:15:13,561 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 50558, None)
2018-02-06 16:15:13,562 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 50558, None)
2018-02-06 16:15:13,814 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2bffa76d{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,900 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('E:\IntelliJWorkspaceMumu\mumu-spark\spark-warehouse').
2018-02-06 16:15:13,901 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'E:\IntelliJWorkspaceMumu\mumu-spark\spark-warehouse'.
2018-02-06 16:15:13,909 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a078481{/SQL,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,910 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1929425f{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,912 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@676f0a60{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,912 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@535b8c24{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 16:15:13,914 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@33a053d{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 16:15:15,227 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 16:17:28,386 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 16:17:28,878 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 16:17:28,900 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 16:17:28,901 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 16:17:28,902 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 16:17:28,903 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 16:17:28,904 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 16:17:29,354 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 50967.
2018-02-06 16:17:29,381 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 16:17:29,435 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 16:17:29,438 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 16:17:29,438 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 16:17:29,446 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-d22e8aba-bb83-43f4-a687-11ac55339cfd
2018-02-06 16:17:29,484 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 16:17:29,537 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 16:17:29,637 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3480ms
2018-02-06 16:17:29,720 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 16:17:29,733 INFO[org.spark_project.jetty.server.Server:403] - Started @3577ms
2018-02-06 16:17:29,753 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@73d06cf2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:17:29,753 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 16:17:29,775 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/jobs,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,778 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58670130{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,780 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@9bd0fa6{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,781 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e4de19b{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,782 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46f699d5{/stages,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,782 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1991f767{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,784 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c6daf0{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,786 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2488b073{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,786 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55787112{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,787 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7db82169{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,788 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f74e835{/storage,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,788 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f28bd56{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,790 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@19fe4644{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,791 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5be067de{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,791 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18245eb0{/environment,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,792 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@24fb6a80{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,793 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@72a85671{/executors,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,793 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18f20260{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,794 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a48e6e2{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,796 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3a94964{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,801 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d0b5baf{/static,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,802 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@675d8c96{/,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,803 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2ed3b1f5{/api,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,804 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d366c9b{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,805 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@24faea88{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 16:17:29,807 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 16:17:29,983 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 16:17:30,031 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50980.
2018-02-06 16:17:30,032 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:50980
2018-02-06 16:17:30,037 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 16:17:30,042 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 50980, None)
2018-02-06 16:17:30,061 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:50980 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 50980, None)
2018-02-06 16:17:30,066 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 50980, None)
2018-02-06 16:17:30,067 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 50980, None)
2018-02-06 16:17:30,438 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2bffa76d{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:30,535 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('E:\IntelliJWorkspaceMumu\mumu-spark\spark-warehouse').
2018-02-06 16:17:30,540 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'E:\IntelliJWorkspaceMumu\mumu-spark\spark-warehouse'.
2018-02-06 16:17:30,558 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6a078481{/SQL,null,AVAILABLE,@Spark}
2018-02-06 16:17:30,559 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1929425f{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:30,560 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@676f0a60{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 16:17:30,561 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@535b8c24{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:30,563 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@33a053d{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 16:17:31,848 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 16:17:41,511 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 16:17:41,515 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 16:17:41,518 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<value: string>
2018-02-06 16:17:41,527 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 16:17:42,056 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 206.055426 ms
2018-02-06 16:17:42,118 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 322.9 KB, free 631.5 MB)
2018-02-06 16:17:42,184 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.6 KB, free 631.5 MB)
2018-02-06 16:17:42,187 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:50980 (size: 27.6 KB, free: 631.8 MB)
2018-02-06 16:17:42,197 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from json at BaseSparkSQLTest.java:23
2018-02-06 16:17:42,211 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 16:17:42,423 INFO[org.apache.spark.SparkContext:54] - Starting job: json at BaseSparkSQLTest.java:23
2018-02-06 16:17:42,440 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at BaseSparkSQLTest.java:23) with 1 output partitions
2018-02-06 16:17:42,440 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at BaseSparkSQLTest.java:23)
2018-02-06 16:17:42,441 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 16:17:42,442 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 16:17:42,446 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23), which has no missing parents
2018-02-06 16:17:42,465 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.5 KB, free 631.4 MB)
2018-02-06 16:17:42,472 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KB, free 631.4 MB)
2018-02-06 16:17:42,473 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:50980 (size: 5.3 KB, free: 631.8 MB)
2018-02-06 16:17:42,473 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-06 16:17:42,485 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at BaseSparkSQLTest.java:23) (first 15 tasks are for partitions Vector(0))
2018-02-06 16:17:42,486 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-06 16:17:42,532 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 16:17:42,542 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-06 16:17:42,665 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 16:17:42,740 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 35.771532 ms
2018-02-06 16:17:43,014 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 2733 bytes result sent to driver
2018-02-06 16:17:43,025 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 506 ms on localhost (executor driver) (1/1)
2018-02-06 16:17:43,029 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-06 16:17:43,033 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at BaseSparkSQLTest.java:23) finished in 0.526 s
2018-02-06 16:17:43,040 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at BaseSparkSQLTest.java:23, took 0.616901 s
2018-02-06 16:17:43,118 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Pruning directories with: 
2018-02-06 16:17:43,119 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Post-Scan Filters: 
2018-02-06 16:17:43,120 INFO[org.apache.spark.sql.execution.datasources.FileSourceStrategy:54] - Output Data Schema: struct<bill_accept_org: string, bill_category_name: string, bill_classify: string, bill_desc: string, bill_discount_amount: double ... 22 more fields>
2018-02-06 16:17:43,120 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Pushed Filters: 
2018-02-06 16:17:43,159 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 322.9 KB, free 631.1 MB)
2018-02-06 16:17:43,172 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KB, free 631.1 MB)
2018-02-06 16:17:43,174 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:50980 (size: 27.6 KB, free: 631.7 MB)
2018-02-06 16:17:43,176 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from show at BaseSparkSQL.java:23
2018-02-06 16:17:43,183 INFO[org.apache.spark.sql.execution.FileSourceScanExec:54] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2018-02-06 16:17:43,203 INFO[org.apache.spark.SparkContext:54] - Starting job: show at BaseSparkSQL.java:23
2018-02-06 16:17:43,204 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at BaseSparkSQL.java:23) with 1 output partitions
2018-02-06 16:17:43,204 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at BaseSparkSQL.java:23)
2018-02-06 16:17:43,204 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-06 16:17:43,204 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-06 16:17:43,205 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at BaseSparkSQL.java:23), which has no missing parents
2018-02-06 16:17:43,215 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 10.9 KB, free 631.1 MB)
2018-02-06 16:17:43,221 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KB, free 631.1 MB)
2018-02-06 16:17:43,223 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:50980 (size: 5.8 KB, free: 631.7 MB)
2018-02-06 16:17:43,224 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-06 16:17:43,225 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at BaseSparkSQL.java:23) (first 15 tasks are for partitions Vector(0))
2018-02-06 16:17:43,225 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-06 16:17:43,227 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 5355 bytes)
2018-02-06 16:17:43,228 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-06 16:17:43,245 INFO[org.apache.spark.sql.execution.datasources.FileScanRDD:54] - Reading File path: hdfs://192.168.11.25:9000/mumu/spark/sparksql/data/bill.json/part-00000-55dbc4d2-c7b6-4c88-8e29-4296afca15bc-c000.json, range: 0-39692, partition values: [empty row]
2018-02-06 16:17:43,293 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 37.193612 ms
2018-02-06 16:17:43,356 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 5386 bytes result sent to driver
2018-02-06 16:17:43,357 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 131 ms on localhost (executor driver) (1/1)
2018-02-06 16:17:43,357 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-06 16:17:43,358 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at BaseSparkSQL.java:23) finished in 0.132 s
2018-02-06 16:17:43,358 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at BaseSparkSQL.java:23, took 0.155522 s
2018-02-06 16:17:43,409 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 34.309131 ms
2018-02-06 16:17:43,442 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 16:17:43,447 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@73d06cf2{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:17:43,449 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 16:17:43,465 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 16:17:43,504 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 16:17:43,505 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 16:17:43,513 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 16:17:43,522 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 16:17:43,529 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 16:17:43,532 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 16:17:43,534 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-e422fee0-34c5-4b09-b514-161b5fb844e8
2018-02-06 16:17:57,263 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 16:17:57,752 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 16:17:57,775 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 16:17:57,776 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 16:17:57,777 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 16:17:57,777 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 16:17:57,778 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 16:17:58,256 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 51031.
2018-02-06 16:17:58,290 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 16:17:58,318 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 16:17:58,321 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 16:17:58,322 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 16:17:58,333 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-33c91724-b970-4ca3-89ef-3962bd6e7efb
2018-02-06 16:17:58,355 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 16:17:58,412 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 16:17:58,505 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @4351ms
2018-02-06 16:17:58,570 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 16:17:58,584 INFO[org.spark_project.jetty.server.Server:403] - Started @4431ms
2018-02-06 16:17:58,634 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@170cf454{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:17:58,635 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 16:17:58,665 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18e7143f{/jobs,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,666 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@770d4269{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,667 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1922e6d{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,670 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74bdc168{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,671 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@532a02d9{/stages,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,672 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7bb3a9fe{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,673 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7f811d00{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,674 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7807ac2c{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,675 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4b6166aa{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,677 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4fd4cae3{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,677 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a1217f9{/storage,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,678 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@523424b5{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,679 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@319dead1{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,680 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a7e2d9d{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,681 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b52c0d6{/environment,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,681 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4cc76301{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,690 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f19b8b3{/executors,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,694 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a486d78{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,697 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7ef2d7a6{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,698 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c36250e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,705 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@49f5c307{/static,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,706 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@724f138e{/,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,707 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@32fe9d0a{/api,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,708 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@47428937{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,709 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7caa550{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 16:17:58,711 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 16:17:58,823 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 16:17:58,856 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51044.
2018-02-06 16:17:58,857 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:51044
2018-02-06 16:17:58,859 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 16:17:58,862 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 51044, None)
2018-02-06 16:17:58,864 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:51044 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 51044, None)
2018-02-06 16:17:58,876 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 51044, None)
2018-02-06 16:17:58,878 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 51044, None)
2018-02-06 16:17:59,159 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@253c1256{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:59,194 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('E:\IntelliJWorkspaceMumu\mumu-spark\hive-warehouse').
2018-02-06 16:17:59,194 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'E:\IntelliJWorkspaceMumu\mumu-spark\hive-warehouse'.
2018-02-06 16:17:59,203 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@907f2b7{/SQL,null,AVAILABLE,@Spark}
2018-02-06 16:17:59,204 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@537b32ef{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:59,205 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@304b9f1a{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 16:17:59,206 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@107e5441{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 16:17:59,208 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e4ea0bd{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 16:18:00,380 INFO[org.apache.spark.sql.hive.HiveUtils:54] - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2018-02-06 16:18:01,059 INFO[org.apache.hadoop.hive.metastore.HiveMetaStore:589] - 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
2018-02-06 16:18:01,091 INFO[org.apache.hadoop.hive.metastore.ObjectStore:289] - ObjectStore, initialize called
2018-02-06 16:18:01,283 INFO[DataNucleus.Persistence:77] - Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
2018-02-06 16:18:01,284 INFO[DataNucleus.Persistence:77] - Property datanucleus.cache.level2 unknown - will be ignored
2018-02-06 16:18:03,550 INFO[org.apache.hadoop.hive.metastore.ObjectStore:370] - Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
2018-02-06 16:18:04,857 INFO[DataNucleus.Datastore:77] - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2018-02-06 16:18:04,858 INFO[DataNucleus.Datastore:77] - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2018-02-06 16:18:05,873 INFO[DataNucleus.Datastore:77] - The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
2018-02-06 16:18:05,873 INFO[DataNucleus.Datastore:77] - The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
2018-02-06 16:18:06,105 INFO[org.apache.hadoop.hive.metastore.MetaStoreDirectSql:139] - Using direct SQL, underlying DB is DERBY
2018-02-06 16:18:06,107 INFO[org.apache.hadoop.hive.metastore.ObjectStore:272] - Initialized ObjectStore
2018-02-06 16:18:06,244 WARN[org.apache.hadoop.hive.metastore.ObjectStore:6666] - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
2018-02-06 16:18:06,392 WARN[org.apache.hadoop.hive.metastore.ObjectStore:568] - Failed to get database default, returning NoSuchObjectException
2018-02-06 16:18:06,617 INFO[org.apache.hadoop.hive.metastore.HiveMetaStore:663] - Added admin role in metastore
2018-02-06 16:18:06,623 INFO[org.apache.hadoop.hive.metastore.HiveMetaStore:672] - Added public role in metastore
2018-02-06 16:18:06,729 INFO[org.apache.hadoop.hive.metastore.HiveMetaStore:712] - No user is added in admin role, since config is empty
2018-02-06 16:18:06,845 INFO[org.apache.hadoop.hive.metastore.HiveMetaStore:746] - 0: get_all_databases
2018-02-06 16:18:06,848 INFO[org.apache.hadoop.hive.metastore.HiveMetaStore.audit:371] - ugi=Administrator	ip=unknown-ip-addr	cmd=get_all_databases	
2018-02-06 16:18:06,870 INFO[org.apache.hadoop.hive.metastore.HiveMetaStore:746] - 0: get_functions: db=default pat=*
2018-02-06 16:18:06,870 INFO[org.apache.hadoop.hive.metastore.HiveMetaStore.audit:371] - ugi=Administrator	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
2018-02-06 16:18:06,872 INFO[DataNucleus.Datastore:77] - The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
2018-02-06 16:18:07,622 INFO[org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/Administrator
2018-02-06 16:18:07,623 INFO[org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/Administrator/AppData/Local/Temp/Administrator
2018-02-06 16:18:07,625 INFO[org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/Administrator/AppData/Local/Temp/a43804ee-d12c-4b2c-8c62-5b0276fd5db4_resources
2018-02-06 16:18:07,626 INFO[org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/Administrator/a43804ee-d12c-4b2c-8c62-5b0276fd5db4
2018-02-06 16:18:07,628 INFO[org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/Administrator/AppData/Local/Temp/Administrator/a43804ee-d12c-4b2c-8c62-5b0276fd5db4
2018-02-06 16:18:07,630 INFO[org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/Administrator/a43804ee-d12c-4b2c-8c62-5b0276fd5db4/_tmp_space.db
2018-02-06 16:18:07,633 INFO[org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is E:\IntelliJWorkspaceMumu\mumu-spark\hive-warehouse
2018-02-06 16:18:07,668 INFO[org.apache.hadoop.hive.metastore.HiveMetaStore:746] - 0: get_database: default
2018-02-06 16:18:07,670 INFO[org.apache.hadoop.hive.metastore.HiveMetaStore.audit:371] - ugi=Administrator	ip=unknown-ip-addr	cmd=get_database: default	
2018-02-06 16:18:07,699 INFO[org.apache.hadoop.hive.metastore.HiveMetaStore:746] - 0: get_database: global_temp
2018-02-06 16:18:07,699 INFO[org.apache.hadoop.hive.metastore.HiveMetaStore.audit:371] - ugi=Administrator	ip=unknown-ip-addr	cmd=get_database: global_temp	
2018-02-06 16:18:07,700 WARN[org.apache.hadoop.hive.metastore.ObjectStore:568] - Failed to get database global_temp, returning NoSuchObjectException
2018-02-06 16:18:08,116 INFO[org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/Administrator/AppData/Local/Temp/4153694e-60af-4ccb-a662-a0a468c41777_resources
2018-02-06 16:18:08,117 INFO[org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/Administrator/4153694e-60af-4ccb-a662-a0a468c41777
2018-02-06 16:18:08,118 INFO[org.apache.hadoop.hive.ql.session.SessionState:641] - Created local directory: C:/Users/Administrator/AppData/Local/Temp/Administrator/4153694e-60af-4ccb-a662-a0a468c41777
2018-02-06 16:18:08,121 INFO[org.apache.hadoop.hive.ql.session.SessionState:641] - Created HDFS directory: /tmp/hive/Administrator/4153694e-60af-4ccb-a662-a0a468c41777/_tmp_space.db
2018-02-06 16:18:08,122 INFO[org.apache.spark.sql.hive.client.HiveClientImpl:54] - Warehouse location for Hive client (version 1.2.1) is E:\IntelliJWorkspaceMumu\mumu-spark\hive-warehouse
2018-02-06 16:18:08,222 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-06 16:18:08,233 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive
2018-02-06 16:18:08,740 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-06 16:18:08,753 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@170cf454{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:18:08,757 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-06 16:18:08,769 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-06 16:18:08,783 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-06 16:18:08,783 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-06 16:18:08,792 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-06 16:18:08,795 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-06 16:18:08,799 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-06 16:18:08,800 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-06 16:18:08,801 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-3a23fb45-52ab-4ef2-a3ca-328a53a7f595
2018-02-06 16:32:12,000 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 16:32:12,467 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 16:32:12,502 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 16:32:12,503 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 16:32:12,504 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 16:32:12,505 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 16:32:12,505 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 16:32:12,998 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 51950.
2018-02-06 16:32:13,023 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 16:32:13,048 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 16:32:13,052 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 16:32:13,053 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 16:32:13,065 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-ebc4f4b1-0b19-49ed-9ad3-c960b1e434e4
2018-02-06 16:32:13,096 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 16:32:13,157 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 16:32:13,277 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3512ms
2018-02-06 16:32:13,345 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 16:32:13,360 INFO[org.spark_project.jetty.server.Server:403] - Started @3597ms
2018-02-06 16:32:13,383 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@7f69d591{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:32:13,383 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 16:32:13,408 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f9b7332{/jobs,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,409 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4a8ab068{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,410 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@76a82f33{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,411 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@644c78d4{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,411 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@611f8234{/stages,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,412 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7cbee484{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,412 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@62923ee6{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,414 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@b91d8c4{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,415 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a77614d{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4a067c25{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,417 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3bde62ff{/storage,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,418 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2baa8d82{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,419 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@791cbf87{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,419 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@754777cd{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,422 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@372ea2bc{/environment,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,422 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f08c4b{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,423 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7de0c6ae{/executors,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,424 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@cdc3aae{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,425 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5dcbb60{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,426 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@21526f6c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,436 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@299266e2{/static,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,437 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@37eeec90{/,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,438 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@c9413d8{/api,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,439 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3b9d6699{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,440 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@21694e53{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,443 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 16:32:13,596 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 16:32:13,634 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51963.
2018-02-06 16:32:13,635 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:51963
2018-02-06 16:32:13,637 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 16:32:13,642 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 51963, None)
2018-02-06 16:32:13,649 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:51963 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 51963, None)
2018-02-06 16:32:13,660 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 51963, None)
2018-02-06 16:32:13,661 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 51963, None)
2018-02-06 16:32:13,926 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@8dfe921{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,975 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('E:\IntelliJWorkspaceMumu\mumu-spark\hive-warehouse').
2018-02-06 16:32:13,976 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'E:\IntelliJWorkspaceMumu\mumu-spark\hive-warehouse'.
2018-02-06 16:32:13,985 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f815e7f{/SQL,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,986 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@304b9f1a{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,987 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1f14f20c{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,987 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@62315f22{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 16:32:13,991 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@48e7b3d2{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 16:33:22,155 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 16:33:22,625 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 16:33:22,647 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 16:33:22,647 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 16:33:22,648 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 16:33:22,649 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 16:33:22,649 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 16:33:23,130 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 52043.
2018-02-06 16:33:23,153 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 16:33:23,173 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 16:33:23,177 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 16:33:23,177 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 16:33:23,189 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-466bc66c-3e30-4889-b9b6-d3d2792728fa
2018-02-06 16:33:23,213 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 16:33:23,270 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 16:33:23,361 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2987ms
2018-02-06 16:33:23,425 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 16:33:23,439 INFO[org.spark_project.jetty.server.Server:403] - Started @3065ms
2018-02-06 16:33:23,458 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@5e944453{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:33:23,459 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 16:33:23,481 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18e7143f{/jobs,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,482 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@770d4269{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,483 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1922e6d{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,485 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74bdc168{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,485 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@532a02d9{/stages,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,486 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7bb3a9fe{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,487 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7f811d00{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,488 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7807ac2c{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,489 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4b6166aa{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,490 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4fd4cae3{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,491 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a1217f9{/storage,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,491 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@523424b5{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,493 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@319dead1{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,493 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a7e2d9d{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,494 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b52c0d6{/environment,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,495 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4cc76301{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,496 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f19b8b3{/executors,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,497 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a486d78{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,498 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7ef2d7a6{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,498 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c36250e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,504 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@49f5c307{/static,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,505 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@724f138e{/,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,506 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@32fe9d0a{/api,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,507 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@47428937{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,508 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7caa550{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,510 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 16:33:23,591 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 16:33:23,621 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52056.
2018-02-06 16:33:23,622 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:52056
2018-02-06 16:33:23,624 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 16:33:23,625 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 52056, None)
2018-02-06 16:33:23,629 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:52056 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 52056, None)
2018-02-06 16:33:23,632 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 52056, None)
2018-02-06 16:33:23,633 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 52056, None)
2018-02-06 16:33:23,832 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@253c1256{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,860 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('E:\IntelliJWorkspaceMumu\mumu-spark\hive-warehouse').
2018-02-06 16:33:23,861 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'E:\IntelliJWorkspaceMumu\mumu-spark\hive-warehouse'.
2018-02-06 16:33:23,869 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@907f2b7{/SQL,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,870 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@537b32ef{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,871 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@304b9f1a{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,872 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@107e5441{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 16:33:23,875 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e4ea0bd{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 16:38:55,504 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 16:38:55,926 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 16:38:55,948 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 16:38:55,948 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 16:38:55,949 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 16:38:55,950 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 16:38:55,950 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 16:38:56,402 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 52360.
2018-02-06 16:38:56,422 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 16:38:56,443 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 16:38:56,447 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 16:38:56,447 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 16:38:56,456 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-6e7d6e0a-1052-48ec-9ece-4975a1fc5127
2018-02-06 16:38:56,479 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 16:38:56,538 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 16:38:56,631 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2850ms
2018-02-06 16:38:56,696 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 16:38:56,710 INFO[org.spark_project.jetty.server.Server:403] - Started @2930ms
2018-02-06 16:38:56,731 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@770e4b0{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:38:56,731 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 16:38:56,757 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18e7143f{/jobs,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,758 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@770d4269{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,758 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1922e6d{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,759 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74bdc168{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,760 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@532a02d9{/stages,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,760 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7bb3a9fe{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,761 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7f811d00{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,763 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7807ac2c{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,764 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4b6166aa{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,765 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4fd4cae3{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,766 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a1217f9{/storage,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,767 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@523424b5{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,767 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@319dead1{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,768 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a7e2d9d{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,768 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b52c0d6{/environment,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,769 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4cc76301{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,769 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f19b8b3{/executors,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,770 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a486d78{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,771 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7ef2d7a6{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,771 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c36250e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,778 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@49f5c307{/static,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,779 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@724f138e{/,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,779 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@32fe9d0a{/api,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,780 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@47428937{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,781 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7caa550{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 16:38:56,783 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 16:38:56,889 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 16:38:56,924 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52373.
2018-02-06 16:38:56,925 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:52373
2018-02-06 16:38:56,928 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 16:38:56,931 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 52373, None)
2018-02-06 16:38:56,936 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:52373 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 52373, None)
2018-02-06 16:38:56,942 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 52373, None)
2018-02-06 16:38:56,943 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 52373, None)
2018-02-06 16:38:57,123 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@253c1256{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 16:38:57,159 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('E:\IntelliJWorkspaceMumu\mumu-spark\hive-warehouse').
2018-02-06 16:38:57,160 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'E:\IntelliJWorkspaceMumu\mumu-spark\hive-warehouse'.
2018-02-06 16:38:57,169 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@907f2b7{/SQL,null,AVAILABLE,@Spark}
2018-02-06 16:38:57,171 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@537b32ef{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 16:38:57,173 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@304b9f1a{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 16:38:57,173 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@107e5441{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 16:38:57,176 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e4ea0bd{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 16:40:35,107 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 16:40:35,553 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 16:40:35,574 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 16:40:35,576 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 16:40:35,576 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 16:40:35,577 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 16:40:35,577 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 16:40:36,025 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 52466.
2018-02-06 16:40:36,047 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 16:40:36,066 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 16:40:36,069 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 16:40:36,070 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 16:40:36,080 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-bbd74ecc-9f5a-454f-aed7-0bd5a5b434ae
2018-02-06 16:40:36,105 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 16:40:36,162 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 16:40:36,256 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2856ms
2018-02-06 16:40:36,313 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 16:40:36,326 INFO[org.spark_project.jetty.server.Server:403] - Started @2928ms
2018-02-06 16:40:36,346 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@545e4480{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 16:40:36,346 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 16:40:36,369 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f9b7332{/jobs,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,369 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4a8ab068{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,370 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@76a82f33{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,371 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@644c78d4{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,372 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@611f8234{/stages,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,373 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7cbee484{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,373 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@62923ee6{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,375 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@b91d8c4{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,376 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a77614d{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,376 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4a067c25{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,377 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3bde62ff{/storage,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,378 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2baa8d82{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,378 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@791cbf87{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,379 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@754777cd{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,379 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@372ea2bc{/environment,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,380 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2f08c4b{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,381 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7de0c6ae{/executors,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,382 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@cdc3aae{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,382 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5dcbb60{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,383 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@21526f6c{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,391 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@299266e2{/static,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,392 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@37eeec90{/,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,394 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@c9413d8{/api,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,394 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3b9d6699{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,395 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@21694e53{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,397 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 16:40:36,481 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 16:40:36,521 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52479.
2018-02-06 16:40:36,522 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:52479
2018-02-06 16:40:36,524 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 16:40:36,526 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 52479, None)
2018-02-06 16:40:36,540 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:52479 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 52479, None)
2018-02-06 16:40:36,549 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 52479, None)
2018-02-06 16:40:36,550 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 52479, None)
2018-02-06 16:40:36,817 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@8dfe921{/metrics/json,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,848 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('E:\IntelliJWorkspaceMumu\mumu-spark\hive-warehouse').
2018-02-06 16:40:36,849 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'E:\IntelliJWorkspaceMumu\mumu-spark\hive-warehouse'.
2018-02-06 16:40:36,860 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f815e7f{/SQL,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,861 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@304b9f1a{/SQL/json,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,862 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1f14f20c{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,863 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@62315f22{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-06 16:40:36,865 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@48e7b3d2{/static/sql,null,AVAILABLE,@Spark}
2018-02-06 16:40:38,053 INFO[org.apache.spark.sql.hive.HiveUtils:54] - Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
2018-02-06 17:20:46,231 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-06 17:20:46,784 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-06 17:20:46,826 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-06 17:20:46,827 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-06 17:20:46,828 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-06 17:20:46,829 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-06 17:20:46,831 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-06 17:20:47,435 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 54172.
2018-02-06 17:20:47,463 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-06 17:20:47,520 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-06 17:20:47,524 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-06 17:20:47,524 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-06 17:20:47,537 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-6019d150-e3c7-476c-8c08-60c94d165fd8
2018-02-06 17:20:47,573 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-06 17:20:47,641 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-06 17:20:47,766 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3775ms
2018-02-06 17:20:47,859 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-06 17:20:47,875 INFO[org.spark_project.jetty.server.Server:403] - Started @3885ms
2018-02-06 17:20:47,910 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@7a8b8916{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-06 17:20:47,910 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-06 17:20:47,941 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/jobs,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,941 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@419a20a6{/jobs/json,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,942 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3383649e{/jobs/job,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,943 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ce61929{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,944 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4bf3798b{/stages,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,945 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@74e47444{/stages/json,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,946 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@59d2103b{/stages/stage,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,949 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@46f699d5{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,949 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1991f767{/stages/pool,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,950 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c6daf0{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,951 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@659eef7{/storage,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,951 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2488b073{/storage/json,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,952 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55787112{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,955 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7db82169{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,956 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@f74e835{/environment,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,957 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f28bd56{/environment/json,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,957 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@19fe4644{/executors,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,958 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5be067de{/executors/json,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,959 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@18245eb0{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,959 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@24fb6a80{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,969 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@72a85671{/static,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,970 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@297ea53a{/,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,972 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5bf22f18{/api,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,973 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@675d8c96{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,973 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2ed3b1f5{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-06 17:20:47,976 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-06 17:20:48,069 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-06 17:20:48,100 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54185.
2018-02-06 17:20:48,101 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:54185
2018-02-06 17:20:48,107 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-06 17:20:48,110 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 54185, None)
2018-02-06 17:20:48,115 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:54185 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 54185, None)
2018-02-06 17:20:48,125 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 54185, None)
2018-02-06 17:20:48,127 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 54185, None)
2018-02-06 17:20:48,404 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@39fc6b2c{/metrics/json,null,AVAILABLE,@Spark}
