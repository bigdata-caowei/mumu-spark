2018-02-05 10:44:11,017 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 10:44:11,756 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 10:44:11,803 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 10:44:11,804 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 10:44:11,806 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 10:44:11,808 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 10:44:11,810 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 10:44:12,412 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 50739.
2018-02-05 10:44:12,450 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 10:44:12,535 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 10:44:12,543 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 10:44:12,544 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 10:44:12,558 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-9d6482ab-ec19-4f6a-9c11-9eb8f21761a5
2018-02-05 10:44:12,594 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 10:44:12,682 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 10:44:12,795 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3783ms
2018-02-05 10:44:12,900 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 10:44:12,919 INFO[org.spark_project.jetty.server.Server:403] - Started @3908ms
2018-02-05 10:44:12,947 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@ce699dd{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 10:44:12,947 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 10:44:12,988 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@61bcd567{/jobs,null,AVAILABLE,@Spark}
2018-02-05 10:44:12,989 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2e54db99{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 10:44:12,990 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4f2613d1{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 10:44:12,991 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@773f7880{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 10:44:12,991 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/stages,null,AVAILABLE,@Spark}
2018-02-05 10:44:12,992 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@83298d7{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 10:44:12,993 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5b080f3a{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 10:44:12,994 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7d3430a7{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 10:44:12,994 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2756c0a7{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 10:44:12,995 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@69637b10{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 10:44:12,998 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@165b2f7f{/storage,null,AVAILABLE,@Spark}
2018-02-05 10:44:12,998 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@277f7dd3{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 10:44:12,999 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2364305a{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 10:44:13,000 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bc715b8{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 10:44:13,000 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/environment,null,AVAILABLE,@Spark}
2018-02-05 10:44:13,001 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 10:44:13,001 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/executors,null,AVAILABLE,@Spark}
2018-02-05 10:44:13,002 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 10:44:13,003 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 10:44:13,004 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 10:44:13,012 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/static,null,AVAILABLE,@Spark}
2018-02-05 10:44:13,013 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/,null,AVAILABLE,@Spark}
2018-02-05 10:44:13,014 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/api,null,AVAILABLE,@Spark}
2018-02-05 10:44:13,015 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5860f3d7{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 10:44:13,015 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 10:44:13,019 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 10:44:13,256 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Connecting to master spark://192.168.11.25:7077...
2018-02-05 10:44:13,345 INFO[org.apache.spark.network.client.TransportClientFactory:254] - Successfully created connection to /192.168.11.25:7077 after 50 ms (0 ms spent in bootstraps)
2018-02-05 10:44:13,485 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Connected to Spark cluster with app ID app-20180205104505-0000
2018-02-05 10:44:13,491 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor added: app-20180205104505-0000/0 on worker-20180205104345-192.168.11.25-46218 (192.168.11.25:46218) with 4 cores
2018-02-05 10:44:13,492 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Granted executor ID app-20180205104505-0000/0 on hostPort 192.168.11.25:46218 with 4 cores, 1024.0 MB RAM
2018-02-05 10:44:13,505 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50761.
2018-02-05 10:44:13,506 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:50761
2018-02-05 10:44:13,508 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 10:44:13,511 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 50761, None)
2018-02-05 10:44:13,518 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:50761 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 50761, None)
2018-02-05 10:44:13,521 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 50761, None)
2018-02-05 10:44:13,522 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 50761, None)
2018-02-05 10:44:13,555 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205104505-0000/0 is now RUNNING
2018-02-05 10:44:14,081 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@10ad20cb{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 10:44:14,109 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2018-02-05 10:44:14,604 INFO[org.apache.spark.SparkContext:54] - Starting job: count at SparkRDDOperation.java:31
2018-02-05 10:44:14,647 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (count at SparkRDDOperation.java:31) with 2 output partitions
2018-02-05 10:44:14,648 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (count at SparkRDDOperation.java:31)
2018-02-05 10:44:14,649 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-05 10:44:14,662 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-05 10:44:14,691 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at SparkRDDOperation.java:28), which has no missing parents
2018-02-05 10:44:14,843 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.25:35852) with ID 0
2018-02-05 10:44:14,894 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.25:45435 with 366.3 MB RAM, BlockManagerId(0, 192.168.11.25, 45435, None)
2018-02-05 10:44:14,940 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 1448.0 B, free 631.8 MB)
2018-02-05 10:44:14,992 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 1006.0 B, free 631.8 MB)
2018-02-05 10:44:15,000 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:50761 (size: 1006.0 B, free: 631.8 MB)
2018-02-05 10:44:15,003 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-05 10:44:15,322 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at SparkRDDOperation.java:28) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 10:44:15,323 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 2 tasks
2018-02-05 10:44:15,387 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, 192.168.11.25, executor 0, partition 0, PROCESS_LOCAL, 4847 bytes)
2018-02-05 10:44:15,399 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, 192.168.11.25, executor 0, partition 1, PROCESS_LOCAL, 4844 bytes)
2018-02-05 10:44:15,633 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.25:45435 (size: 1006.0 B, free: 366.3 MB)
2018-02-05 10:44:15,891 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_0_0 in memory on 192.168.11.25:45435 (size: 128.0 B, free: 366.3 MB)
2018-02-05 10:44:15,892 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_0_1 in memory on 192.168.11.25:45435 (size: 120.0 B, free: 366.3 MB)
2018-02-05 10:44:15,927 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 567 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 10:44:15,928 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 539 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 10:44:15,929 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 10:44:15,936 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (count at SparkRDDOperation.java:31) finished in 0.583 s
2018-02-05 10:44:15,941 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: count at SparkRDDOperation.java:31, took 1.337078 s
2018-02-05 10:44:15,956 INFO[org.apache.spark.SparkContext:54] - Starting job: collect at SparkRDDOperation.java:32
2018-02-05 10:44:15,957 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (collect at SparkRDDOperation.java:32) with 2 output partitions
2018-02-05 10:44:15,957 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (collect at SparkRDDOperation.java:32)
2018-02-05 10:44:15,957 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-05 10:44:15,960 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-05 10:44:15,960 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (ParallelCollectionRDD[0] at parallelize at SparkRDDOperation.java:28), which has no missing parents
2018-02-05 10:44:15,963 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 1616.0 B, free 631.8 MB)
2018-02-05 10:44:15,966 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 1050.0 B, free 631.8 MB)
2018-02-05 10:44:15,967 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:50761 (size: 1050.0 B, free: 631.8 MB)
2018-02-05 10:44:15,968 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-05 10:44:15,969 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 1 (ParallelCollectionRDD[0] at parallelize at SparkRDDOperation.java:28) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 10:44:15,969 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 2 tasks
2018-02-05 10:44:15,974 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 2, 192.168.11.25, executor 0, partition 0, PROCESS_LOCAL, 4847 bytes)
2018-02-05 10:44:15,974 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 3, 192.168.11.25, executor 0, partition 1, PROCESS_LOCAL, 4844 bytes)
2018-02-05 10:44:15,990 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.25:45435 (size: 1050.0 B, free: 366.3 MB)
2018-02-05 10:44:16,005 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 2) in 33 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 10:44:16,005 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 3) in 31 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 10:44:16,006 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-05 10:44:16,006 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (collect at SparkRDDOperation.java:32) finished in 0.034 s
2018-02-05 10:44:16,006 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: collect at SparkRDDOperation.java:32, took 0.050260 s
2018-02-05 10:44:16,018 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@ce699dd{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 10:44:16,020 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-05 10:44:16,025 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Shutting down all executors
2018-02-05 10:44:16,025 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Asking each executor to shut down
2018-02-05 10:44:16,038 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-05 10:44:16,053 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-05 10:44:16,053 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-05 10:44:16,058 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-05 10:44:16,060 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-05 10:44:16,064 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-05 10:44:16,068 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-05 10:44:16,070 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-c0a7cfbb-2c99-43ba-95cb-b5748349e9b2
2018-02-05 15:31:49,428 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 15:31:50,276 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 15:31:50,320 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 15:31:50,321 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 15:31:50,322 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 15:31:50,325 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 15:31:50,326 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 15:31:50,884 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 52295.
2018-02-05 15:31:50,911 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 15:31:50,968 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 15:31:50,974 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 15:31:50,975 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 15:31:50,988 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-aecad0cb-7fac-4c25-bf4a-0dca8ea3758a
2018-02-05 15:31:51,021 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 15:31:51,070 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 15:31:51,181 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3247ms
2018-02-05 15:31:51,270 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 15:31:51,287 INFO[org.spark_project.jetty.server.Server:403] - Started @3354ms
2018-02-05 15:31:51,313 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@488488fc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 15:31:51,313 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 15:31:51,341 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7829b776{/jobs,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,342 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2692b61e{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,342 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@62fad19{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,343 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2e54db99{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,344 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4f2613d1{/stages,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,344 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,345 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@878452d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,346 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42a9e5d1{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,347 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@773cbf4f{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,347 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@665e9289{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,348 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f603e89{/storage,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,348 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@350ec41e{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,349 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71984c3{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,349 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5536379e{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,350 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6c8bca63{/environment,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,351 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,352 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/executors,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,352 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f1c5af9{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,353 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,354 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,368 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/static,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,369 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,369 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/api,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,370 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,371 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 15:31:51,373 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 15:31:51,511 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Connecting to master spark://192.168.11.25:7077...
2018-02-05 15:31:51,561 INFO[org.apache.spark.network.client.TransportClientFactory:254] - Successfully created connection to /192.168.11.25:7077 after 36 ms (0 ms spent in bootstraps)
2018-02-05 15:31:51,735 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Connected to Spark cluster with app ID app-20180205153242-0000
2018-02-05 15:31:51,749 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor added: app-20180205153242-0000/0 on worker-20180205151248-192.168.11.25-42981 (192.168.11.25:42981) with 4 cores
2018-02-05 15:31:51,750 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Granted executor ID app-20180205153242-0000/0 on hostPort 192.168.11.25:42981 with 4 cores, 1024.0 MB RAM
2018-02-05 15:31:51,755 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52319.
2018-02-05 15:31:51,756 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:52319
2018-02-05 15:31:51,760 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 15:31:51,764 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 52319, None)
2018-02-05 15:31:51,767 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:52319 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 52319, None)
2018-02-05 15:31:51,770 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 52319, None)
2018-02-05 15:31:51,771 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 52319, None)
2018-02-05 15:31:51,805 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205153242-0000/0 is now RUNNING
2018-02-05 15:31:52,036 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@33aa93c{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 15:31:52,066 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2018-02-05 15:31:53,165 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.25:51045) with ID 0
2018-02-05 15:31:53,204 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.25:52440 with 366.3 MB RAM, BlockManagerId(0, 192.168.11.25, 52440, None)
2018-02-05 15:31:53,230 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 316.2 KB, free 631.5 MB)
2018-02-05 15:31:53,373 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 631.5 MB)
2018-02-05 15:31:53,381 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:52319 (size: 27.2 KB, free: 631.8 MB)
2018-02-05 15:31:53,389 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from textFile at SparkWordCount.java:26
2018-02-05 15:31:53,478 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-05 15:31:53,488 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@488488fc{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 15:31:53,490 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-05 15:31:53,495 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Shutting down all executors
2018-02-05 15:31:53,497 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Asking each executor to shut down
2018-02-05 15:31:53,506 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-05 15:31:53,518 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-05 15:31:53,519 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-05 15:31:53,524 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-05 15:31:53,526 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-05 15:31:53,530 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-05 15:31:53,530 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-05 15:31:53,531 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-4fcb8896-54bb-4579-87cc-fce8ea1a5d8c
2018-02-05 15:32:50,951 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 15:32:51,760 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 15:32:51,797 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 15:32:51,797 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 15:32:51,798 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 15:32:51,799 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 15:32:51,799 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 15:32:52,334 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 52443.
2018-02-05 15:32:52,357 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 15:32:52,409 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 15:32:52,412 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 15:32:52,413 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 15:32:52,424 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-6d86dce9-f09d-4df3-bc66-28d897ab8a3a
2018-02-05 15:32:52,447 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 15:32:52,499 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 15:32:52,588 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2704ms
2018-02-05 15:32:52,691 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 15:32:52,708 INFO[org.spark_project.jetty.server.Server:403] - Started @2825ms
2018-02-05 15:32:52,731 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@ce699dd{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 15:32:52,732 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 15:32:52,758 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7829b776{/jobs,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,758 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2692b61e{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,760 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@62fad19{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,763 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2e54db99{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,764 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4f2613d1{/stages,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,765 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,765 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@878452d{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,767 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42a9e5d1{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,768 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@773cbf4f{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,768 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@665e9289{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,769 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f603e89{/storage,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,770 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@350ec41e{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,771 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71984c3{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,771 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5536379e{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,772 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6c8bca63{/environment,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,773 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,774 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/executors,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,774 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f1c5af9{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,775 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,777 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,783 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/static,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,784 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@16f7b4af{/,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,785 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a1d21f{/api,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,786 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,788 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4e517165{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 15:32:52,790 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 15:32:52,931 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Connecting to master spark://192.168.11.25:7077...
2018-02-05 15:32:53,000 INFO[org.apache.spark.network.client.TransportClientFactory:254] - Successfully created connection to /192.168.11.25:7077 after 45 ms (0 ms spent in bootstraps)
2018-02-05 15:32:53,204 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Connected to Spark cluster with app ID app-20180205153344-0001
2018-02-05 15:32:53,227 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor added: app-20180205153344-0001/0 on worker-20180205151248-192.168.11.25-42981 (192.168.11.25:42981) with 4 cores
2018-02-05 15:32:53,228 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Granted executor ID app-20180205153344-0001/0 on hostPort 192.168.11.25:42981 with 4 cores, 1024.0 MB RAM
2018-02-05 15:32:53,234 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205153344-0001/0 is now RUNNING
2018-02-05 15:32:53,236 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52465.
2018-02-05 15:32:53,237 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:52465
2018-02-05 15:32:53,238 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 15:32:53,241 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 52465, None)
2018-02-05 15:32:53,244 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:52465 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 52465, None)
2018-02-05 15:32:53,247 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 52465, None)
2018-02-05 15:32:53,248 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 52465, None)
2018-02-05 15:32:53,523 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@33aa93c{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 15:32:53,547 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2018-02-05 15:32:54,385 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.25:35277) with ID 0
2018-02-05 15:32:54,536 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.25:46221 with 366.3 MB RAM, BlockManagerId(0, 192.168.11.25, 46221, None)
2018-02-05 15:32:54,674 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 316.2 KB, free 631.5 MB)
2018-02-05 15:32:54,757 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 631.5 MB)
2018-02-05 15:32:54,761 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:52465 (size: 27.2 KB, free: 631.8 MB)
2018-02-05 15:32:54,765 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from textFile at SparkWordCount.java:27
2018-02-05 15:33:02,638 INFO[org.apache.hadoop.mapred.FileInputFormat:256] - Total input files to process : 212
2018-02-05 15:33:02,700 INFO[org.apache.spark.SparkContext:54] - Starting job: collect at SparkWordCount.java:44
2018-02-05 15:33:02,714 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (collect at SparkWordCount.java:44) with 212 output partitions
2018-02-05 15:33:02,715 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (collect at SparkWordCount.java:44)
2018-02-05 15:33:02,715 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-05 15:33:02,717 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-05 15:33:02,729 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:29), which has no missing parents
2018-02-05 15:33:02,769 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 631.5 MB)
2018-02-05 15:33:02,773 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 631.5 MB)
2018-02-05 15:33:02,775 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:52465 (size: 2.2 KB, free: 631.8 MB)
2018-02-05 15:33:02,775 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-05 15:33:02,790 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 212 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:29) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2018-02-05 15:33:02,791 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 212 tasks
2018-02-05 15:33:02,829 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 15:33:02,831 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, 192.168.11.25, executor 0, partition 1, ANY, 4889 bytes)
2018-02-05 15:33:02,832 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 0.0 (TID 2, 192.168.11.25, executor 0, partition 2, ANY, 4889 bytes)
2018-02-05 15:33:02,832 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 0.0 (TID 3, 192.168.11.25, executor 0, partition 3, ANY, 4889 bytes)
2018-02-05 15:33:03,044 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.25:46221 (size: 2.2 KB, free: 366.3 MB)
2018-02-05 15:33:03,244 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 0.0 (TID 4, 192.168.11.25, executor 0, partition 4, ANY, 4889 bytes)
2018-02-05 15:33:03,247 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 0.0 (TID 5, 192.168.11.25, executor 0, partition 5, ANY, 4889 bytes)
2018-02-05 15:33:03,249 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 0.0 (TID 6, 192.168.11.25, executor 0, partition 6, ANY, 4889 bytes)
2018-02-05 15:33:03,252 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 0.0 (TID 7, 192.168.11.25, executor 0, partition 7, ANY, 4889 bytes)
2018-02-05 15:33:03,269 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 0.0 (TID 8, 192.168.11.25, executor 0, partition 8, ANY, 4889 bytes)
2018-02-05 15:33:03,274 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 0.0 (TID 9, 192.168.11.25, executor 0, partition 9, ANY, 4889 bytes)
2018-02-05 15:33:03,275 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 0.0 (TID 10, 192.168.11.25, executor 0, partition 10, ANY, 4889 bytes)
2018-02-05 15:33:03,276 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 0.0 (TID 11, 192.168.11.25, executor 0, partition 11, ANY, 4889 bytes)
2018-02-05 15:33:03,282 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 3.0 in stage 0.0 (TID 3, 192.168.11.25, executor 0): java.lang.ClassNotFoundException: com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:67)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:80)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2018-02-05 15:33:03,285 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 5.0 in stage 0.0 (TID 5) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 1]
2018-02-05 15:33:03,286 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 6.0 in stage 0.0 (TID 6) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 2]
2018-02-05 15:33:03,288 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 2.0 in stage 0.0 (TID 2) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 3]
2018-02-05 15:33:03,299 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 7.0 in stage 0.0 (TID 7) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 4]
2018-02-05 15:33:03,300 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 1.0 in stage 0.0 (TID 1) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 5]
2018-02-05 15:33:03,300 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 0.0 in stage 0.0 (TID 0) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 6]
2018-02-05 15:33:03,304 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.1 in stage 0.0 (TID 12, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 15:33:03,306 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.1 in stage 0.0 (TID 13, 192.168.11.25, executor 0, partition 1, ANY, 4889 bytes)
2018-02-05 15:33:03,307 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.1 in stage 0.0 (TID 14, 192.168.11.25, executor 0, partition 7, ANY, 4889 bytes)
2018-02-05 15:33:03,310 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.1 in stage 0.0 (TID 15, 192.168.11.25, executor 0, partition 2, ANY, 4889 bytes)
2018-02-05 15:33:03,316 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 4.0 in stage 0.0 (TID 4) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 7]
2018-02-05 15:33:03,316 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 8.0 in stage 0.0 (TID 8) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 8]
2018-02-05 15:33:03,317 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 11.0 in stage 0.0 (TID 11) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 9]
2018-02-05 15:33:03,318 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 9.0 in stage 0.0 (TID 9) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 10]
2018-02-05 15:33:03,318 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 10.0 in stage 0.0 (TID 10) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 11]
2018-02-05 15:33:03,321 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.1 in stage 0.0 (TID 16, 192.168.11.25, executor 0, partition 10, ANY, 4889 bytes)
2018-02-05 15:33:03,322 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 0.1 in stage 0.0 (TID 12) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 12]
2018-02-05 15:33:03,323 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.2 in stage 0.0 (TID 17, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 15:33:03,325 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 1.1 in stage 0.0 (TID 13) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 13]
2018-02-05 15:33:03,335 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.2 in stage 0.0 (TID 18, 192.168.11.25, executor 0, partition 1, ANY, 4889 bytes)
2018-02-05 15:33:03,336 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.1 in stage 0.0 (TID 19, 192.168.11.25, executor 0, partition 9, ANY, 4889 bytes)
2018-02-05 15:33:03,337 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 10.1 in stage 0.0 (TID 16) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 14]
2018-02-05 15:33:03,337 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 7.1 in stage 0.0 (TID 14) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 15]
2018-02-05 15:33:03,338 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.2 in stage 0.0 (TID 20, 192.168.11.25, executor 0, partition 7, ANY, 4889 bytes)
2018-02-05 15:33:03,339 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 0.2 in stage 0.0 (TID 17) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 16]
2018-02-05 15:33:03,340 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.3 in stage 0.0 (TID 21, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 15:33:03,342 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 2.1 in stage 0.0 (TID 15) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 17]
2018-02-05 15:33:03,348 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.2 in stage 0.0 (TID 22, 192.168.11.25, executor 0, partition 2, ANY, 4889 bytes)
2018-02-05 15:33:03,349 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.2 in stage 0.0 (TID 23, 192.168.11.25, executor 0, partition 10, ANY, 4889 bytes)
2018-02-05 15:33:03,353 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.1 in stage 0.0 (TID 24, 192.168.11.25, executor 0, partition 11, ANY, 4889 bytes)
2018-02-05 15:33:03,353 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 0.3 in stage 0.0 (TID 21) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 18]
2018-02-05 15:33:03,356 ERROR[org.apache.spark.scheduler.TaskSetManager:70] - Task 0 in stage 0.0 failed 4 times; aborting job
2018-02-05 15:33:03,361 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 7.2 in stage 0.0 (TID 20) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 19]
2018-02-05 15:33:03,362 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 9.1 in stage 0.0 (TID 19) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 20]
2018-02-05 15:33:03,362 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 1.2 in stage 0.0 (TID 18) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 21]
2018-02-05 15:33:03,364 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 10.2 in stage 0.0 (TID 23) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 22]
2018-02-05 15:33:03,366 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 15:33:03,367 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 11.1 in stage 0.0 (TID 24) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 23]
2018-02-05 15:33:03,367 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 15:33:03,367 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 2.2 in stage 0.0 (TID 22) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 24]
2018-02-05 15:33:03,368 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 15:33:03,368 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Cancelling stage 0
2018-02-05 15:33:03,369 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (collect at SparkWordCount.java:44) failed in 0.557 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 21, 192.168.11.25, executor 0): java.lang.ClassNotFoundException: com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:67)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:80)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
2018-02-05 15:33:03,374 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 failed: collect at SparkWordCount.java:44, took 0.672739 s
2018-02-05 15:33:03,390 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-05 15:33:03,398 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@ce699dd{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 15:33:03,402 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-05 15:33:03,407 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Shutting down all executors
2018-02-05 15:33:03,407 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Asking each executor to shut down
2018-02-05 15:33:03,429 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-05 15:33:03,442 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-05 15:33:03,442 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-05 15:33:03,447 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-05 15:33:03,449 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-05 15:33:03,453 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-05 15:33:03,454 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-05 15:33:03,455 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-a603460f-f70c-4a3a-a302-f475da98c3ba
2018-02-05 15:33:52,652 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 15:33:53,558 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 15:33:53,612 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 15:33:53,613 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 15:33:53,613 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 15:33:53,614 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 15:33:53,615 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 15:33:54,099 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 52605.
2018-02-05 15:33:54,119 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 15:33:54,170 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 15:33:54,173 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 15:33:54,174 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 15:33:54,183 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-c4ac0d42-9c82-44f7-bc57-3aa1d89330eb
2018-02-05 15:33:54,208 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 15:33:54,251 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 15:33:54,336 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2756ms
2018-02-05 15:33:54,413 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 15:33:54,430 INFO[org.spark_project.jetty.server.Server:403] - Started @2851ms
2018-02-05 15:33:54,452 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@15eebbff{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 15:33:54,453 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 15:33:54,477 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5778826f{/jobs,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,478 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7b64240d{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,478 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@47dbb1e2{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,480 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55e8ec2f{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,482 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d24ffa1{/stages,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,483 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@773f7880{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,484 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,486 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5b080f3a{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,487 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6b54655f{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,487 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7d3430a7{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,488 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2756c0a7{/storage,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,488 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@69637b10{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,489 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@165b2f7f{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,490 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@277f7dd3{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,491 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2364305a{/environment,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,491 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bc715b8{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,492 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/executors,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,492 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,493 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,495 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,505 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/static,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,506 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,507 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/api,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,508 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,509 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 15:33:54,511 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 15:33:54,663 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Connecting to master spark://192.168.11.25:7077...
2018-02-05 15:33:54,728 INFO[org.apache.spark.network.client.TransportClientFactory:254] - Successfully created connection to /192.168.11.25:7077 after 47 ms (0 ms spent in bootstraps)
2018-02-05 15:33:54,832 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Connected to Spark cluster with app ID app-20180205153445-0002
2018-02-05 15:33:54,834 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor added: app-20180205153445-0002/0 on worker-20180205151248-192.168.11.25-42981 (192.168.11.25:42981) with 4 cores
2018-02-05 15:33:54,835 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Granted executor ID app-20180205153445-0002/0 on hostPort 192.168.11.25:42981 with 4 cores, 1024.0 MB RAM
2018-02-05 15:33:54,848 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205153445-0002/0 is now RUNNING
2018-02-05 15:33:54,852 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52629.
2018-02-05 15:33:54,853 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:52629
2018-02-05 15:33:54,855 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 15:33:54,858 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 52629, None)
2018-02-05 15:33:54,865 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:52629 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 52629, None)
2018-02-05 15:33:54,878 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 52629, None)
2018-02-05 15:33:54,880 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 52629, None)
2018-02-05 15:33:55,097 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@32c0915e{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 15:33:55,118 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2018-02-05 15:33:56,018 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.25:53780) with ID 0
2018-02-05 15:33:56,054 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.25:42724 with 366.3 MB RAM, BlockManagerId(0, 192.168.11.25, 42724, None)
2018-02-05 15:33:56,079 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 316.2 KB, free 631.5 MB)
2018-02-05 15:33:56,159 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 631.5 MB)
2018-02-05 15:33:56,164 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:52629 (size: 27.2 KB, free: 631.8 MB)
2018-02-05 15:33:56,173 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from textFile at SparkWordCount.java:27
2018-02-05 15:34:03,880 INFO[org.apache.hadoop.mapred.FileInputFormat:256] - Total input files to process : 212
2018-02-05 15:34:03,933 INFO[org.apache.spark.SparkContext:54] - Starting job: collect at SparkWordCount.java:44
2018-02-05 15:34:03,943 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (collect at SparkWordCount.java:44) with 212 output partitions
2018-02-05 15:34:03,944 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (collect at SparkWordCount.java:44)
2018-02-05 15:34:03,945 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-05 15:34:03,951 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-05 15:34:03,958 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:29), which has no missing parents
2018-02-05 15:34:04,026 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 631.5 MB)
2018-02-05 15:34:04,032 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 631.5 MB)
2018-02-05 15:34:04,033 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:52629 (size: 2.2 KB, free: 631.8 MB)
2018-02-05 15:34:04,034 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-05 15:34:04,049 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 212 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:29) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2018-02-05 15:34:04,051 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 212 tasks
2018-02-05 15:34:04,099 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 15:34:04,101 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, 192.168.11.25, executor 0, partition 1, ANY, 4889 bytes)
2018-02-05 15:34:04,102 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 0.0 (TID 2, 192.168.11.25, executor 0, partition 2, ANY, 4889 bytes)
2018-02-05 15:34:04,102 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 0.0 (TID 3, 192.168.11.25, executor 0, partition 3, ANY, 4889 bytes)
2018-02-05 15:34:04,306 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.25:42724 (size: 2.2 KB, free: 366.3 MB)
2018-02-05 15:34:04,424 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 0.0 (TID 4, 192.168.11.25, executor 0, partition 4, ANY, 4889 bytes)
2018-02-05 15:34:04,428 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 0.0 (TID 5, 192.168.11.25, executor 0, partition 5, ANY, 4889 bytes)
2018-02-05 15:34:04,430 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 0.0 (TID 6, 192.168.11.25, executor 0, partition 6, ANY, 4889 bytes)
2018-02-05 15:34:04,431 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 0.0 (TID 7, 192.168.11.25, executor 0, partition 7, ANY, 4889 bytes)
2018-02-05 15:34:04,440 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 0.0 (TID 8, 192.168.11.25, executor 0, partition 8, ANY, 4889 bytes)
2018-02-05 15:34:04,448 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 0.0 (TID 9, 192.168.11.25, executor 0, partition 9, ANY, 4889 bytes)
2018-02-05 15:34:04,456 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 3.0 in stage 0.0 (TID 3, 192.168.11.25, executor 0): java.lang.ClassNotFoundException: com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:67)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:80)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2018-02-05 15:34:04,463 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 4.0 in stage 0.0 (TID 4) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 1]
2018-02-05 15:34:04,469 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 5.0 in stage 0.0 (TID 5) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 2]
2018-02-05 15:34:04,469 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 0.0 in stage 0.0 (TID 0) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 3]
2018-02-05 15:34:04,476 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 2.0 in stage 0.0 (TID 2) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 4]
2018-02-05 15:34:04,478 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.1 in stage 0.0 (TID 10, 192.168.11.25, executor 0, partition 2, ANY, 4889 bytes)
2018-02-05 15:34:04,480 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.1 in stage 0.0 (TID 11, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 15:34:04,485 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.1 in stage 0.0 (TID 12, 192.168.11.25, executor 0, partition 5, ANY, 4889 bytes)
2018-02-05 15:34:04,494 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.1 in stage 0.0 (TID 13, 192.168.11.25, executor 0, partition 4, ANY, 4889 bytes)
2018-02-05 15:34:04,497 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.1 in stage 0.0 (TID 14, 192.168.11.25, executor 0, partition 3, ANY, 4889 bytes)
2018-02-05 15:34:04,503 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 0.0 (TID 15, 192.168.11.25, executor 0, partition 10, ANY, 4889 bytes)
2018-02-05 15:34:04,504 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 1.0 in stage 0.0 (TID 1) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 5]
2018-02-05 15:34:04,506 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 9.0 in stage 0.0 (TID 9) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 6]
2018-02-05 15:34:04,507 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.1 in stage 0.0 (TID 16, 192.168.11.25, executor 0, partition 9, ANY, 4889 bytes)
2018-02-05 15:34:04,507 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 8.0 in stage 0.0 (TID 8) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 7]
2018-02-05 15:34:04,508 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.1 in stage 0.0 (TID 17, 192.168.11.25, executor 0, partition 8, ANY, 4889 bytes)
2018-02-05 15:34:04,508 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 6.0 in stage 0.0 (TID 6) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 8]
2018-02-05 15:34:04,509 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 7.0 in stage 0.0 (TID 7) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 9]
2018-02-05 15:34:04,509 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 5.1 in stage 0.0 (TID 12) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 10]
2018-02-05 15:34:04,510 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.2 in stage 0.0 (TID 18, 192.168.11.25, executor 0, partition 5, ANY, 4889 bytes)
2018-02-05 15:34:04,523 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 3.1 in stage 0.0 (TID 14) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 11]
2018-02-05 15:34:04,524 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 0.1 in stage 0.0 (TID 11) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 12]
2018-02-05 15:34:04,525 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 4.1 in stage 0.0 (TID 13) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 13]
2018-02-05 15:34:04,525 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 2.1 in stage 0.0 (TID 10) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 14]
2018-02-05 15:34:04,526 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.2 in stage 0.0 (TID 19, 192.168.11.25, executor 0, partition 2, ANY, 4889 bytes)
2018-02-05 15:34:04,529 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.2 in stage 0.0 (TID 20, 192.168.11.25, executor 0, partition 4, ANY, 4889 bytes)
2018-02-05 15:34:04,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 10.0 in stage 0.0 (TID 15) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 15]
2018-02-05 15:34:04,532 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.1 in stage 0.0 (TID 21, 192.168.11.25, executor 0, partition 10, ANY, 4889 bytes)
2018-02-05 15:34:04,532 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 9.1 in stage 0.0 (TID 16) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 16]
2018-02-05 15:34:04,534 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.2 in stage 0.0 (TID 22, 192.168.11.25, executor 0, partition 9, ANY, 4889 bytes)
2018-02-05 15:34:04,536 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 8.1 in stage 0.0 (TID 17) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 17]
2018-02-05 15:34:04,537 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 5.2 in stage 0.0 (TID 18) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 18]
2018-02-05 15:34:04,539 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.3 in stage 0.0 (TID 23, 192.168.11.25, executor 0, partition 5, ANY, 4889 bytes)
2018-02-05 15:34:04,540 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 2.2 in stage 0.0 (TID 19) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 19]
2018-02-05 15:34:04,546 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.3 in stage 0.0 (TID 24, 192.168.11.25, executor 0, partition 2, ANY, 4889 bytes)
2018-02-05 15:34:04,547 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 4.2 in stage 0.0 (TID 20) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 20]
2018-02-05 15:34:04,549 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.3 in stage 0.0 (TID 25, 192.168.11.25, executor 0, partition 4, ANY, 4889 bytes)
2018-02-05 15:34:04,550 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.2 in stage 0.0 (TID 26, 192.168.11.25, executor 0, partition 8, ANY, 4889 bytes)
2018-02-05 15:34:04,551 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 10.1 in stage 0.0 (TID 21) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 21]
2018-02-05 15:34:04,552 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 9.2 in stage 0.0 (TID 22) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 22]
2018-02-05 15:34:04,553 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.3 in stage 0.0 (TID 27, 192.168.11.25, executor 0, partition 9, ANY, 4889 bytes)
2018-02-05 15:34:04,556 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 5.3 in stage 0.0 (TID 23) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 23]
2018-02-05 15:34:04,557 ERROR[org.apache.spark.scheduler.TaskSetManager:70] - Task 5 in stage 0.0 failed 4 times; aborting job
2018-02-05 15:34:04,563 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 4.3 in stage 0.0 (TID 25) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 24]
2018-02-05 15:34:04,564 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Cancelling stage 0
2018-02-05 15:34:04,567 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Stage 0 was cancelled
2018-02-05 15:34:04,569 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (collect at SparkWordCount.java:44) failed in 0.496 s due to Job aborted due to stage failure: Task 5 in stage 0.0 failed 4 times, most recent failure: Lost task 5.3 in stage 0.0 (TID 23, 192.168.11.25, executor 0): java.lang.ClassNotFoundException: com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:67)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:80)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
2018-02-05 15:34:04,569 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 8.2 in stage 0.0 (TID 26) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 25]
2018-02-05 15:34:04,570 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 15:34:04,571 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 2.3 in stage 0.0 (TID 24) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 26]
2018-02-05 15:34:04,571 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 15:34:04,572 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 9.3 in stage 0.0 (TID 27) on 192.168.11.25, executor 0: java.lang.ClassNotFoundException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1) [duplicate 27]
2018-02-05 15:34:04,573 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 15:34:04,578 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 failed: collect at SparkWordCount.java:44, took 0.644494 s
2018-02-05 15:34:04,603 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-05 15:34:04,611 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@15eebbff{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 15:34:04,614 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-05 15:34:04,617 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Shutting down all executors
2018-02-05 15:34:04,618 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Asking each executor to shut down
2018-02-05 15:34:04,633 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-05 15:34:04,680 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-05 15:34:04,680 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-05 15:34:04,685 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-05 15:34:04,688 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-05 15:34:04,693 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-05 15:34:04,693 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-05 15:34:04,695 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-fa5254ec-4f2d-41f6-ac94-901fbc7cea8d
2018-02-05 15:34:19,246 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 15:34:20,240 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 15:34:20,280 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 15:34:20,281 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 15:34:20,281 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 15:34:20,282 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 15:34:20,283 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 15:34:20,785 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 52700.
2018-02-05 15:34:20,805 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 15:34:20,858 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 15:34:20,861 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 15:34:20,862 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 15:34:20,872 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-b13548db-1288-4ff5-971a-11df48633d52
2018-02-05 15:34:20,896 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 15:34:20,943 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 15:34:21,026 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2795ms
2018-02-05 15:34:21,098 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 15:34:21,117 INFO[org.spark_project.jetty.server.Server:403] - Started @2886ms
2018-02-05 15:34:21,136 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@15eebbff{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 15:34:21,136 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 15:34:21,163 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5778826f{/jobs,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,164 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7b64240d{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,164 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@47dbb1e2{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,165 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55e8ec2f{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,166 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d24ffa1{/stages,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,167 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@773f7880{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,168 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,170 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5b080f3a{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,171 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6b54655f{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,171 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7d3430a7{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,172 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2756c0a7{/storage,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,172 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@69637b10{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,174 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@165b2f7f{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,174 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@277f7dd3{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,175 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2364305a{/environment,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,176 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bc715b8{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,180 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/executors,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,181 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,182 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,183 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,190 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/static,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,191 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,192 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/api,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,193 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,193 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,196 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 15:34:21,303 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Connecting to master spark://192.168.11.25:7077...
2018-02-05 15:34:21,357 INFO[org.apache.spark.network.client.TransportClientFactory:254] - Successfully created connection to /192.168.11.25:7077 after 35 ms (0 ms spent in bootstraps)
2018-02-05 15:34:21,447 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Connected to Spark cluster with app ID app-20180205153512-0003
2018-02-05 15:34:21,449 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor added: app-20180205153512-0003/0 on worker-20180205151248-192.168.11.25-42981 (192.168.11.25:42981) with 4 cores
2018-02-05 15:34:21,453 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Granted executor ID app-20180205153512-0003/0 on hostPort 192.168.11.25:42981 with 4 cores, 1024.0 MB RAM
2018-02-05 15:34:21,457 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205153512-0003/0 is now RUNNING
2018-02-05 15:34:21,463 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52724.
2018-02-05 15:34:21,463 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:52724
2018-02-05 15:34:21,465 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 15:34:21,467 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 52724, None)
2018-02-05 15:34:21,471 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:52724 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 52724, None)
2018-02-05 15:34:21,474 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 52724, None)
2018-02-05 15:34:21,475 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 52724, None)
2018-02-05 15:34:21,655 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@32c0915e{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 15:34:21,669 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2018-02-05 15:34:22,646 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 316.2 KB, free 631.5 MB)
2018-02-05 15:34:22,721 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 631.5 MB)
2018-02-05 15:34:22,726 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:52724 (size: 27.2 KB, free: 631.8 MB)
2018-02-05 15:34:22,734 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from textFile at SparkWordCount.java:27
2018-02-05 15:34:22,780 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.25:47086) with ID 0
2018-02-05 15:34:22,815 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.25:46126 with 366.3 MB RAM, BlockManagerId(0, 192.168.11.25, 46126, None)
2018-02-05 15:38:59,198 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 15:38:59,978 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 15:39:00,037 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 15:39:00,038 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 15:39:00,039 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 15:39:00,040 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 15:39:00,041 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 15:39:00,534 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 53222.
2018-02-05 15:39:00,554 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 15:39:00,605 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 15:39:00,608 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 15:39:00,609 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 15:39:00,619 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-7b9b3082-79ba-40bd-9d48-3725c1b2fd76
2018-02-05 15:39:00,647 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 15:39:00,696 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 15:39:00,783 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2748ms
2018-02-05 15:39:00,853 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 15:39:00,874 INFO[org.spark_project.jetty.server.Server:403] - Started @2840ms
2018-02-05 15:39:00,898 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@553f1d75{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 15:39:00,898 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 15:39:00,922 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@458342d3{/jobs,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,922 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4f2613d1{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,923 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,924 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,925 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@83298d7{/stages,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,925 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5b080f3a{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,926 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6b54655f{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,927 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2756c0a7{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,928 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@69637b10{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,929 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@165b2f7f{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,929 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@277f7dd3{/storage,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,930 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2364305a{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,930 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bc715b8{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,931 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,935 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/environment,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,936 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,937 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/executors,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,938 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,939 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,940 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,947 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51850751{/static,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,948 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,950 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/api,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,951 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,952 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@427b5f92{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 15:39:00,953 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 15:39:01,064 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Connecting to master spark://192.168.11.25:7077...
2018-02-05 15:39:01,135 INFO[org.apache.spark.network.client.TransportClientFactory:254] - Successfully created connection to /192.168.11.25:7077 after 55 ms (0 ms spent in bootstraps)
2018-02-05 15:39:01,245 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Connected to Spark cluster with app ID app-20180205153952-0004
2018-02-05 15:39:01,248 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor added: app-20180205153952-0004/0 on worker-20180205151248-192.168.11.25-42981 (192.168.11.25:42981) with 4 cores
2018-02-05 15:39:01,250 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Granted executor ID app-20180205153952-0004/0 on hostPort 192.168.11.25:42981 with 4 cores, 1024.0 MB RAM
2018-02-05 15:39:01,263 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205153952-0004/0 is now RUNNING
2018-02-05 15:39:01,280 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53246.
2018-02-05 15:39:01,282 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:53246
2018-02-05 15:39:01,283 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 15:39:01,287 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 53246, None)
2018-02-05 15:39:01,299 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:53246 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 53246, None)
2018-02-05 15:39:01,303 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 53246, None)
2018-02-05 15:39:01,304 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 53246, None)
2018-02-05 15:39:01,555 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c282004{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 15:39:01,590 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2018-02-05 15:39:01,599 INFO[org.apache.spark.SparkContext:54] - Added JAR hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar at hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1517816341598
2018-02-05 15:39:02,577 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.25:51095) with ID 0
2018-02-05 15:39:02,612 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.25:45842 with 366.3 MB RAM, BlockManagerId(0, 192.168.11.25, 45842, None)
2018-02-05 15:39:02,678 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 316.2 KB, free 631.5 MB)
2018-02-05 15:39:02,754 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 631.5 MB)
2018-02-05 15:39:02,760 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:53246 (size: 27.2 KB, free: 631.8 MB)
2018-02-05 15:39:02,771 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from textFile at SparkWordCount.java:27
2018-02-05 15:39:10,424 INFO[org.apache.hadoop.mapred.FileInputFormat:256] - Total input files to process : 212
2018-02-05 15:39:10,475 INFO[org.apache.spark.SparkContext:54] - Starting job: collect at SparkWordCount.java:44
2018-02-05 15:39:10,489 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (collect at SparkWordCount.java:44) with 212 output partitions
2018-02-05 15:39:10,489 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (collect at SparkWordCount.java:44)
2018-02-05 15:39:10,490 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-05 15:39:10,492 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-05 15:39:10,499 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:29), which has no missing parents
2018-02-05 15:39:10,549 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 3.7 KB, free 631.5 MB)
2018-02-05 15:39:10,554 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 631.5 MB)
2018-02-05 15:39:10,555 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:53246 (size: 2.2 KB, free: 631.8 MB)
2018-02-05 15:39:10,556 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-05 15:39:10,569 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 212 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:29) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
2018-02-05 15:39:10,570 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 212 tasks
2018-02-05 15:39:10,605 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 15:39:10,607 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, 192.168.11.25, executor 0, partition 1, ANY, 4889 bytes)
2018-02-05 15:39:10,608 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 2.0 in stage 0.0 (TID 2, 192.168.11.25, executor 0, partition 2, ANY, 4889 bytes)
2018-02-05 15:39:10,609 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 3.0 in stage 0.0 (TID 3, 192.168.11.25, executor 0, partition 3, ANY, 4889 bytes)
2018-02-05 15:39:11,655 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.25:45842 (size: 2.2 KB, free: 366.3 MB)
2018-02-05 15:39:11,761 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.25:45842 (size: 27.2 KB, free: 366.3 MB)
2018-02-05 15:39:12,034 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 4.0 in stage 0.0 (TID 4, 192.168.11.25, executor 0, partition 4, ANY, 4889 bytes)
2018-02-05 15:39:12,036 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 5.0 in stage 0.0 (TID 5, 192.168.11.25, executor 0, partition 5, ANY, 4889 bytes)
2018-02-05 15:39:12,065 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 1472 ms on 192.168.11.25 (executor 0) (1/212)
2018-02-05 15:39:12,066 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 3.0 in stage 0.0 (TID 3) in 1458 ms on 192.168.11.25 (executor 0) (2/212)
2018-02-05 15:39:12,089 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 6.0 in stage 0.0 (TID 6, 192.168.11.25, executor 0, partition 6, ANY, 4889 bytes)
2018-02-05 15:39:12,093 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 4.0 in stage 0.0 (TID 4) in 60 ms on 192.168.11.25 (executor 0) (3/212)
2018-02-05 15:39:12,837 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_5 in memory on 192.168.11.25:45842 (size: 1558.5 KB, free: 364.7 MB)
2018-02-05 15:39:12,851 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 7.0 in stage 0.0 (TID 7, 192.168.11.25, executor 0, partition 7, ANY, 4889 bytes)
2018-02-05 15:39:12,868 INFO[org.apache.spark.network.client.TransportClientFactory:254] - Successfully created connection to /192.168.11.25:45842 after 2 ms (0 ms spent in bootstraps)
2018-02-05 15:39:12,883 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_1 in memory on 192.168.11.25:45842 (size: 2.1 MB, free: 362.6 MB)
2018-02-05 15:39:12,888 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 8.0 in stage 0.0 (TID 8, 192.168.11.25, executor 0, partition 8, ANY, 4889 bytes)
2018-02-05 15:39:13,194 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 5.0 in stage 0.0 (TID 5) in 1158 ms on 192.168.11.25 (executor 0) (4/212)
2018-02-05 15:39:13,588 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_5 on 192.168.11.25:45842 in memory (size: 1558.5 KB, free: 364.1 MB)
2018-02-05 15:39:13,601 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_8 in memory on 192.168.11.25:45842 (size: 2.1 MB, free: 362.0 MB)
2018-02-05 15:39:13,604 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_7 in memory on 192.168.11.25:45842 (size: 3.1 MB, free: 358.9 MB)
2018-02-05 15:39:13,611 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 9.0 in stage 0.0 (TID 9, 192.168.11.25, executor 0, partition 9, ANY, 4889 bytes)
2018-02-05 15:39:13,616 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 10.0 in stage 0.0 (TID 10, 192.168.11.25, executor 0, partition 10, ANY, 4889 bytes)
2018-02-05 15:39:13,967 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 11.0 in stage 0.0 (TID 11, 192.168.11.25, executor 0, partition 11, ANY, 4889 bytes)
2018-02-05 15:39:13,967 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_6 in memory on 192.168.11.25:45842 (size: 6.1 MB, free: 352.8 MB)
2018-02-05 15:39:13,970 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_9 in memory on 192.168.11.25:45842 (size: 2.6 MB, free: 350.2 MB)
2018-02-05 15:39:13,980 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 10.0 in stage 0.0 (TID 10) in 364 ms on 192.168.11.25 (executor 0) (5/212)
2018-02-05 15:39:13,983 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 12.0 in stage 0.0 (TID 12, 192.168.11.25, executor 0, partition 12, ANY, 4889 bytes)
2018-02-05 15:39:13,984 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 13.0 in stage 0.0 (TID 13, 192.168.11.25, executor 0, partition 13, ANY, 4889 bytes)
2018-02-05 15:39:14,004 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 3396 ms on 192.168.11.25 (executor 0) (6/212)
2018-02-05 15:39:14,018 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_1 on 192.168.11.25:45842 in memory (size: 2.1 MB, free: 352.3 MB)
2018-02-05 15:39:14,157 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 8.0 in stage 0.0 (TID 8) in 1269 ms on 192.168.11.25 (executor 0) (7/212)
2018-02-05 15:39:14,258 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 14.0 in stage 0.0 (TID 14, 192.168.11.25, executor 0, partition 14, ANY, 4889 bytes)
2018-02-05 15:39:14,259 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_2 in memory on 192.168.11.25:45842 (size: 11.8 MB, free: 340.5 MB)
2018-02-05 15:39:14,259 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_13 in memory on 192.168.11.25:45842 (size: 1350.9 KB, free: 339.1 MB)
2018-02-05 15:39:14,262 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_8 on 192.168.11.25:45842 in memory (size: 2.1 MB, free: 341.2 MB)
2018-02-05 15:39:14,263 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_12 in memory on 192.168.11.25:45842 (size: 3.4 MB, free: 337.9 MB)
2018-02-05 15:39:14,273 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 15.0 in stage 0.0 (TID 15, 192.168.11.25, executor 0, partition 15, ANY, 4889 bytes)
2018-02-05 15:39:14,274 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 11.0 in stage 0.0 (TID 11) in 308 ms on 192.168.11.25 (executor 0) (8/212)
2018-02-05 15:39:14,280 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 16.0 in stage 0.0 (TID 16, 192.168.11.25, executor 0, partition 16, ANY, 4889 bytes)
2018-02-05 15:39:14,281 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 17.0 in stage 0.0 (TID 17, 192.168.11.25, executor 0, partition 17, ANY, 4889 bytes)
2018-02-05 15:39:14,356 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_14 in memory on 192.168.11.25:45842 (size: 1238.2 KB, free: 336.7 MB)
2018-02-05 15:39:14,490 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 7.0 in stage 0.0 (TID 7) in 1640 ms on 192.168.11.25 (executor 0) (9/212)
2018-02-05 15:39:14,532 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 18.0 in stage 0.0 (TID 18, 192.168.11.25, executor 0, partition 18, ANY, 4889 bytes)
2018-02-05 15:39:14,541 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_7 on 192.168.11.25:45842 in memory (size: 3.1 MB, free: 339.8 MB)
2018-02-05 15:39:14,868 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 19.0 in stage 0.0 (TID 19, 192.168.11.25, executor 0, partition 19, ANY, 4889 bytes)
2018-02-05 15:39:14,868 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_16 in memory on 192.168.11.25:45842 (size: 2.1 MB, free: 337.7 MB)
2018-02-05 15:39:14,869 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_15 in memory on 192.168.11.25:45842 (size: 3.6 MB, free: 334.1 MB)
2018-02-05 15:39:14,870 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_17 in memory on 192.168.11.25:45842 (size: 4.3 MB, free: 329.7 MB)
2018-02-05 15:39:14,880 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 20.0 in stage 0.0 (TID 20, 192.168.11.25, executor 0, partition 20, ANY, 4889 bytes)
2018-02-05 15:39:14,881 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 21.0 in stage 0.0 (TID 21, 192.168.11.25, executor 0, partition 21, ANY, 4889 bytes)
2018-02-05 15:39:14,882 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 22.0 in stage 0.0 (TID 22, 192.168.11.25, executor 0, partition 22, ANY, 4889 bytes)
2018-02-05 15:39:14,946 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 23.0 in stage 0.0 (TID 23, 192.168.11.25, executor 0, partition 23, ANY, 4889 bytes)
2018-02-05 15:39:15,020 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 24.0 in stage 0.0 (TID 24, 192.168.11.25, executor 0, partition 24, ANY, 4889 bytes)
2018-02-05 15:39:15,146 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_21 in memory on 192.168.11.25:45842 (size: 2.2 MB, free: 327.5 MB)
2018-02-05 15:39:15,157 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 25.0 in stage 0.0 (TID 25, 192.168.11.25, executor 0, partition 25, ANY, 4889 bytes)
2018-02-05 15:39:15,180 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_24 in memory on 192.168.11.25:45842 (size: 1514.4 KB, free: 326.0 MB)
2018-02-05 15:39:15,253 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 26.0 in stage 0.0 (TID 26, 192.168.11.25, executor 0, partition 26, ANY, 4889 bytes)
2018-02-05 15:39:15,377 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 27.0 in stage 0.0 (TID 27, 192.168.11.25, executor 0, partition 27, ANY, 4889 bytes)
2018-02-05 15:39:15,377 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_22 in memory on 192.168.11.25:45842 (size: 4.9 MB, free: 321.1 MB)
2018-02-05 15:39:15,377 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 6.0 in stage 0.0 (TID 6) in 3288 ms on 192.168.11.25 (executor 0) (10/212)
2018-02-05 15:39:15,389 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_6 on 192.168.11.25:45842 in memory (size: 6.1 MB, free: 327.2 MB)
2018-02-05 15:39:15,395 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 28.0 in stage 0.0 (TID 28, 192.168.11.25, executor 0, partition 28, ANY, 4889 bytes)
2018-02-05 15:39:15,533 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 9.0 in stage 0.0 (TID 9) in 1923 ms on 192.168.11.25 (executor 0) (11/212)
2018-02-05 15:39:15,748 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_9 on 192.168.11.25:45842 in memory (size: 2.6 MB, free: 329.9 MB)
2018-02-05 15:39:15,817 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_23 in memory on 192.168.11.25:45842 (size: 5.4 MB, free: 324.4 MB)
2018-02-05 15:39:15,829 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 29.0 in stage 0.0 (TID 29, 192.168.11.25, executor 0, partition 29, ANY, 4889 bytes)
2018-02-05 15:39:16,843 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_28 in memory on 192.168.11.25:45842 (size: 6.0 MB, free: 318.4 MB)
2018-02-05 15:39:16,854 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 30.0 in stage 0.0 (TID 30, 192.168.11.25, executor 0, partition 30, ANY, 4889 bytes)
2018-02-05 15:39:17,144 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_27 in memory on 192.168.11.25:45842 (size: 9.1 MB, free: 309.3 MB)
2018-02-05 15:39:17,156 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 31.0 in stage 0.0 (TID 31, 192.168.11.25, executor 0, partition 31, ANY, 4889 bytes)
2018-02-05 15:39:18,810 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 2.0 in stage 0.0 (TID 2) in 8202 ms on 192.168.11.25 (executor 0) (12/212)
2018-02-05 15:39:18,828 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 32.0 in stage 0.0 (TID 32, 192.168.11.25, executor 0, partition 32, ANY, 4889 bytes)
2018-02-05 15:39:18,828 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_29 in memory on 192.168.11.25:45842 (size: 9.0 MB, free: 300.3 MB)
2018-02-05 15:39:18,829 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_30 in memory on 192.168.11.25:45842 (size: 5.6 MB, free: 294.8 MB)
2018-02-05 15:39:18,830 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_26 in memory on 192.168.11.25:45842 (size: 12.6 MB, free: 282.2 MB)
2018-02-05 15:39:18,831 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_2 on 192.168.11.25:45842 in memory (size: 11.8 MB, free: 294.0 MB)
2018-02-05 15:39:18,831 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 18.0 in stage 0.0 (TID 18) in 4300 ms on 192.168.11.25 (executor 0) (13/212)
2018-02-05 15:39:18,842 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 33.0 in stage 0.0 (TID 33, 192.168.11.25, executor 0, partition 33, ANY, 4889 bytes)
2018-02-05 15:39:18,843 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 34.0 in stage 0.0 (TID 34, 192.168.11.25, executor 0, partition 34, ANY, 4889 bytes)
2018-02-05 15:39:18,844 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 35.0 in stage 0.0 (TID 35, 192.168.11.25, executor 0, partition 35, ANY, 4889 bytes)
2018-02-05 15:39:18,901 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 13.0 in stage 0.0 (TID 13) in 4917 ms on 192.168.11.25 (executor 0) (14/212)
2018-02-05 15:39:18,989 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 36.0 in stage 0.0 (TID 36, 192.168.11.25, executor 0, partition 36, ANY, 4889 bytes)
2018-02-05 15:39:18,989 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_13 on 192.168.11.25:45842 in memory (size: 1350.9 KB, free: 295.3 MB)
2018-02-05 15:39:19,306 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 12.0 in stage 0.0 (TID 12) in 5324 ms on 192.168.11.25 (executor 0) (15/212)
2018-02-05 15:39:19,362 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 14.0 in stage 0.0 (TID 14) in 5103 ms on 192.168.11.25 (executor 0) (16/212)
2018-02-05 15:39:19,364 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 20.0 in stage 0.0 (TID 20) in 4485 ms on 192.168.11.25 (executor 0) (17/212)
2018-02-05 15:39:19,371 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 19.0 in stage 0.0 (TID 19) in 4504 ms on 192.168.11.25 (executor 0) (18/212)
2018-02-05 15:39:19,831 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_14 on 192.168.11.25:45842 in memory (size: 1238.2 KB, free: 296.6 MB)
2018-02-05 15:39:19,832 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_12 on 192.168.11.25:45842 in memory (size: 3.4 MB, free: 299.9 MB)
2018-02-05 15:39:20,032 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_34 in memory on 192.168.11.25:45842 (size: 5.9 MB, free: 294.0 MB)
2018-02-05 15:39:20,043 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 37.0 in stage 0.0 (TID 37, 192.168.11.25, executor 0, partition 37, ANY, 4889 bytes)
2018-02-05 15:39:20,062 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 16.0 in stage 0.0 (TID 16) in 5783 ms on 192.168.11.25 (executor 0) (19/212)
2018-02-05 15:39:20,076 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_16 on 192.168.11.25:45842 in memory (size: 2.1 MB, free: 296.1 MB)
2018-02-05 15:39:20,126 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_35 in memory on 192.168.11.25:45842 (size: 6.2 MB, free: 289.9 MB)
2018-02-05 15:39:20,131 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_33 in memory on 192.168.11.25:45842 (size: 6.7 MB, free: 283.2 MB)
2018-02-05 15:39:20,137 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 38.0 in stage 0.0 (TID 38, 192.168.11.25, executor 0, partition 38, ANY, 4889 bytes)
2018-02-05 15:39:20,143 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 39.0 in stage 0.0 (TID 39, 192.168.11.25, executor 0, partition 39, ANY, 4889 bytes)
2018-02-05 15:39:20,187 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_36 in memory on 192.168.11.25:45842 (size: 6.9 MB, free: 276.3 MB)
2018-02-05 15:39:20,209 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 40.0 in stage 0.0 (TID 40, 192.168.11.25, executor 0, partition 40, ANY, 4889 bytes)
2018-02-05 15:39:20,349 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 41.0 in stage 0.0 (TID 41, 192.168.11.25, executor 0, partition 41, ANY, 4889 bytes)
2018-02-05 15:39:20,533 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 42.0 in stage 0.0 (TID 42, 192.168.11.25, executor 0, partition 42, ANY, 4889 bytes)
2018-02-05 15:39:20,611 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 15.0 in stage 0.0 (TID 15) in 6338 ms on 192.168.11.25 (executor 0) (20/212)
2018-02-05 15:39:20,629 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 25.0 in stage 0.0 (TID 25) in 5472 ms on 192.168.11.25 (executor 0) (21/212)
2018-02-05 15:39:20,864 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_15 on 192.168.11.25:45842 in memory (size: 3.6 MB, free: 279.9 MB)
2018-02-05 15:39:20,923 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_37 in memory on 192.168.11.25:45842 (size: 5.1 MB, free: 274.8 MB)
2018-02-05 15:39:20,957 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 43.0 in stage 0.0 (TID 43, 192.168.11.25, executor 0, partition 43, ANY, 4889 bytes)
2018-02-05 15:39:21,237 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_40 in memory on 192.168.11.25:45842 (size: 6.1 MB, free: 268.7 MB)
2018-02-05 15:39:21,244 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 17.0 in stage 0.0 (TID 17) in 6963 ms on 192.168.11.25 (executor 0) (22/212)
2018-02-05 15:39:21,249 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 44.0 in stage 0.0 (TID 44, 192.168.11.25, executor 0, partition 44, ANY, 4889 bytes)
2018-02-05 15:39:21,254 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_17 on 192.168.11.25:45842 in memory (size: 4.3 MB, free: 273.0 MB)
2018-02-05 15:39:21,272 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_41 in memory on 192.168.11.25:45842 (size: 5.6 MB, free: 267.5 MB)
2018-02-05 15:39:21,278 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 45.0 in stage 0.0 (TID 45, 192.168.11.25, executor 0, partition 45, ANY, 4889 bytes)
2018-02-05 15:39:21,394 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 21.0 in stage 0.0 (TID 21) in 6513 ms on 192.168.11.25 (executor 0) (23/212)
2018-02-05 15:39:21,408 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_21 on 192.168.11.25:45842 in memory (size: 2.2 MB, free: 269.7 MB)
2018-02-05 15:39:21,496 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_42 in memory on 192.168.11.25:45842 (size: 6.4 MB, free: 263.3 MB)
2018-02-05 15:39:21,502 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_45 in memory on 192.168.11.25:45842 (size: 2.0 MB, free: 261.3 MB)
2018-02-05 15:39:21,507 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 46.0 in stage 0.0 (TID 46, 192.168.11.25, executor 0, partition 46, ANY, 4889 bytes)
2018-02-05 15:39:21,514 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 47.0 in stage 0.0 (TID 47, 192.168.11.25, executor 0, partition 47, ANY, 4889 bytes)
2018-02-05 15:39:21,514 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 24.0 in stage 0.0 (TID 24) in 6494 ms on 192.168.11.25 (executor 0) (24/212)
2018-02-05 15:39:21,524 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_24 on 192.168.11.25:45842 in memory (size: 1514.4 KB, free: 262.8 MB)
2018-02-05 15:39:22,128 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_46 in memory on 192.168.11.25:45842 (size: 1959.9 KB, free: 260.8 MB)
2018-02-05 15:39:22,129 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_43 in memory on 192.168.11.25:45842 (size: 6.9 MB, free: 254.0 MB)
2018-02-05 15:39:22,139 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 48.0 in stage 0.0 (TID 48, 192.168.11.25, executor 0, partition 48, ANY, 4889 bytes)
2018-02-05 15:39:22,145 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 49.0 in stage 0.0 (TID 49, 192.168.11.25, executor 0, partition 49, ANY, 4889 bytes)
2018-02-05 15:39:23,416 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_44 in memory on 192.168.11.25:45842 (size: 9.0 MB, free: 245.0 MB)
2018-02-05 15:39:23,490 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 22.0 in stage 0.0 (TID 22) in 8607 ms on 192.168.11.25 (executor 0) (25/212)
2018-02-05 15:39:23,510 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 50.0 in stage 0.0 (TID 50, 192.168.11.25, executor 0, partition 50, ANY, 4889 bytes)
2018-02-05 15:39:23,511 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 31.0 in stage 0.0 (TID 31) in 6356 ms on 192.168.11.25 (executor 0) (26/212)
2018-02-05 15:39:23,515 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_22 on 192.168.11.25:45842 in memory (size: 4.9 MB, free: 249.9 MB)
2018-02-05 15:39:23,531 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_47 in memory on 192.168.11.25:45842 (size: 8.1 MB, free: 241.8 MB)
2018-02-05 15:39:23,543 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 51.0 in stage 0.0 (TID 51, 192.168.11.25, executor 0, partition 51, ANY, 4889 bytes)
2018-02-05 15:39:23,638 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_49 in memory on 192.168.11.25:45842 (size: 7.7 MB, free: 234.1 MB)
2018-02-05 15:39:23,658 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 52.0 in stage 0.0 (TID 52, 192.168.11.25, executor 0, partition 52, ANY, 4889 bytes)
2018-02-05 15:39:23,785 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_48 in memory on 192.168.11.25:45842 (size: 8.8 MB, free: 225.3 MB)
2018-02-05 15:39:23,950 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 23.0 in stage 0.0 (TID 23) in 9005 ms on 192.168.11.25 (executor 0) (27/212)
2018-02-05 15:39:24,275 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_23 on 192.168.11.25:45842 in memory (size: 5.4 MB, free: 230.8 MB)
2018-02-05 15:39:24,275 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 53.0 in stage 0.0 (TID 53, 192.168.11.25, executor 0, partition 53, ANY, 4889 bytes)
2018-02-05 15:39:24,440 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_52 in memory on 192.168.11.25:45842 (size: 3.6 MB, free: 227.2 MB)
2018-02-05 15:39:24,452 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 54.0 in stage 0.0 (TID 54, 192.168.11.25, executor 0, partition 54, ANY, 4889 bytes)
2018-02-05 15:39:24,480 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_53 in memory on 192.168.11.25:45842 (size: 2.9 MB, free: 224.3 MB)
2018-02-05 15:39:24,487 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 55.0 in stage 0.0 (TID 55, 192.168.11.25, executor 0, partition 55, ANY, 4889 bytes)
2018-02-05 15:39:24,676 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_51 in memory on 192.168.11.25:45842 (size: 6.9 MB, free: 217.3 MB)
2018-02-05 15:39:24,689 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 56.0 in stage 0.0 (TID 56, 192.168.11.25, executor 0, partition 56, ANY, 4889 bytes)
2018-02-05 15:39:24,760 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_50 in memory on 192.168.11.25:45842 (size: 7.9 MB, free: 209.5 MB)
2018-02-05 15:39:24,770 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 57.0 in stage 0.0 (TID 57, 192.168.11.25, executor 0, partition 57, ANY, 4889 bytes)
2018-02-05 15:39:24,953 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 28.0 in stage 0.0 (TID 28) in 9558 ms on 192.168.11.25 (executor 0) (28/212)
2018-02-05 15:39:25,277 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_28 on 192.168.11.25:45842 in memory (size: 6.0 MB, free: 215.5 MB)
2018-02-05 15:39:25,708 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_54 in memory on 192.168.11.25:45842 (size: 7.4 MB, free: 208.1 MB)
2018-02-05 15:39:25,731 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 58.0 in stage 0.0 (TID 58, 192.168.11.25, executor 0, partition 58, ANY, 4889 bytes)
2018-02-05 15:39:25,746 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_55 in memory on 192.168.11.25:45842 (size: 7.3 MB, free: 200.8 MB)
2018-02-05 15:39:25,758 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 59.0 in stage 0.0 (TID 59, 192.168.11.25, executor 0, partition 59, ANY, 4889 bytes)
2018-02-05 15:39:26,037 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 27.0 in stage 0.0 (TID 27) in 10661 ms on 192.168.11.25 (executor 0) (29/212)
2018-02-05 15:39:26,049 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 32.0 in stage 0.0 (TID 32) in 7221 ms on 192.168.11.25 (executor 0) (30/212)
2018-02-05 15:39:26,448 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_27 on 192.168.11.25:45842 in memory (size: 9.1 MB, free: 209.9 MB)
2018-02-05 15:39:26,556 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_59 in memory on 192.168.11.25:45842 (size: 2.6 MB, free: 207.3 MB)
2018-02-05 15:39:26,568 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 60.0 in stage 0.0 (TID 60, 192.168.11.25, executor 0, partition 60, ANY, 4889 bytes)
2018-02-05 15:39:26,759 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_56 in memory on 192.168.11.25:45842 (size: 10.2 MB, free: 197.1 MB)
2018-02-05 15:39:26,771 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 61.0 in stage 0.0 (TID 61, 192.168.11.25, executor 0, partition 61, ANY, 4889 bytes)
2018-02-05 15:39:26,880 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_60 in memory on 192.168.11.25:45842 (size: 2.8 MB, free: 194.2 MB)
2018-02-05 15:39:26,893 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 62.0 in stage 0.0 (TID 62, 192.168.11.25, executor 0, partition 62, ANY, 4889 bytes)
2018-02-05 15:39:27,416 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_57 in memory on 192.168.11.25:45842 (size: 11.5 MB, free: 182.7 MB)
2018-02-05 15:39:27,431 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 63.0 in stage 0.0 (TID 63, 192.168.11.25, executor 0, partition 63, ANY, 4889 bytes)
2018-02-05 15:39:27,794 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 29.0 in stage 0.0 (TID 29) in 11965 ms on 192.168.11.25 (executor 0) (31/212)
2018-02-05 15:39:28,002 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_29 on 192.168.11.25:45842 in memory (size: 9.0 MB, free: 191.7 MB)
2018-02-05 15:39:29,223 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_63 in memory on 192.168.11.25:45842 (size: 5.5 MB, free: 186.2 MB)
2018-02-05 15:39:29,236 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 64.0 in stage 0.0 (TID 64, 192.168.11.25, executor 0, partition 64, ANY, 4889 bytes)
2018-02-05 15:39:31,638 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_64 in memory on 192.168.11.25:45842 (size: 3.7 MB, free: 182.6 MB)
2018-02-05 15:39:31,639 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_58 in memory on 192.168.11.25:45842 (size: 15.1 MB, free: 167.4 MB)
2018-02-05 15:39:31,640 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_62 in memory on 192.168.11.25:45842 (size: 13.2 MB, free: 154.2 MB)
2018-02-05 15:39:31,641 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_61 in memory on 192.168.11.25:45842 (size: 16.0 MB, free: 138.3 MB)
2018-02-05 15:39:31,646 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 65.0 in stage 0.0 (TID 65, 192.168.11.25, executor 0, partition 65, ANY, 4889 bytes)
2018-02-05 15:39:31,648 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 66.0 in stage 0.0 (TID 66, 192.168.11.25, executor 0, partition 66, ANY, 4889 bytes)
2018-02-05 15:39:31,649 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 67.0 in stage 0.0 (TID 67, 192.168.11.25, executor 0, partition 67, ANY, 4889 bytes)
2018-02-05 15:39:31,650 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 68.0 in stage 0.0 (TID 68, 192.168.11.25, executor 0, partition 68, ANY, 4889 bytes)
2018-02-05 15:39:31,790 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 26.0 in stage 0.0 (TID 26) in 16538 ms on 192.168.11.25 (executor 0) (32/212)
2018-02-05 15:39:31,830 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_26 on 192.168.11.25:45842 in memory (size: 12.6 MB, free: 150.9 MB)
2018-02-05 15:39:31,938 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_66 in memory on 192.168.11.25:45842 (size: 1810.2 KB, free: 149.1 MB)
2018-02-05 15:39:31,950 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_67 in memory on 192.168.11.25:45842 (size: 1735.8 KB, free: 147.4 MB)
2018-02-05 15:39:31,952 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 69.0 in stage 0.0 (TID 69, 192.168.11.25, executor 0, partition 69, ANY, 4889 bytes)
2018-02-05 15:39:31,962 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 70.0 in stage 0.0 (TID 70, 192.168.11.25, executor 0, partition 70, ANY, 4889 bytes)
2018-02-05 15:39:32,066 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_65 in memory on 192.168.11.25:45842 (size: 3.6 MB, free: 143.8 MB)
2018-02-05 15:39:32,066 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_68 in memory on 192.168.11.25:45842 (size: 3.2 MB, free: 140.6 MB)
2018-02-05 15:39:32,078 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 71.0 in stage 0.0 (TID 71, 192.168.11.25, executor 0, partition 71, ANY, 4889 bytes)
2018-02-05 15:39:33,346 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 72.0 in stage 0.0 (TID 72, 192.168.11.25, executor 0, partition 72, ANY, 4889 bytes)
2018-02-05 15:39:33,348 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_69 in memory on 192.168.11.25:45842 (size: 2.8 MB, free: 137.8 MB)
2018-02-05 15:39:33,348 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_71 in memory on 192.168.11.25:45842 (size: 4.4 MB, free: 133.4 MB)
2018-02-05 15:39:33,349 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_70 in memory on 192.168.11.25:45842 (size: 7.0 MB, free: 126.5 MB)
2018-02-05 15:39:33,357 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 73.0 in stage 0.0 (TID 73, 192.168.11.25, executor 0, partition 73, ANY, 4889 bytes)
2018-02-05 15:39:33,360 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 74.0 in stage 0.0 (TID 74, 192.168.11.25, executor 0, partition 74, ANY, 4889 bytes)
2018-02-05 15:39:33,364 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 75.0 in stage 0.0 (TID 75, 192.168.11.25, executor 0, partition 75, ANY, 4889 bytes)
2018-02-05 15:39:33,439 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 30.0 in stage 0.0 (TID 30) in 16585 ms on 192.168.11.25 (executor 0) (33/212)
2018-02-05 15:39:33,455 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_30 on 192.168.11.25:45842 in memory (size: 5.6 MB, free: 132.0 MB)
2018-02-05 15:39:33,988 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_73 in memory on 192.168.11.25:45842 (size: 2.2 MB, free: 129.8 MB)
2018-02-05 15:39:33,992 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_74 in memory on 192.168.11.25:45842 (size: 2.5 MB, free: 127.3 MB)
2018-02-05 15:39:33,999 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 76.0 in stage 0.0 (TID 76, 192.168.11.25, executor 0, partition 76, ANY, 4889 bytes)
2018-02-05 15:39:34,005 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 77.0 in stage 0.0 (TID 77, 192.168.11.25, executor 0, partition 77, ANY, 4889 bytes)
2018-02-05 15:39:35,484 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_72 in memory on 192.168.11.25:45842 (size: 5.2 MB, free: 122.1 MB)
2018-02-05 15:39:35,485 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_75 in memory on 192.168.11.25:45842 (size: 5.6 MB, free: 116.5 MB)
2018-02-05 15:39:35,486 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_76 in memory on 192.168.11.25:45842 (size: 4.4 MB, free: 112.1 MB)
2018-02-05 15:39:35,487 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_77 in memory on 192.168.11.25:45842 (size: 4.9 MB, free: 107.2 MB)
2018-02-05 15:39:35,489 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 78.0 in stage 0.0 (TID 78, 192.168.11.25, executor 0, partition 78, ANY, 4889 bytes)
2018-02-05 15:39:35,491 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 79.0 in stage 0.0 (TID 79, 192.168.11.25, executor 0, partition 79, ANY, 4889 bytes)
2018-02-05 15:39:35,493 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 80.0 in stage 0.0 (TID 80, 192.168.11.25, executor 0, partition 80, ANY, 4889 bytes)
2018-02-05 15:39:35,495 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 81.0 in stage 0.0 (TID 81, 192.168.11.25, executor 0, partition 81, ANY, 4889 bytes)
2018-02-05 15:39:36,471 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_81 in memory on 192.168.11.25:45842 (size: 1578.1 KB, free: 105.6 MB)
2018-02-05 15:39:36,472 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_80 in memory on 192.168.11.25:45842 (size: 2.0 MB, free: 103.6 MB)
2018-02-05 15:39:36,473 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_79 in memory on 192.168.11.25:45842 (size: 3.3 MB, free: 100.3 MB)
2018-02-05 15:39:36,474 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_78 in memory on 192.168.11.25:45842 (size: 4.3 MB, free: 95.9 MB)
2018-02-05 15:39:36,483 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 82.0 in stage 0.0 (TID 82, 192.168.11.25, executor 0, partition 82, ANY, 4889 bytes)
2018-02-05 15:39:36,485 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 34.0 in stage 0.0 (TID 34) in 17643 ms on 192.168.11.25 (executor 0) (34/212)
2018-02-05 15:39:36,490 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 83.0 in stage 0.0 (TID 83, 192.168.11.25, executor 0, partition 83, ANY, 4889 bytes)
2018-02-05 15:39:36,492 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 84.0 in stage 0.0 (TID 84, 192.168.11.25, executor 0, partition 84, ANY, 4889 bytes)
2018-02-05 15:39:36,493 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 85.0 in stage 0.0 (TID 85, 192.168.11.25, executor 0, partition 85, ANY, 4889 bytes)
2018-02-05 15:39:36,496 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 39.0 in stage 0.0 (TID 39) in 16354 ms on 192.168.11.25 (executor 0) (35/212)
2018-02-05 15:39:36,504 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_34 on 192.168.11.25:45842 in memory (size: 5.9 MB, free: 101.9 MB)
2018-02-05 15:39:36,517 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 38.0 in stage 0.0 (TID 38) in 16380 ms on 192.168.11.25 (executor 0) (36/212)
2018-02-05 15:39:37,609 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_85 in memory on 192.168.11.25:45842 (size: 4.2 MB, free: 97.7 MB)
2018-02-05 15:39:37,611 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_82 in memory on 192.168.11.25:45842 (size: 4.5 MB, free: 93.2 MB)
2018-02-05 15:39:37,613 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_83 in memory on 192.168.11.25:45842 (size: 4.8 MB, free: 88.5 MB)
2018-02-05 15:39:37,614 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_84 in memory on 192.168.11.25:45842 (size: 4.9 MB, free: 83.6 MB)
2018-02-05 15:39:37,615 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 86.0 in stage 0.0 (TID 86, 192.168.11.25, executor 0, partition 86, ANY, 4889 bytes)
2018-02-05 15:39:37,617 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 87.0 in stage 0.0 (TID 87, 192.168.11.25, executor 0, partition 87, ANY, 4889 bytes)
2018-02-05 15:39:37,619 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 88.0 in stage 0.0 (TID 88, 192.168.11.25, executor 0, partition 88, ANY, 4889 bytes)
2018-02-05 15:39:37,621 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 89.0 in stage 0.0 (TID 89, 192.168.11.25, executor 0, partition 89, ANY, 4889 bytes)
2018-02-05 15:39:38,597 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_88 in memory on 192.168.11.25:45842 (size: 2.1 MB, free: 81.5 MB)
2018-02-05 15:39:38,598 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_87 in memory on 192.168.11.25:45842 (size: 2.7 MB, free: 78.8 MB)
2018-02-05 15:39:38,604 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 90.0 in stage 0.0 (TID 90, 192.168.11.25, executor 0, partition 90, ANY, 4889 bytes)
2018-02-05 15:39:38,605 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 91.0 in stage 0.0 (TID 91, 192.168.11.25, executor 0, partition 91, ANY, 4889 bytes)
2018-02-05 15:39:39,588 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_89 in memory on 192.168.11.25:45842 (size: 6.0 MB, free: 72.8 MB)
2018-02-05 15:39:39,588 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_86 in memory on 192.168.11.25:45842 (size: 6.9 MB, free: 65.8 MB)
2018-02-05 15:39:40,314 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 92.0 in stage 0.0 (TID 92, 192.168.11.25, executor 0, partition 92, ANY, 4889 bytes)
2018-02-05 15:39:40,314 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_91 in memory on 192.168.11.25:45842 (size: 7.9 MB, free: 57.9 MB)
2018-02-05 15:39:40,315 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 93.0 in stage 0.0 (TID 93, 192.168.11.25, executor 0, partition 93, ANY, 4889 bytes)
2018-02-05 15:39:40,325 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 94.0 in stage 0.0 (TID 94, 192.168.11.25, executor 0, partition 94, ANY, 4889 bytes)
2018-02-05 15:39:40,326 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 35.0 in stage 0.0 (TID 35) in 21482 ms on 192.168.11.25 (executor 0) (37/212)
2018-02-05 15:39:40,337 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_35 on 192.168.11.25:45842 in memory (size: 6.2 MB, free: 64.1 MB)
2018-02-05 15:39:43,450 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_90 in memory on 192.168.11.25:45842 (size: 14.0 MB, free: 50.1 MB)
2018-02-05 15:39:44,162 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_94 in memory on 192.168.11.25:45842 (size: 6.4 MB, free: 43.8 MB)
2018-02-05 15:39:44,165 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_93 in memory on 192.168.11.25:45842 (size: 7.7 MB, free: 36.1 MB)
2018-02-05 15:39:44,165 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 95.0 in stage 0.0 (TID 95, 192.168.11.25, executor 0, partition 95, ANY, 4889 bytes)
2018-02-05 15:39:44,950 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 96.0 in stage 0.0 (TID 96, 192.168.11.25, executor 0, partition 96, ANY, 4889 bytes)
2018-02-05 15:39:44,951 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 97.0 in stage 0.0 (TID 97, 192.168.11.25, executor 0, partition 97, ANY, 4889 bytes)
2018-02-05 15:39:44,952 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_95 in memory on 192.168.11.25:45842 (size: 2.5 MB, free: 33.6 MB)
2018-02-05 15:39:44,953 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_92 in memory on 192.168.11.25:45842 (size: 10.4 MB, free: 23.2 MB)
2018-02-05 15:39:47,571 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 98.0 in stage 0.0 (TID 98, 192.168.11.25, executor 0, partition 98, ANY, 4889 bytes)
2018-02-05 15:39:47,571 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_97 in memory on 192.168.11.25:45842 (size: 5.0 MB, free: 18.2 MB)
2018-02-05 15:39:49,488 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_96 in memory on 192.168.11.25:45842 (size: 10.1 MB, free: 8.0 MB)
2018-02-05 15:39:49,487 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 99.0 in stage 0.0 (TID 99, 192.168.11.25, executor 0, partition 99, ANY, 4889 bytes)
2018-02-05 15:39:50,681 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 100.0 in stage 0.0 (TID 100, 192.168.11.25, executor 0, partition 100, ANY, 4889 bytes)
2018-02-05 15:39:50,682 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 101.0 in stage 0.0 (TID 101, 192.168.11.25, executor 0, partition 101, ANY, 4889 bytes)
2018-02-05 15:40:26,909 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205153952-0004/0 is now EXITED (Command exited with code 52)
2018-02-05 15:41:04,045 ERROR[org.apache.spark.network.client.TransportResponseHandler:144] - Still have 1 requests outstanding when connection from /192.168.11.25:45842 is closed
2018-02-05 15:41:05,923 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Executor app-20180205153952-0004/0 removed: Command exited with code 52
2018-02-05 15:41:06,593 INFO[org.apache.spark.network.shuffle.RetryingBlockFetcher:164] - Retrying fetch (1/3) for 1 outstanding blocks after 5000 ms
2018-02-05 15:43:34,645 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 102.0 in stage 0.0 (TID 102, 192.168.11.25, executor 0, partition 102, ANY, 4889 bytes)
2018-02-05 15:43:34,660 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor added: app-20180205153952-0004/1 on worker-20180205151248-192.168.11.25-42981 (192.168.11.25:42981) with 4 cores
2018-02-05 15:43:34,661 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Granted executor ID app-20180205153952-0004/1 on hostPort 192.168.11.25:42981 with 4 cores, 1024.0 MB RAM
2018-02-05 15:43:34,661 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205153952-0004/1 is now RUNNING
2018-02-05 15:43:34,665 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205153952-0004/1 is now EXITED (Command exited with code 1)
2018-02-05 15:43:34,665 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Executor app-20180205153952-0004/1 removed: Command exited with code 1
2018-02-05 15:43:34,665 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor added: app-20180205153952-0004/2 on worker-20180205151248-192.168.11.25-42981 (192.168.11.25:42981) with 4 cores
2018-02-05 15:43:34,666 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Granted executor ID app-20180205153952-0004/2 on hostPort 192.168.11.25:42981 with 4 cores, 1024.0 MB RAM
2018-02-05 15:43:34,666 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205153952-0004/2 is now RUNNING
2018-02-05 15:43:34,667 ERROR[org.apache.spark.scheduler.TaskSchedulerImpl:70] - Lost executor 0 on 192.168.11.25: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,692 WARN[org.apache.spark.HeartbeatReceiver:66] - Removing executor 0 with no recent heartbeats: 152787 ms exceeds timeout 120000 ms
2018-02-05 15:43:34,694 ERROR[org.apache.spark.network.server.TransportRequestHandler:196] - Error sending result RpcResponse{requestId=8311422107239811576, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=47 cap=64]}} to /192.168.11.25:51111; closing connection
java.nio.channels.ClosedChannelException
2018-02-05 15:43:34,695 ERROR[org.apache.spark.network.server.TransportRequestHandler:196] - Error sending result RpcResponse{requestId=7284924465832290182, body=NioManagedBuffer{buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]}} to /192.168.11.25:51095; closing connection
java.nio.channels.ClosedChannelException
2018-02-05 15:43:34,718 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 92.0 in stage 0.0 (TID 92, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,720 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 101.0 in stage 0.0 (TID 101, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,721 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 83.0 in stage 0.0 (TID 83, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,721 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 95.0 in stage 0.0 (TID 95, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,721 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 77.0 in stage 0.0 (TID 77, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,721 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 86.0 in stage 0.0 (TID 86, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,722 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 50.0 in stage 0.0 (TID 50, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,722 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 59.0 in stage 0.0 (TID 59, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,722 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 41.0 in stage 0.0 (TID 41, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,722 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 68.0 in stage 0.0 (TID 68, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,722 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 53.0 in stage 0.0 (TID 53, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,723 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 62.0 in stage 0.0 (TID 62, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,723 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 44.0 in stage 0.0 (TID 44, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,723 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 80.0 in stage 0.0 (TID 80, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,723 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 89.0 in stage 0.0 (TID 89, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,723 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 98.0 in stage 0.0 (TID 98, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,724 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 71.0 in stage 0.0 (TID 71, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,724 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 74.0 in stage 0.0 (TID 74, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,724 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 56.0 in stage 0.0 (TID 56, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,724 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 47.0 in stage 0.0 (TID 47, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,724 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 65.0 in stage 0.0 (TID 65, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,724 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 46.0 in stage 0.0 (TID 46, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,725 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 100.0 in stage 0.0 (TID 100, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,725 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 82.0 in stage 0.0 (TID 82, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,725 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 91.0 in stage 0.0 (TID 91, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,725 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 55.0 in stage 0.0 (TID 55, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,725 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 64.0 in stage 0.0 (TID 64, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,725 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 73.0 in stage 0.0 (TID 73, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,726 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 58.0 in stage 0.0 (TID 58, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,726 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 67.0 in stage 0.0 (TID 67, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,726 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 85.0 in stage 0.0 (TID 85, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,726 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 94.0 in stage 0.0 (TID 94, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,727 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 40.0 in stage 0.0 (TID 40, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,727 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 49.0 in stage 0.0 (TID 49, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,727 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 76.0 in stage 0.0 (TID 76, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,727 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 97.0 in stage 0.0 (TID 97, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,727 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 79.0 in stage 0.0 (TID 79, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,728 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 88.0 in stage 0.0 (TID 88, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,728 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 70.0 in stage 0.0 (TID 70, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,728 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 43.0 in stage 0.0 (TID 43, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,728 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 52.0 in stage 0.0 (TID 52, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,728 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 61.0 in stage 0.0 (TID 61, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,729 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 37.0 in stage 0.0 (TID 37, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,729 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 60.0 in stage 0.0 (TID 60, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,729 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 87.0 in stage 0.0 (TID 87, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,732 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 96.0 in stage 0.0 (TID 96, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,733 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 69.0 in stage 0.0 (TID 69, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,733 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 78.0 in stage 0.0 (TID 78, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,733 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 99.0 in stage 0.0 (TID 99, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,734 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 63.0 in stage 0.0 (TID 63, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,734 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 90.0 in stage 0.0 (TID 90, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,734 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 45.0 in stage 0.0 (TID 45, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,734 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 54.0 in stage 0.0 (TID 54, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,734 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 72.0 in stage 0.0 (TID 72, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,734 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 81.0 in stage 0.0 (TID 81, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,734 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 36.0 in stage 0.0 (TID 36, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,735 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 48.0 in stage 0.0 (TID 48, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,736 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 57.0 in stage 0.0 (TID 57, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,736 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 84.0 in stage 0.0 (TID 84, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,737 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 102.0 in stage 0.0 (TID 102, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,737 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 93.0 in stage 0.0 (TID 93, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,737 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 75.0 in stage 0.0 (TID 75, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,737 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 66.0 in stage 0.0 (TID 66, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,737 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 42.0 in stage 0.0 (TID 42, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,737 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 51.0 in stage 0.0 (TID 51, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,737 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 33.0 in stage 0.0 (TID 33, 192.168.11.25, executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.
2018-02-05 15:43:34,744 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Executor lost: 0 (epoch 0)
2018-02-05 15:43:34,745 ERROR[org.apache.spark.scheduler.TaskSchedulerImpl:70] - Lost an executor 0 (already removed): Executor heartbeat timed out after 152787 ms
2018-02-05 15:43:34,745 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Trying to remove executor 0 from BlockManagerMaster.
2018-02-05 15:43:34,747 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Removal of executor 0 requested
2018-02-05 15:43:34,749 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Asked to remove non-existent executor 0
2018-02-05 15:43:34,762 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Removal of executor 1 requested
2018-02-05 15:43:34,763 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Asked to remove non-existent executor 1
2018-02-05 15:43:34,764 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_59 !
2018-02-05 15:43:34,764 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_58 !
2018-02-05 15:43:34,764 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_62 !
2018-02-05 15:43:34,764 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_61 !
2018-02-05 15:43:34,764 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_64 !
2018-02-05 15:43:34,764 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_63 !
2018-02-05 15:43:34,764 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_66 !
2018-02-05 15:43:34,764 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_65 !
2018-02-05 15:43:34,765 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_68 !
2018-02-05 15:43:34,765 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_67 !
2018-02-05 15:43:34,765 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_60 !
2018-02-05 15:43:34,765 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_48 !
2018-02-05 15:43:34,765 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_47 !
2018-02-05 15:43:34,765 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_49 !
2018-02-05 15:43:34,765 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_51 !
2018-02-05 15:43:34,765 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_95 !
2018-02-05 15:43:34,765 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_50 !
2018-02-05 15:43:34,765 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_94 !
2018-02-05 15:43:34,765 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_53 !
2018-02-05 15:43:34,765 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_97 !
2018-02-05 15:43:34,765 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_52 !
2018-02-05 15:43:34,766 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_96 !
2018-02-05 15:43:34,766 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_55 !
2018-02-05 15:43:34,766 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_54 !
2018-02-05 15:43:34,767 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_57 !
2018-02-05 15:43:34,767 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_56 !
2018-02-05 15:43:34,767 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_91 !
2018-02-05 15:43:34,767 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_90 !
2018-02-05 15:43:34,767 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_93 !
2018-02-05 15:43:34,767 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_92 !
2018-02-05 15:43:34,767 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_37 !
2018-02-05 15:43:34,767 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_36 !
2018-02-05 15:43:34,767 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_40 !
2018-02-05 15:43:34,767 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_84 !
2018-02-05 15:43:34,768 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_83 !
2018-02-05 15:43:34,768 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_42 !
2018-02-05 15:43:34,768 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_86 !
2018-02-05 15:43:34,768 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_41 !
2018-02-05 15:43:34,768 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_85 !
2018-02-05 15:43:34,768 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_44 !
2018-02-05 15:43:34,768 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_88 !
2018-02-05 15:43:34,768 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_43 !
2018-02-05 15:43:34,768 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_87 !
2018-02-05 15:43:34,782 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_46 !
2018-02-05 15:43:34,782 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_45 !
2018-02-05 15:43:34,782 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_89 !
2018-02-05 15:43:34,782 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_80 !
2018-02-05 15:43:34,782 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_82 !
2018-02-05 15:43:34,782 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_81 !
2018-02-05 15:43:34,782 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_69 !
2018-02-05 15:43:34,782 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_73 !
2018-02-05 15:43:34,782 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_72 !
2018-02-05 15:43:34,782 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_75 !
2018-02-05 15:43:34,782 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_74 !
2018-02-05 15:43:34,782 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_33 !
2018-02-05 15:43:34,782 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_77 !
2018-02-05 15:43:34,782 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_76 !
2018-02-05 15:43:34,783 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_79 !
2018-02-05 15:43:34,783 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_78 !
2018-02-05 15:43:34,783 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_71 !
2018-02-05 15:43:34,783 WARN[org.apache.spark.storage.BlockManagerMasterEndpoint:66] - No more replicas available for taskresult_70 !
2018-02-05 15:43:34,784 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Removing block manager BlockManagerId(0, 192.168.11.25, 45842, None)
2018-02-05 15:43:34,785 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Trying to remove executor 0 from BlockManagerMaster.
2018-02-05 15:43:34,785 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Trying to remove executor 1 from BlockManagerMaster.
2018-02-05 15:43:34,787 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Requesting to kill executor(s) 0
2018-02-05 15:43:34,787 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Removed 0 successfully in removeExecutor
2018-02-05 15:43:34,789 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Shuffle files lost for executor: 0 (epoch 0)
2018-02-05 15:43:34,791 WARN[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:66] - Executor to kill 0 does not exist!
2018-02-05 15:43:34,795 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Actual list of executor(s) to be killed is 
2018-02-05 15:43:35,041 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 33.0 in stage 0.0 (TID 33) in 256200 ms on 192.168.11.25 (executor 0) (38/212)
2018-02-05 15:43:35,107 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 37.0 in stage 0.0 (TID 37) in 255065 ms on 192.168.11.25 (executor 0) (39/212)
2018-02-05 15:43:35,156 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 36.0 in stage 0.0 (TID 36) in 256167 ms on 192.168.11.25 (executor 0) (40/212)
2018-02-05 15:43:35,189 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.25:51118) with ID 2
2018-02-05 15:43:35,190 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 51.1 in stage 0.0 (TID 103, 192.168.11.25, executor 2, partition 51, ANY, 4889 bytes)
2018-02-05 15:43:35,190 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 42.1 in stage 0.0 (TID 104, 192.168.11.25, executor 2, partition 42, ANY, 4889 bytes)
2018-02-05 15:43:35,191 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 66.1 in stage 0.0 (TID 105, 192.168.11.25, executor 2, partition 66, ANY, 4889 bytes)
2018-02-05 15:43:35,191 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 75.1 in stage 0.0 (TID 106, 192.168.11.25, executor 2, partition 75, ANY, 4889 bytes)
2018-02-05 15:43:35,221 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.25:34613 with 366.3 MB RAM, BlockManagerId(2, 192.168.11.25, 34613, None)
2018-02-05 15:43:36,149 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.25:34613 (size: 2.2 KB, free: 366.3 MB)
2018-02-05 15:43:36,246 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.25:34613 (size: 27.2 KB, free: 366.3 MB)
2018-02-05 15:43:37,116 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_105 in memory on 192.168.11.25:34613 (size: 1810.3 KB, free: 364.5 MB)
2018-02-05 15:43:37,122 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 93.1 in stage 0.0 (TID 107, 192.168.11.25, executor 2, partition 93, ANY, 4889 bytes)
2018-02-05 15:43:37,128 INFO[org.apache.spark.network.client.TransportClientFactory:254] - Successfully created connection to /192.168.11.25:34613 after 2 ms (0 ms spent in bootstraps)
2018-02-05 15:43:37,411 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 66.1 in stage 0.0 (TID 105) in 2220 ms on 192.168.11.25 (executor 2) (41/212)
2018-02-05 15:43:37,485 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_105 on 192.168.11.25:34613 in memory (size: 1810.3 KB, free: 366.3 MB)
2018-02-05 15:43:37,562 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_106 in memory on 192.168.11.25:34613 (size: 5.6 MB, free: 360.7 MB)
2018-02-05 15:43:37,566 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 102.1 in stage 0.0 (TID 108, 192.168.11.25, executor 2, partition 102, ANY, 4889 bytes)
2018-02-05 15:43:38,303 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_104 in memory on 192.168.11.25:34613 (size: 6.4 MB, free: 354.3 MB)
2018-02-05 15:43:38,311 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 84.1 in stage 0.0 (TID 109, 192.168.11.25, executor 2, partition 84, ANY, 4889 bytes)
2018-02-05 15:43:38,369 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_103 in memory on 192.168.11.25:34613 (size: 6.9 MB, free: 347.4 MB)
2018-02-05 15:43:38,387 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 57.1 in stage 0.0 (TID 110, 192.168.11.25, executor 2, partition 57, ANY, 4889 bytes)
2018-02-05 15:43:38,430 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_108 in memory on 192.168.11.25:34613 (size: 2.2 MB, free: 345.2 MB)
2018-02-05 15:43:38,441 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 48.1 in stage 0.0 (TID 111, 192.168.11.25, executor 2, partition 48, ANY, 4889 bytes)
2018-02-05 15:43:38,891 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 75.1 in stage 0.0 (TID 106) in 3699 ms on 192.168.11.25 (executor 2) (42/212)
2018-02-05 15:43:39,061 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_106 on 192.168.11.25:34613 in memory (size: 5.6 MB, free: 350.8 MB)
2018-02-05 15:43:39,070 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_107 in memory on 192.168.11.25:34613 (size: 7.7 MB, free: 343.1 MB)
2018-02-05 15:43:39,078 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 81.1 in stage 0.0 (TID 112, 192.168.11.25, executor 2, partition 81, ANY, 4889 bytes)
2018-02-05 15:43:39,207 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_112 in memory on 192.168.11.25:34613 (size: 1578.1 KB, free: 341.6 MB)
2018-02-05 15:43:39,219 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 72.1 in stage 0.0 (TID 113, 192.168.11.25, executor 2, partition 72, ANY, 4889 bytes)
2018-02-05 15:43:39,285 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_109 in memory on 192.168.11.25:34613 (size: 4.9 MB, free: 336.7 MB)
2018-02-05 15:43:39,296 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 54.1 in stage 0.0 (TID 114, 192.168.11.25, executor 2, partition 54, ANY, 4889 bytes)
2018-02-05 15:43:39,661 INFO[org.apache.spark.network.client.TransportClientFactory:179] - Found inactive connection to /192.168.11.25:45842, creating a new one.
2018-02-05 15:43:41,962 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_113 in memory on 192.168.11.25:34613 (size: 5.2 MB, free: 331.4 MB)
2018-02-05 15:43:41,962 ERROR[org.apache.spark.network.shuffle.RetryingBlockFetcher:143] - Exception while beginning fetch of 1 outstanding blocks (after 1 retries)
java.io.IOException: Failed to connect to /192.168.11.25:45842
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:98)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused: no further information: /192.168.11.25:45842
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 2 more
2018-02-05 15:43:41,963 INFO[org.apache.spark.network.shuffle.RetryingBlockFetcher:164] - Retrying fetch (2/3) for 1 outstanding blocks after 5000 ms
2018-02-05 15:43:41,963 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_111 in memory on 192.168.11.25:34613 (size: 8.8 MB, free: 322.6 MB)
2018-02-05 15:43:41,964 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_114 in memory on 192.168.11.25:34613 (size: 7.4 MB, free: 315.2 MB)
2018-02-05 15:43:41,964 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_110 in memory on 192.168.11.25:34613 (size: 11.5 MB, free: 303.7 MB)
2018-02-05 15:43:41,970 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 45.1 in stage 0.0 (TID 115, 192.168.11.25, executor 2, partition 45, ANY, 4889 bytes)
2018-02-05 15:43:41,971 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 90.1 in stage 0.0 (TID 116, 192.168.11.25, executor 2, partition 90, ANY, 4889 bytes)
2018-02-05 15:43:41,972 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 63.1 in stage 0.0 (TID 117, 192.168.11.25, executor 2, partition 63, ANY, 4889 bytes)
2018-02-05 15:43:41,978 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 99.1 in stage 0.0 (TID 118, 192.168.11.25, executor 2, partition 99, ANY, 4889 bytes)
2018-02-05 15:43:42,037 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 42.1 in stage 0.0 (TID 104) in 6847 ms on 192.168.11.25 (executor 2) (43/212)
2018-02-05 15:43:42,052 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_104 on 192.168.11.25:34613 in memory (size: 6.4 MB, free: 310.1 MB)
2018-02-05 15:43:42,453 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_115 in memory on 192.168.11.25:34613 (size: 2.0 MB, free: 308.1 MB)
2018-02-05 15:43:42,463 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 78.1 in stage 0.0 (TID 119, 192.168.11.25, executor 2, partition 78, ANY, 4889 bytes)
2018-02-05 15:43:43,664 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_117 in memory on 192.168.11.25:34613 (size: 5.5 MB, free: 302.6 MB)
2018-02-05 15:43:43,664 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_119 in memory on 192.168.11.25:34613 (size: 4.3 MB, free: 298.2 MB)
2018-02-05 15:43:43,670 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 69.1 in stage 0.0 (TID 120, 192.168.11.25, executor 2, partition 69, ANY, 4889 bytes)
2018-02-05 15:43:43,672 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 96.1 in stage 0.0 (TID 121, 192.168.11.25, executor 2, partition 96, ANY, 4889 bytes)
2018-02-05 15:43:43,696 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 51.1 in stage 0.0 (TID 103) in 8507 ms on 192.168.11.25 (executor 2) (44/212)
2018-02-05 15:43:43,730 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_103 on 192.168.11.25:34613 in memory (size: 6.9 MB, free: 305.2 MB)
2018-02-05 15:43:43,791 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 102.1 in stage 0.0 (TID 108) in 6225 ms on 192.168.11.25 (executor 2) (45/212)
2018-02-05 15:43:43,823 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_108 on 192.168.11.25:34613 in memory (size: 2.2 MB, free: 307.4 MB)
2018-02-05 15:43:43,977 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_120 in memory on 192.168.11.25:34613 (size: 2.8 MB, free: 304.6 MB)
2018-02-05 15:43:43,988 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 87.1 in stage 0.0 (TID 122, 192.168.11.25, executor 2, partition 87, ANY, 4889 bytes)
2018-02-05 15:43:44,138 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_118 in memory on 192.168.11.25:34613 (size: 11.3 MB, free: 293.2 MB)
2018-02-05 15:43:44,148 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 60.1 in stage 0.0 (TID 123, 192.168.11.25, executor 2, partition 60, ANY, 4889 bytes)
2018-02-05 15:43:44,976 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_122 in memory on 192.168.11.25:34613 (size: 2.7 MB, free: 290.5 MB)
2018-02-05 15:43:44,986 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 61.1 in stage 0.0 (TID 124, 192.168.11.25, executor 2, partition 61, ANY, 4889 bytes)
2018-02-05 15:43:45,063 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_116 in memory on 192.168.11.25:34613 (size: 14.0 MB, free: 276.5 MB)
2018-02-05 15:43:45,076 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 52.1 in stage 0.0 (TID 125, 192.168.11.25, executor 2, partition 52, ANY, 4889 bytes)
2018-02-05 15:43:45,160 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_123 in memory on 192.168.11.25:34613 (size: 2.8 MB, free: 273.7 MB)
2018-02-05 15:43:45,174 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 43.1 in stage 0.0 (TID 126, 192.168.11.25, executor 2, partition 43, ANY, 4889 bytes)
2018-02-05 15:43:46,301 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_125 in memory on 192.168.11.25:34613 (size: 3.6 MB, free: 270.1 MB)
2018-02-05 15:43:46,303 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_121 in memory on 192.168.11.25:34613 (size: 10.1 MB, free: 260.0 MB)
2018-02-05 15:43:46,306 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 70.1 in stage 0.0 (TID 127, 192.168.11.25, executor 2, partition 70, ANY, 4889 bytes)
2018-02-05 15:43:46,309 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 88.1 in stage 0.0 (TID 128, 192.168.11.25, executor 2, partition 88, ANY, 4889 bytes)
2018-02-05 15:43:46,348 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 81.1 in stage 0.0 (TID 112) in 7271 ms on 192.168.11.25 (executor 2) (46/212)
2018-02-05 15:43:46,358 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 93.1 in stage 0.0 (TID 107) in 9236 ms on 192.168.11.25 (executor 2) (47/212)
2018-02-05 15:43:46,364 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_112 on 192.168.11.25:34613 in memory (size: 1578.1 KB, free: 261.5 MB)
2018-02-05 15:43:46,369 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_107 on 192.168.11.25:34613 in memory (size: 7.7 MB, free: 269.2 MB)
2018-02-05 15:43:46,480 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_128 in memory on 192.168.11.25:34613 (size: 2.1 MB, free: 267.1 MB)
2018-02-05 15:43:46,481 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_126 in memory on 192.168.11.25:34613 (size: 6.9 MB, free: 260.2 MB)
2018-02-05 15:43:46,490 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 79.1 in stage 0.0 (TID 129, 192.168.11.25, executor 2, partition 79, ANY, 4889 bytes)
2018-02-05 15:43:46,495 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 97.1 in stage 0.0 (TID 130, 192.168.11.25, executor 2, partition 97, ANY, 4889 bytes)
2018-02-05 15:43:46,855 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 84.1 in stage 0.0 (TID 109) in 8544 ms on 192.168.11.25 (executor 2) (48/212)
2018-02-05 15:43:46,965 INFO[org.apache.spark.network.client.TransportClientFactory:179] - Found inactive connection to /192.168.11.25:45842, creating a new one.
2018-02-05 15:43:47,218 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_109 on 192.168.11.25:34613 in memory (size: 4.9 MB, free: 265.1 MB)
2018-02-05 15:43:47,332 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_129 in memory on 192.168.11.25:34613 (size: 3.3 MB, free: 261.8 MB)
2018-02-05 15:43:47,343 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 76.1 in stage 0.0 (TID 131, 192.168.11.25, executor 2, partition 76, ANY, 4889 bytes)
2018-02-05 15:43:47,492 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_130 in memory on 192.168.11.25:34613 (size: 5.0 MB, free: 256.8 MB)
2018-02-05 15:43:47,502 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 49.1 in stage 0.0 (TID 132, 192.168.11.25, executor 2, partition 49, ANY, 4889 bytes)
2018-02-05 15:43:47,520 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_127 in memory on 192.168.11.25:34613 (size: 7.0 MB, free: 249.8 MB)
2018-02-05 15:43:47,530 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 40.1 in stage 0.0 (TID 133, 192.168.11.25, executor 2, partition 40, ANY, 4889 bytes)
2018-02-05 15:43:48,706 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_131 in memory on 192.168.11.25:34613 (size: 4.4 MB, free: 245.4 MB)
2018-02-05 15:43:48,707 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_124 in memory on 192.168.11.25:34613 (size: 16.0 MB, free: 229.4 MB)
2018-02-05 15:43:48,709 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_133 in memory on 192.168.11.25:34613 (size: 6.1 MB, free: 223.3 MB)
2018-02-05 15:43:48,710 ERROR[org.apache.spark.network.shuffle.RetryingBlockFetcher:143] - Exception while beginning fetch of 1 outstanding blocks (after 2 retries)
java.io.IOException: Failed to connect to /192.168.11.25:45842
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:98)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused: no further information: /192.168.11.25:45842
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 2 more
2018-02-05 15:43:48,711 INFO[org.apache.spark.network.shuffle.RetryingBlockFetcher:164] - Retrying fetch (3/3) for 1 outstanding blocks after 5000 ms
2018-02-05 15:43:48,712 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 94.1 in stage 0.0 (TID 134, 192.168.11.25, executor 2, partition 94, ANY, 4889 bytes)
2018-02-05 15:43:48,711 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_132 in memory on 192.168.11.25:34613 (size: 7.7 MB, free: 215.7 MB)
2018-02-05 15:43:48,713 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 85.1 in stage 0.0 (TID 135, 192.168.11.25, executor 2, partition 85, ANY, 4889 bytes)
2018-02-05 15:43:48,714 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 67.1 in stage 0.0 (TID 136, 192.168.11.25, executor 2, partition 67, ANY, 4889 bytes)
2018-02-05 15:43:48,721 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 58.1 in stage 0.0 (TID 137, 192.168.11.25, executor 2, partition 58, ANY, 4889 bytes)
2018-02-05 15:43:48,821 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 72.1 in stage 0.0 (TID 113) in 9603 ms on 192.168.11.25 (executor 2) (49/212)
2018-02-05 15:43:48,909 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_113 on 192.168.11.25:34613 in memory (size: 5.2 MB, free: 220.9 MB)
2018-02-05 15:43:49,139 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_136 in memory on 192.168.11.25:34613 (size: 1735.8 KB, free: 219.2 MB)
2018-02-05 15:43:49,150 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 73.1 in stage 0.0 (TID 138, 192.168.11.25, executor 2, partition 73, ANY, 4889 bytes)
2018-02-05 15:43:49,337 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_135 in memory on 192.168.11.25:34613 (size: 4.2 MB, free: 215.0 MB)
2018-02-05 15:43:49,348 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 64.1 in stage 0.0 (TID 139, 192.168.11.25, executor 2, partition 64, ANY, 4889 bytes)
2018-02-05 15:43:49,435 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_138 in memory on 192.168.11.25:34613 (size: 2.2 MB, free: 212.8 MB)
2018-02-05 15:43:49,447 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 55.1 in stage 0.0 (TID 140, 192.168.11.25, executor 2, partition 55, ANY, 4889 bytes)
2018-02-05 15:43:52,151 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_134 in memory on 192.168.11.25:34613 (size: 6.4 MB, free: 206.4 MB)
2018-02-05 15:43:52,153 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_139 in memory on 192.168.11.25:34613 (size: 3.7 MB, free: 202.7 MB)
2018-02-05 15:43:52,154 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_140 in memory on 192.168.11.25:34613 (size: 7.3 MB, free: 195.5 MB)
2018-02-05 15:43:52,154 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_137 in memory on 192.168.11.25:34613 (size: 15.1 MB, free: 180.4 MB)
2018-02-05 15:43:52,159 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 91.1 in stage 0.0 (TID 141, 192.168.11.25, executor 2, partition 91, ANY, 4889 bytes)
2018-02-05 15:43:52,161 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 82.1 in stage 0.0 (TID 142, 192.168.11.25, executor 2, partition 82, ANY, 4889 bytes)
2018-02-05 15:43:52,162 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 100.1 in stage 0.0 (TID 143, 192.168.11.25, executor 2, partition 100, ANY, 4889 bytes)
2018-02-05 15:43:52,163 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 46.1 in stage 0.0 (TID 144, 192.168.11.25, executor 2, partition 46, ANY, 4889 bytes)
2018-02-05 15:43:52,271 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 48.1 in stage 0.0 (TID 111) in 13830 ms on 192.168.11.25 (executor 2) (50/212)
2018-02-05 15:43:52,287 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_111 on 192.168.11.25:34613 in memory (size: 8.8 MB, free: 189.2 MB)
2018-02-05 15:43:52,416 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_144 in memory on 192.168.11.25:34613 (size: 1959.9 KB, free: 187.3 MB)
2018-02-05 15:43:52,914 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 65.1 in stage 0.0 (TID 145, 192.168.11.25, executor 2, partition 65, ANY, 4889 bytes)
2018-02-05 15:43:54,306 INFO[org.apache.spark.network.client.TransportClientFactory:179] - Found inactive connection to /192.168.11.25:45842, creating a new one.
2018-02-05 15:43:54,309 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_142 in memory on 192.168.11.25:34613 (size: 4.5 MB, free: 182.8 MB)
2018-02-05 15:43:54,310 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_145 in memory on 192.168.11.25:34613 (size: 3.6 MB, free: 179.2 MB)
2018-02-05 15:43:54,310 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_141 in memory on 192.168.11.25:34613 (size: 7.9 MB, free: 171.3 MB)
2018-02-05 15:43:54,311 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_143 in memory on 192.168.11.25:34613 (size: 8.6 MB, free: 162.7 MB)
2018-02-05 15:43:54,317 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 47.1 in stage 0.0 (TID 146, 192.168.11.25, executor 2, partition 47, ANY, 4889 bytes)
2018-02-05 15:43:54,318 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 56.1 in stage 0.0 (TID 147, 192.168.11.25, executor 2, partition 56, ANY, 4889 bytes)
2018-02-05 15:43:54,319 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 74.1 in stage 0.0 (TID 148, 192.168.11.25, executor 2, partition 74, ANY, 4889 bytes)
2018-02-05 15:43:54,324 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 71.1 in stage 0.0 (TID 149, 192.168.11.25, executor 2, partition 71, ANY, 4889 bytes)
2018-02-05 15:43:55,604 ERROR[org.apache.spark.network.shuffle.RetryingBlockFetcher:143] - Exception while beginning fetch of 1 outstanding blocks (after 3 retries)
java.io.IOException: Failed to connect to /192.168.11.25:45842
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:98)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.net.ConnectException: Connection refused: no further information: /192.168.11.25:45842
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 2 more
2018-02-05 15:43:55,604 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_148 in memory on 192.168.11.25:34613 (size: 2.5 MB, free: 160.3 MB)
2018-02-05 15:43:55,606 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_149 in memory on 192.168.11.25:34613 (size: 4.4 MB, free: 155.9 MB)
2018-02-05 15:43:55,609 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_147 in memory on 192.168.11.25:34613 (size: 10.2 MB, free: 145.7 MB)
2018-02-05 15:43:55,611 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_146 in memory on 192.168.11.25:34613 (size: 8.1 MB, free: 137.7 MB)
2018-02-05 15:43:55,612 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 98.1 in stage 0.0 (TID 150, 192.168.11.25, executor 2, partition 98, ANY, 4889 bytes)
2018-02-05 15:43:55,615 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 89.1 in stage 0.0 (TID 151, 192.168.11.25, executor 2, partition 89, ANY, 4889 bytes)
2018-02-05 15:43:55,616 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 80.1 in stage 0.0 (TID 152, 192.168.11.25, executor 2, partition 80, ANY, 4889 bytes)
2018-02-05 15:43:55,617 WARN[org.apache.spark.storage.BlockManager:87] - Failed to fetch block after 1 fetch failures. Most recent failure cause:
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:205)
	at org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:105)
	at org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:642)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:82)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$1.apply(TaskResultGetter.scala:63)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:62)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to connect to /192.168.11.25:45842
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:232)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:182)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:98)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:141)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.lambda$initiateRetry$0(RetryingBlockFetcher.java:169)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	... 1 more
Caused by: java.net.ConnectException: Connection refused: no further information: /192.168.11.25:45842
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:208)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:287)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	... 2 more
2018-02-05 15:43:55,619 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 44.1 in stage 0.0 (TID 153, 192.168.11.25, executor 2, partition 44, ANY, 4889 bytes)
2018-02-05 15:43:55,641 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 54.1 in stage 0.0 (TID 114) in 16345 ms on 192.168.11.25 (executor 2) (51/212)
2018-02-05 15:43:55,653 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed taskresult_114 on 192.168.11.25:34613 in memory (size: 7.4 MB, free: 145.1 MB)
2018-02-05 15:43:56,049 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_152 in memory on 192.168.11.25:34613 (size: 2.0 MB, free: 143.0 MB)
2018-02-05 15:43:56,059 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 62.1 in stage 0.0 (TID 154, 192.168.11.25, executor 2, partition 62, ANY, 4889 bytes)
2018-02-05 15:43:59,110 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_151 in memory on 192.168.11.25:34613 (size: 6.0 MB, free: 137.1 MB)
2018-02-05 15:43:59,111 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_153 in memory on 192.168.11.25:34613 (size: 9.0 MB, free: 128.1 MB)
2018-02-05 15:43:59,112 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_150 in memory on 192.168.11.25:34613 (size: 10.7 MB, free: 117.4 MB)
2018-02-05 15:43:59,115 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 53.1 in stage 0.0 (TID 155, 192.168.11.25, executor 2, partition 53, ANY, 4889 bytes)
2018-02-05 15:43:59,119 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 68.1 in stage 0.0 (TID 156, 192.168.11.25, executor 2, partition 68, ANY, 4889 bytes)
2018-02-05 15:43:59,130 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 41.1 in stage 0.0 (TID 157, 192.168.11.25, executor 2, partition 41, ANY, 4889 bytes)
2018-02-05 15:44:00,346 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_155 in memory on 192.168.11.25:34613 (size: 2.9 MB, free: 114.5 MB)
2018-02-05 15:44:00,347 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_154 in memory on 192.168.11.25:34613 (size: 13.2 MB, free: 101.3 MB)
2018-02-05 15:44:00,347 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_156 in memory on 192.168.11.25:34613 (size: 3.2 MB, free: 98.0 MB)
2018-02-05 15:44:00,348 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_157 in memory on 192.168.11.25:34613 (size: 5.6 MB, free: 92.5 MB)
2018-02-05 15:44:00,352 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 59.1 in stage 0.0 (TID 158, 192.168.11.25, executor 2, partition 59, ANY, 4889 bytes)
2018-02-05 15:44:00,353 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 50.1 in stage 0.0 (TID 159, 192.168.11.25, executor 2, partition 50, ANY, 4889 bytes)
2018-02-05 15:44:00,354 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 86.1 in stage 0.0 (TID 160, 192.168.11.25, executor 2, partition 86, ANY, 4889 bytes)
2018-02-05 15:44:00,355 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 77.1 in stage 0.0 (TID 161, 192.168.11.25, executor 2, partition 77, ANY, 4889 bytes)
2018-02-05 15:44:01,516 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_158 in memory on 192.168.11.25:34613 (size: 2.6 MB, free: 89.8 MB)
2018-02-05 15:44:01,518 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_161 in memory on 192.168.11.25:34613 (size: 4.9 MB, free: 84.9 MB)
2018-02-05 15:44:02,680 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_160 in memory on 192.168.11.25:34613 (size: 6.9 MB, free: 78.0 MB)
2018-02-05 15:44:03,788 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 95.1 in stage 0.0 (TID 162, 192.168.11.25, executor 2, partition 95, ANY, 4889 bytes)
2018-02-05 15:44:03,789 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 83.1 in stage 0.0 (TID 163, 192.168.11.25, executor 2, partition 83, ANY, 4889 bytes)
2018-02-05 15:44:03,789 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_159 in memory on 192.168.11.25:34613 (size: 7.9 MB, free: 70.1 MB)
2018-02-05 15:44:03,789 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 101.1 in stage 0.0 (TID 164, 192.168.11.25, executor 2, partition 101, ANY, 4889 bytes)
2018-02-05 15:44:04,890 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 92.1 in stage 0.0 (TID 165, 192.168.11.25, executor 2, partition 92, ANY, 4889 bytes)
2018-02-05 15:44:06,199 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_162 in memory on 192.168.11.25:34613 (size: 2.5 MB, free: 67.6 MB)
2018-02-05 15:44:06,199 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_164 in memory on 192.168.11.25:34613 (size: 3.6 MB, free: 63.9 MB)
2018-02-05 15:44:06,200 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added taskresult_163 in memory on 192.168.11.25:34613 (size: 4.8 MB, free: 59.2 MB)
2018-02-05 15:44:14,239 WARN[org.apache.spark.network.server.TransportChannelHandler:78] - Exception in connection from /192.168.11.25:34613
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at org.apache.spark.network.BlockTransferService$$anon$1.onBlockFetchSuccess(BlockTransferService.scala:99)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$RetryingBlockFetchListener.onBlockFetchSuccess(RetryingBlockFetcher.java:204)
	at org.apache.spark.network.shuffle.OneForOneBlockFetcher$ChunkCallback.onSuccess(OneForOneBlockFetcher.java:91)
	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:171)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:748)
2018-02-05 15:44:14,240 ERROR[org.apache.spark.network.client.TransportResponseHandler:154] - Still have 1 requests outstanding when connection from /192.168.11.25:34613 is closed
2018-02-05 15:44:22,229 ERROR[org.apache.spark.network.shuffle.RetryingBlockFetcher:218] - Failed to fetch block taskresult_117, and will not retry (0 retries)
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at org.apache.spark.network.BlockTransferService$$anon$1.onBlockFetchSuccess(BlockTransferService.scala:99)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$RetryingBlockFetchListener.onBlockFetchSuccess(RetryingBlockFetcher.java:204)
	at org.apache.spark.network.shuffle.OneForOneBlockFetcher$ChunkCallback.onSuccess(OneForOneBlockFetcher.java:91)
	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:171)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:120)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:254)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:333)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:319)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:787)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:130)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Thread.java:748)
2018-02-05 15:58:10,074 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 15:58:10,623 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 15:58:10,688 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 15:58:10,689 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 15:58:10,690 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 15:58:10,691 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 15:58:10,691 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 15:58:11,221 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 51043.
2018-02-05 15:58:11,245 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 15:58:11,296 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 15:58:11,301 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 15:58:11,301 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 15:58:11,316 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-46127bab-3755-4023-aca1-02e33307534c
2018-02-05 15:58:11,350 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 15:58:11,397 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 15:58:11,503 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2885ms
2018-02-05 15:58:11,585 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 15:58:11,602 INFO[org.spark_project.jetty.server.Server:403] - Started @2985ms
2018-02-05 15:58:11,629 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@553f1d75{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 15:58:11,630 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 15:58:11,663 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@458342d3{/jobs,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,664 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4f2613d1{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,664 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,665 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,666 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@83298d7{/stages,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,666 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5b080f3a{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,667 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6b54655f{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,668 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2756c0a7{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,669 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@69637b10{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,669 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@165b2f7f{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,670 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@277f7dd3{/storage,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,671 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2364305a{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,671 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bc715b8{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,672 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,673 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/environment,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,673 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,674 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/executors,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,674 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,675 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,676 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,685 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51850751{/static,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,686 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,687 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/api,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,688 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,688 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@427b5f92{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 15:58:11,691 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 15:58:11,813 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Connecting to master spark://192.168.11.25:7077...
2018-02-05 15:58:11,860 INFO[org.apache.spark.network.client.TransportClientFactory:254] - Successfully created connection to /192.168.11.25:7077 after 32 ms (0 ms spent in bootstraps)
2018-02-05 15:58:11,971 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Connected to Spark cluster with app ID app-20180205155903-0005
2018-02-05 15:58:11,973 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor added: app-20180205155903-0005/0 on worker-20180205151248-192.168.11.25-42981 (192.168.11.25:42981) with 4 cores
2018-02-05 15:58:11,975 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Granted executor ID app-20180205155903-0005/0 on hostPort 192.168.11.25:42981 with 4 cores, 1024.0 MB RAM
2018-02-05 15:58:11,984 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205155903-0005/0 is now RUNNING
2018-02-05 15:58:11,988 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51065.
2018-02-05 15:58:11,990 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:51065
2018-02-05 15:58:11,992 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 15:58:11,997 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 51065, None)
2018-02-05 15:58:12,000 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:51065 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 51065, None)
2018-02-05 15:58:12,003 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 51065, None)
2018-02-05 15:58:12,003 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 51065, None)
2018-02-05 15:58:12,219 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c282004{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 15:58:12,243 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2018-02-05 15:58:12,252 INFO[org.apache.spark.SparkContext:54] - Added JAR hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar at hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1517817492252
2018-02-05 15:58:13,218 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 316.2 KB, free 631.5 MB)
2018-02-05 15:58:13,296 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 631.5 MB)
2018-02-05 15:58:13,297 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.25:58138) with ID 0
2018-02-05 15:58:13,302 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:51065 (size: 27.2 KB, free: 631.8 MB)
2018-02-05 15:58:13,310 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from textFile at SparkWordCount.java:27
2018-02-05 15:58:13,331 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.25:35277 with 366.3 MB RAM, BlockManagerId(0, 192.168.11.25, 35277, None)
2018-02-05 15:58:20,913 INFO[org.apache.hadoop.mapred.FileInputFormat:256] - Total input files to process : 1
2018-02-05 15:58:20,980 INFO[org.apache.spark.SparkContext:54] - Starting job: collect at SparkWordCount.java:44
2018-02-05 15:58:20,996 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (collect at SparkWordCount.java:44) with 2 output partitions
2018-02-05 15:58:20,996 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (collect at SparkWordCount.java:44)
2018-02-05 15:58:20,997 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-05 15:58:20,998 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-05 15:58:21,007 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:29), which has no missing parents
2018-02-05 15:58:21,038 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 631.5 MB)
2018-02-05 15:58:21,045 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 631.5 MB)
2018-02-05 15:58:21,047 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:51065 (size: 2.2 KB, free: 631.8 MB)
2018-02-05 15:58:21,048 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-05 15:58:21,066 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:29) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 15:58:21,067 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 2 tasks
2018-02-05 15:58:21,106 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 15:58:21,108 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, 192.168.11.25, executor 0, partition 1, ANY, 4889 bytes)
2018-02-05 15:58:21,877 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.25:35277 (size: 2.2 KB, free: 366.3 MB)
2018-02-05 15:58:21,985 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.25:35277 (size: 27.2 KB, free: 366.3 MB)
2018-02-05 15:58:22,388 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 1297 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 15:58:22,405 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 1297 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 15:58:22,409 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (collect at SparkWordCount.java:44) finished in 1.319 s
2018-02-05 15:58:22,411 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 15:58:22,423 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: collect at SparkWordCount.java:44, took 1.442281 s
2018-02-05 15:58:22,554 INFO[org.apache.spark.SparkContext:54] - Starting job: countByKey at SparkWordCount.java:46
2018-02-05 15:58:22,564 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (countByKey at SparkWordCount.java:46)
2018-02-05 15:58:22,565 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (countByKey at SparkWordCount.java:46) with 2 output partitions
2018-02-05 15:58:22,565 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (countByKey at SparkWordCount.java:46)
2018-02-05 15:58:22,565 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-05 15:58:22,565 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-05 15:58:22,567 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:46), which has no missing parents
2018-02-05 15:58:22,573 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 4.8 KB, free 631.5 MB)
2018-02-05 15:58:22,586 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 631.5 MB)
2018-02-05 15:58:22,590 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:51065 (size: 2.7 KB, free: 631.8 MB)
2018-02-05 15:58:22,590 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-05 15:58:22,595 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:46) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 15:58:22,595 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 2 tasks
2018-02-05 15:58:22,597 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 2, 192.168.11.25, executor 0, partition 0, ANY, 4878 bytes)
2018-02-05 15:58:22,597 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 3, 192.168.11.25, executor 0, partition 1, ANY, 4878 bytes)
2018-02-05 15:58:22,654 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.25:35277 (size: 2.7 KB, free: 366.3 MB)
2018-02-05 15:58:22,801 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 3) in 204 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 15:58:22,802 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 2) in 206 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 15:58:22,802 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-05 15:58:22,803 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (countByKey at SparkWordCount.java:46) finished in 0.208 s
2018-02-05 15:58:22,804 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-05 15:58:22,804 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-05 15:58:22,804 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-05 15:58:22,805 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-05 15:58:22,808 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:46), which has no missing parents
2018-02-05 15:58:22,814 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 631.4 MB)
2018-02-05 15:58:22,818 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1812.0 B, free 631.4 MB)
2018-02-05 15:58:22,818 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:51065 (size: 1812.0 B, free: 631.8 MB)
2018-02-05 15:58:22,819 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-05 15:58:22,820 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:46) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 15:58:22,821 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 2 tasks
2018-02-05 15:58:22,824 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 4, 192.168.11.25, executor 0, partition 0, NODE_LOCAL, 4625 bytes)
2018-02-05 15:58:22,825 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 5, 192.168.11.25, executor 0, partition 1, NODE_LOCAL, 4625 bytes)
2018-02-05 15:58:22,843 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.25:35277 (size: 1812.0 B, free: 366.3 MB)
2018-02-05 15:58:22,859 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - Asked to send map output locations for shuffle 0 to 192.168.11.25:58138
2018-02-05 15:58:22,865 INFO[org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 0 is 153 bytes
2018-02-05 15:58:22,899 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 4) in 78 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 15:58:22,899 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 5) in 75 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 15:58:22,900 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-05 15:58:22,901 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (countByKey at SparkWordCount.java:46) finished in 0.079 s
2018-02-05 15:58:22,901 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: countByKey at SparkWordCount.java:46, took 0.346977 s
2018-02-05 15:58:22,911 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-05 15:58:22,916 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@553f1d75{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 15:58:22,919 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-05 15:58:22,924 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Shutting down all executors
2018-02-05 15:58:22,925 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Asking each executor to shut down
2018-02-05 15:58:22,935 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-05 15:58:22,954 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-05 15:58:22,954 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-05 15:58:22,959 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-05 15:58:22,961 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-05 15:58:22,964 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-05 15:58:22,965 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-05 15:58:22,966 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-1ad0ae51-3348-4cfc-8918-38e23956c5b2
2018-02-05 16:05:27,034 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 16:05:27,923 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 16:05:27,957 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 16:05:27,957 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 16:05:27,958 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 16:05:27,959 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 16:05:27,959 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 16:05:28,462 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 50181.
2018-02-05 16:05:28,481 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 16:05:28,532 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 16:05:28,537 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 16:05:28,537 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 16:05:28,547 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-62985f56-ebe8-4252-894f-309f888d5a28
2018-02-05 16:05:28,568 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 16:05:28,609 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 16:05:28,707 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2648ms
2018-02-05 16:05:28,790 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 16:05:28,805 INFO[org.spark_project.jetty.server.Server:403] - Started @2748ms
2018-02-05 16:05:28,825 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@5a23d790{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 16:05:28,825 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 16:05:28,849 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@458342d3{/jobs,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,850 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4f2613d1{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,851 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,852 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,853 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@83298d7{/stages,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,856 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5b080f3a{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,857 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6b54655f{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,860 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2756c0a7{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,861 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@69637b10{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,861 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@165b2f7f{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,862 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@277f7dd3{/storage,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,863 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2364305a{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,864 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bc715b8{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,865 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,865 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/environment,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,866 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,867 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/executors,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,868 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,868 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,871 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,880 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51850751{/static,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,881 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,882 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/api,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,883 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,883 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@427b5f92{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 16:05:28,885 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 16:05:28,985 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Connecting to master spark://192.168.11.25:7077...
2018-02-05 16:05:29,038 INFO[org.apache.spark.network.client.TransportClientFactory:254] - Successfully created connection to /192.168.11.25:7077 after 39 ms (0 ms spent in bootstraps)
2018-02-05 16:05:29,127 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Connected to Spark cluster with app ID app-20180205160620-0006
2018-02-05 16:05:29,131 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor added: app-20180205160620-0006/0 on worker-20180205151248-192.168.11.25-42981 (192.168.11.25:42981) with 4 cores
2018-02-05 16:05:29,131 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Granted executor ID app-20180205160620-0006/0 on hostPort 192.168.11.25:42981 with 4 cores, 1024.0 MB RAM
2018-02-05 16:05:29,143 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205160620-0006/0 is now RUNNING
2018-02-05 16:05:29,144 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50203.
2018-02-05 16:05:29,144 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:50203
2018-02-05 16:05:29,146 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 16:05:29,147 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 50203, None)
2018-02-05 16:05:29,150 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:50203 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 50203, None)
2018-02-05 16:05:29,154 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 50203, None)
2018-02-05 16:05:29,155 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 50203, None)
2018-02-05 16:05:29,327 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c282004{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 16:05:29,345 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2018-02-05 16:05:29,350 INFO[org.apache.spark.SparkContext:54] - Added JAR hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar at hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1517817929350
2018-02-05 16:05:30,201 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 316.2 KB, free 631.5 MB)
2018-02-05 16:05:30,259 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 631.5 MB)
2018-02-05 16:05:30,262 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:50203 (size: 27.2 KB, free: 631.8 MB)
2018-02-05 16:05:30,266 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from textFile at SparkWordCount.java:28
2018-02-05 16:05:30,360 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.25:42537) with ID 0
2018-02-05 16:05:30,394 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.25:42387 with 366.3 MB RAM, BlockManagerId(0, 192.168.11.25, 42387, None)
2018-02-05 16:05:37,728 INFO[org.apache.hadoop.mapred.FileInputFormat:256] - Total input files to process : 1
2018-02-05 16:05:37,784 INFO[org.apache.spark.SparkContext:54] - Starting job: collect at SparkWordCount.java:45
2018-02-05 16:05:37,795 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (collect at SparkWordCount.java:45) with 2 output partitions
2018-02-05 16:05:37,795 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (collect at SparkWordCount.java:45)
2018-02-05 16:05:37,796 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-05 16:05:37,797 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-05 16:05:37,803 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:30), which has no missing parents
2018-02-05 16:05:37,826 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 631.5 MB)
2018-02-05 16:05:37,836 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 631.5 MB)
2018-02-05 16:05:37,837 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:50203 (size: 2.2 KB, free: 631.8 MB)
2018-02-05 16:05:37,838 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:05:37,853 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:30) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:05:37,854 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 2 tasks
2018-02-05 16:05:37,890 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 16:05:37,892 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, 192.168.11.25, executor 0, partition 1, ANY, 4889 bytes)
2018-02-05 16:05:38,643 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.25:42387 (size: 2.2 KB, free: 366.3 MB)
2018-02-05 16:05:38,756 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.25:42387 (size: 27.2 KB, free: 366.3 MB)
2018-02-05 16:05:39,097 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 1221 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 16:05:39,117 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 1226 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 16:05:39,121 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 16:05:39,125 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (collect at SparkWordCount.java:45) finished in 1.252 s
2018-02-05 16:05:39,160 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: collect at SparkWordCount.java:45, took 1.375248 s
2018-02-05 16:05:39,294 INFO[org.apache.spark.SparkContext:54] - Starting job: countByKey at SparkWordCount.java:49
2018-02-05 16:05:39,301 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (countByKey at SparkWordCount.java:49)
2018-02-05 16:05:39,302 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (countByKey at SparkWordCount.java:49) with 2 output partitions
2018-02-05 16:05:39,302 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (countByKey at SparkWordCount.java:49)
2018-02-05 16:05:39,302 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-05 16:05:39,303 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-05 16:05:39,308 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:49), which has no missing parents
2018-02-05 16:05:39,314 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 4.8 KB, free 631.5 MB)
2018-02-05 16:05:39,318 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 631.5 MB)
2018-02-05 16:05:39,320 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:50203 (size: 2.7 KB, free: 631.8 MB)
2018-02-05 16:05:39,321 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:05:39,325 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:49) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:05:39,325 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 2 tasks
2018-02-05 16:05:39,328 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 2, 192.168.11.25, executor 0, partition 0, ANY, 4878 bytes)
2018-02-05 16:05:39,329 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 3, 192.168.11.25, executor 0, partition 1, ANY, 4878 bytes)
2018-02-05 16:05:39,359 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.25:42387 (size: 2.7 KB, free: 366.3 MB)
2018-02-05 16:05:39,575 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_2_0 in memory on 192.168.11.25:42387 (size: 2.4 MB, free: 363.9 MB)
2018-02-05 16:05:39,578 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_2_1 in memory on 192.168.11.25:42387 (size: 2.4 MB, free: 361.4 MB)
2018-02-05 16:05:39,661 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 3) in 333 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 16:05:39,663 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 2) in 336 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 16:05:39,663 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-05 16:05:39,664 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (countByKey at SparkWordCount.java:49) finished in 0.338 s
2018-02-05 16:05:39,665 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-05 16:05:39,666 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-05 16:05:39,666 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-05 16:05:39,667 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-05 16:05:39,674 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:49), which has no missing parents
2018-02-05 16:05:39,680 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 631.4 MB)
2018-02-05 16:05:39,685 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1812.0 B, free 631.4 MB)
2018-02-05 16:05:39,686 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:50203 (size: 1812.0 B, free: 631.8 MB)
2018-02-05 16:05:39,687 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:05:39,688 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:49) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:05:39,688 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 2 tasks
2018-02-05 16:05:39,691 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 4, 192.168.11.25, executor 0, partition 0, NODE_LOCAL, 4625 bytes)
2018-02-05 16:05:39,692 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 5, 192.168.11.25, executor 0, partition 1, NODE_LOCAL, 4625 bytes)
2018-02-05 16:05:39,710 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.25:42387 (size: 1812.0 B, free: 361.4 MB)
2018-02-05 16:05:39,725 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - Asked to send map output locations for shuffle 0 to 192.168.11.25:42537
2018-02-05 16:05:39,728 INFO[org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 0 is 153 bytes
2018-02-05 16:05:39,752 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 5) in 61 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 16:05:39,753 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 4) in 64 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 16:05:39,753 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-05 16:05:39,754 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (countByKey at SparkWordCount.java:49) finished in 0.065 s
2018-02-05 16:05:39,755 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: countByKey at SparkWordCount.java:49, took 0.459512 s
2018-02-05 16:05:39,760 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-05 16:05:39,764 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@5a23d790{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 16:05:39,766 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-05 16:05:39,770 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Shutting down all executors
2018-02-05 16:05:39,770 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Asking each executor to shut down
2018-02-05 16:05:39,777 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-05 16:05:39,797 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-05 16:05:39,797 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-05 16:05:39,801 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-05 16:05:39,804 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-05 16:05:39,809 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-05 16:05:39,809 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-05 16:05:39,810 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-762adf44-64fc-4b9e-8a72-5afcf70fb448
2018-02-05 16:06:18,963 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 16:06:19,824 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 16:06:19,859 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 16:06:19,860 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 16:06:19,860 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 16:06:19,861 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 16:06:19,862 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 16:06:20,327 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 50234.
2018-02-05 16:06:20,346 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 16:06:20,394 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 16:06:20,397 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 16:06:20,397 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 16:06:20,407 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-e71470fb-c168-4590-8775-aacd31e4e7da
2018-02-05 16:06:20,430 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 16:06:20,470 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 16:06:20,546 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2735ms
2018-02-05 16:06:20,613 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 16:06:20,640 INFO[org.spark_project.jetty.server.Server:403] - Started @2830ms
2018-02-05 16:06:20,662 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@553f1d75{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 16:06:20,663 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 16:06:20,686 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@458342d3{/jobs,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,687 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4f2613d1{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,688 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,689 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,690 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@83298d7{/stages,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,691 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5b080f3a{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,693 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6b54655f{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,694 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2756c0a7{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,695 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@69637b10{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,696 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@165b2f7f{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,696 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@277f7dd3{/storage,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,696 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2364305a{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,697 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bc715b8{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,699 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,700 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/environment,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,701 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,701 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/executors,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,702 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,703 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,706 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,713 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51850751{/static,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,714 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,716 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/api,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,717 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,718 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@427b5f92{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 16:06:20,720 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 16:06:20,818 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Connecting to master spark://192.168.11.25:7077...
2018-02-05 16:06:20,888 INFO[org.apache.spark.network.client.TransportClientFactory:254] - Successfully created connection to /192.168.11.25:7077 after 47 ms (0 ms spent in bootstraps)
2018-02-05 16:06:20,985 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Connected to Spark cluster with app ID app-20180205160712-0007
2018-02-05 16:06:20,988 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor added: app-20180205160712-0007/0 on worker-20180205151248-192.168.11.25-42981 (192.168.11.25:42981) with 4 cores
2018-02-05 16:06:20,989 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Granted executor ID app-20180205160712-0007/0 on hostPort 192.168.11.25:42981 with 4 cores, 1024.0 MB RAM
2018-02-05 16:06:21,008 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205160712-0007/0 is now RUNNING
2018-02-05 16:06:21,008 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50256.
2018-02-05 16:06:21,009 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:50256
2018-02-05 16:06:21,011 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 16:06:21,013 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 50256, None)
2018-02-05 16:06:21,017 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:50256 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 50256, None)
2018-02-05 16:06:21,021 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 50256, None)
2018-02-05 16:06:21,023 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 50256, None)
2018-02-05 16:06:21,299 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c282004{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 16:06:21,321 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2018-02-05 16:06:21,329 INFO[org.apache.spark.SparkContext:54] - Added JAR hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar at hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1517817981328
2018-02-05 16:06:22,116 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 316.2 KB, free 631.5 MB)
2018-02-05 16:06:22,193 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 631.5 MB)
2018-02-05 16:06:22,197 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:50256 (size: 27.2 KB, free: 631.8 MB)
2018-02-05 16:06:22,206 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from textFile at SparkWordCount.java:28
2018-02-05 16:06:22,692 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.25:41369) with ID 0
2018-02-05 16:06:22,730 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.25:48513 with 366.3 MB RAM, BlockManagerId(0, 192.168.11.25, 48513, None)
2018-02-05 16:06:29,851 INFO[org.apache.hadoop.mapred.FileInputFormat:256] - Total input files to process : 1
2018-02-05 16:06:29,904 INFO[org.apache.spark.SparkContext:54] - Starting job: collect at SparkWordCount.java:45
2018-02-05 16:06:29,914 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (collect at SparkWordCount.java:45) with 2 output partitions
2018-02-05 16:06:29,914 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (collect at SparkWordCount.java:45)
2018-02-05 16:06:29,915 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-05 16:06:29,916 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-05 16:06:29,925 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:30), which has no missing parents
2018-02-05 16:06:29,947 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 631.5 MB)
2018-02-05 16:06:29,952 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 631.5 MB)
2018-02-05 16:06:29,953 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:50256 (size: 2.2 KB, free: 631.8 MB)
2018-02-05 16:06:29,955 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:06:29,970 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:30) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:06:29,972 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 2 tasks
2018-02-05 16:06:30,003 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 16:06:30,005 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, 192.168.11.25, executor 0, partition 1, ANY, 4889 bytes)
2018-02-05 16:06:30,733 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.25:48513 (size: 2.2 KB, free: 366.3 MB)
2018-02-05 16:06:30,841 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.25:48513 (size: 27.2 KB, free: 366.3 MB)
2018-02-05 16:06:31,198 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 1192 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 16:06:31,209 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 1219 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 16:06:31,211 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 16:06:31,212 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (collect at SparkWordCount.java:45) finished in 1.223 s
2018-02-05 16:06:31,220 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: collect at SparkWordCount.java:45, took 1.314937 s
2018-02-05 16:06:31,392 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.25:48513 in memory (size: 2.2 KB, free: 366.3 MB)
2018-02-05 16:06:31,416 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:50256 in memory (size: 2.2 KB, free: 631.8 MB)
2018-02-05 16:06:31,447 INFO[org.apache.spark.SparkContext:54] - Starting job: countByKey at SparkWordCount.java:49
2018-02-05 16:06:31,453 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (countByKey at SparkWordCount.java:49)
2018-02-05 16:06:31,454 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (countByKey at SparkWordCount.java:49) with 2 output partitions
2018-02-05 16:06:31,454 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (countByKey at SparkWordCount.java:49)
2018-02-05 16:06:31,454 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-05 16:06:31,454 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-05 16:06:31,457 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:49), which has no missing parents
2018-02-05 16:06:31,463 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 4.8 KB, free 631.5 MB)
2018-02-05 16:06:31,466 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 631.5 MB)
2018-02-05 16:06:31,468 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:50256 (size: 2.7 KB, free: 631.8 MB)
2018-02-05 16:06:31,469 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:06:31,471 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:49) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:06:31,471 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 2 tasks
2018-02-05 16:06:31,474 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 2, 192.168.11.25, executor 0, partition 0, ANY, 4878 bytes)
2018-02-05 16:06:31,474 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 3, 192.168.11.25, executor 0, partition 1, ANY, 4878 bytes)
2018-02-05 16:06:31,490 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.25:48513 (size: 2.7 KB, free: 366.3 MB)
2018-02-05 16:06:31,704 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_2_1 in memory on 192.168.11.25:48513 (size: 2.4 MB, free: 363.8 MB)
2018-02-05 16:06:31,706 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_2_0 in memory on 192.168.11.25:48513 (size: 2.4 MB, free: 361.4 MB)
2018-02-05 16:06:31,778 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 3) in 304 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 16:06:31,778 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 2) in 306 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 16:06:31,778 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-05 16:06:31,780 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (countByKey at SparkWordCount.java:49) finished in 0.308 s
2018-02-05 16:06:31,781 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-05 16:06:31,781 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-05 16:06:31,782 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-05 16:06:31,782 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-05 16:06:31,786 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:49), which has no missing parents
2018-02-05 16:06:31,791 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 631.5 MB)
2018-02-05 16:06:31,797 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1812.0 B, free 631.5 MB)
2018-02-05 16:06:31,798 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:50256 (size: 1812.0 B, free: 631.8 MB)
2018-02-05 16:06:31,799 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:06:31,800 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:49) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:06:31,800 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 2 tasks
2018-02-05 16:06:31,803 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 4, 192.168.11.25, executor 0, partition 0, NODE_LOCAL, 4625 bytes)
2018-02-05 16:06:31,803 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 5, 192.168.11.25, executor 0, partition 1, NODE_LOCAL, 4625 bytes)
2018-02-05 16:06:31,818 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.25:48513 (size: 1812.0 B, free: 361.4 MB)
2018-02-05 16:06:31,833 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - Asked to send map output locations for shuffle 0 to 192.168.11.25:41369
2018-02-05 16:06:31,836 INFO[org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 0 is 153 bytes
2018-02-05 16:06:31,860 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 5) in 57 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 16:06:31,861 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 4) in 60 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 16:06:31,861 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-05 16:06:31,862 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (countByKey at SparkWordCount.java:49) finished in 0.060 s
2018-02-05 16:06:31,862 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: countByKey at SparkWordCount.java:49, took 0.414671 s
2018-02-05 16:06:31,867 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-05 16:06:31,872 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@553f1d75{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 16:06:31,876 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-05 16:06:31,881 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Shutting down all executors
2018-02-05 16:06:31,882 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Asking each executor to shut down
2018-02-05 16:06:31,890 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-05 16:06:31,914 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-05 16:06:31,915 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-05 16:06:31,917 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-05 16:06:31,921 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-05 16:06:31,927 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-05 16:06:31,928 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-05 16:06:31,930 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-eca98aea-47bf-4646-926d-3f919e002fb2
2018-02-05 16:07:21,828 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 16:07:22,618 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 16:07:22,662 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 16:07:22,663 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 16:07:22,664 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 16:07:22,665 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 16:07:22,665 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 16:07:23,156 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 50286.
2018-02-05 16:07:23,176 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 16:07:23,224 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 16:07:23,226 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 16:07:23,227 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 16:07:23,237 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-9e8d11f8-3b9e-4be5-93f4-6169a8f2cd71
2018-02-05 16:07:23,259 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 16:07:23,297 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 16:07:23,380 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2638ms
2018-02-05 16:07:23,476 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 16:07:23,491 INFO[org.spark_project.jetty.server.Server:403] - Started @2750ms
2018-02-05 16:07:23,513 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@553f1d75{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 16:07:23,513 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 16:07:23,547 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@458342d3{/jobs,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,548 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4f2613d1{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,549 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,550 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,551 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@83298d7{/stages,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,551 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5b080f3a{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,553 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6b54655f{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,555 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2756c0a7{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,558 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@69637b10{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,559 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@165b2f7f{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,561 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@277f7dd3{/storage,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,562 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2364305a{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,567 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bc715b8{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,569 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,570 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/environment,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,571 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,574 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/executors,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,575 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,576 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,578 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,585 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51850751{/static,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,586 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,590 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/api,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,591 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,593 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@427b5f92{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 16:07:23,596 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 16:07:23,725 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Connecting to master spark://192.168.11.25:7077...
2018-02-05 16:07:23,786 INFO[org.apache.spark.network.client.TransportClientFactory:254] - Successfully created connection to /192.168.11.25:7077 after 44 ms (0 ms spent in bootstraps)
2018-02-05 16:07:23,900 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Connected to Spark cluster with app ID app-20180205160814-0008
2018-02-05 16:07:23,901 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor added: app-20180205160814-0008/0 on worker-20180205151248-192.168.11.25-42981 (192.168.11.25:42981) with 4 cores
2018-02-05 16:07:23,910 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Granted executor ID app-20180205160814-0008/0 on hostPort 192.168.11.25:42981 with 4 cores, 1024.0 MB RAM
2018-02-05 16:07:23,915 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205160814-0008/0 is now RUNNING
2018-02-05 16:07:23,926 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50308.
2018-02-05 16:07:23,927 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:50308
2018-02-05 16:07:23,929 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 16:07:23,931 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 50308, None)
2018-02-05 16:07:23,935 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:50308 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 50308, None)
2018-02-05 16:07:23,937 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 50308, None)
2018-02-05 16:07:23,938 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 50308, None)
2018-02-05 16:07:24,228 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c282004{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 16:07:24,257 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2018-02-05 16:07:24,265 INFO[org.apache.spark.SparkContext:54] - Added JAR hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar at hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1517818044265
2018-02-05 16:07:25,053 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 316.2 KB, free 631.5 MB)
2018-02-05 16:07:25,123 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 631.5 MB)
2018-02-05 16:07:25,128 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:50308 (size: 27.2 KB, free: 631.8 MB)
2018-02-05 16:07:25,137 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from textFile at SparkWordCount.java:28
2018-02-05 16:07:25,179 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.25:48495) with ID 0
2018-02-05 16:07:25,212 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.25:55574 with 366.3 MB RAM, BlockManagerId(0, 192.168.11.25, 55574, None)
2018-02-05 16:07:32,842 INFO[org.apache.hadoop.mapred.FileInputFormat:256] - Total input files to process : 1
2018-02-05 16:07:32,900 INFO[org.apache.spark.SparkContext:54] - Starting job: collect at SparkWordCount.java:45
2018-02-05 16:07:32,912 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (collect at SparkWordCount.java:45) with 2 output partitions
2018-02-05 16:07:32,912 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (collect at SparkWordCount.java:45)
2018-02-05 16:07:32,913 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-05 16:07:32,914 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-05 16:07:32,920 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:30), which has no missing parents
2018-02-05 16:07:32,940 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 631.5 MB)
2018-02-05 16:07:32,952 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 631.5 MB)
2018-02-05 16:07:32,956 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:50308 (size: 2.2 KB, free: 631.8 MB)
2018-02-05 16:07:32,957 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:07:32,974 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:30) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:07:32,976 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 2 tasks
2018-02-05 16:07:33,006 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 16:07:33,008 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, 192.168.11.25, executor 0, partition 1, ANY, 4889 bytes)
2018-02-05 16:07:33,721 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.25:55574 (size: 2.2 KB, free: 366.3 MB)
2018-02-05 16:07:33,837 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.25:55574 (size: 27.2 KB, free: 366.3 MB)
2018-02-05 16:07:34,213 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 1218 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 16:07:34,241 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 1234 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 16:07:34,278 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (collect at SparkWordCount.java:45) finished in 1.273 s
2018-02-05 16:07:34,279 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 16:07:34,290 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: collect at SparkWordCount.java:45, took 1.389468 s
2018-02-05 16:07:34,375 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.25:55574 in memory (size: 2.2 KB, free: 366.3 MB)
2018-02-05 16:07:34,381 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:50308 in memory (size: 2.2 KB, free: 631.8 MB)
2018-02-05 16:07:34,416 INFO[org.apache.spark.SparkContext:54] - Starting job: countByKey at SparkWordCount.java:49
2018-02-05 16:07:34,424 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (countByKey at SparkWordCount.java:49)
2018-02-05 16:07:34,425 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (countByKey at SparkWordCount.java:49) with 2 output partitions
2018-02-05 16:07:34,425 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (countByKey at SparkWordCount.java:49)
2018-02-05 16:07:34,425 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-05 16:07:34,425 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-05 16:07:34,428 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:49), which has no missing parents
2018-02-05 16:07:34,435 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 4.8 KB, free 631.5 MB)
2018-02-05 16:07:34,439 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 631.5 MB)
2018-02-05 16:07:34,442 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:50308 (size: 2.7 KB, free: 631.8 MB)
2018-02-05 16:07:34,443 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:07:34,445 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:49) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:07:34,445 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 2 tasks
2018-02-05 16:07:34,448 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 2, 192.168.11.25, executor 0, partition 0, ANY, 4878 bytes)
2018-02-05 16:07:34,448 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 3, 192.168.11.25, executor 0, partition 1, ANY, 4878 bytes)
2018-02-05 16:07:34,469 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.25:55574 (size: 2.7 KB, free: 366.3 MB)
2018-02-05 16:07:34,655 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_2_1 in memory on 192.168.11.25:55574 (size: 2.4 MB, free: 363.8 MB)
2018-02-05 16:07:34,659 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_2_0 in memory on 192.168.11.25:55574 (size: 2.4 MB, free: 361.4 MB)
2018-02-05 16:07:34,735 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 2) in 289 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 16:07:34,741 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 3) in 293 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 16:07:34,742 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-05 16:07:34,743 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (countByKey at SparkWordCount.java:49) finished in 0.296 s
2018-02-05 16:07:34,744 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-05 16:07:34,744 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-05 16:07:34,744 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-05 16:07:34,745 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-05 16:07:34,748 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:49), which has no missing parents
2018-02-05 16:07:34,753 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 631.5 MB)
2018-02-05 16:07:34,759 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1812.0 B, free 631.5 MB)
2018-02-05 16:07:34,760 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:50308 (size: 1812.0 B, free: 631.8 MB)
2018-02-05 16:07:34,761 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:07:34,762 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:49) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:07:34,762 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 2 tasks
2018-02-05 16:07:34,765 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 4, 192.168.11.25, executor 0, partition 0, NODE_LOCAL, 4625 bytes)
2018-02-05 16:07:34,766 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 5, 192.168.11.25, executor 0, partition 1, NODE_LOCAL, 4625 bytes)
2018-02-05 16:07:34,784 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.25:55574 (size: 1812.0 B, free: 361.4 MB)
2018-02-05 16:07:34,799 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - Asked to send map output locations for shuffle 0 to 192.168.11.25:48495
2018-02-05 16:07:34,803 INFO[org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 0 is 153 bytes
2018-02-05 16:07:34,828 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 4) in 65 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 16:07:34,828 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 5) in 62 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 16:07:34,828 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-05 16:07:34,829 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (countByKey at SparkWordCount.java:49) finished in 0.066 s
2018-02-05 16:07:34,829 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: countByKey at SparkWordCount.java:49, took 0.412847 s
2018-02-05 16:07:34,834 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-05 16:07:34,838 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@553f1d75{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 16:07:34,840 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-05 16:07:34,844 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Shutting down all executors
2018-02-05 16:07:34,844 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Asking each executor to shut down
2018-02-05 16:07:34,851 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-05 16:07:34,869 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-05 16:07:34,870 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-05 16:07:34,871 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-05 16:07:34,873 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-05 16:07:34,877 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-05 16:07:34,877 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-05 16:07:34,878 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-c233d16e-8e14-4cc9-9aca-7dc368bc17c0
2018-02-05 16:12:56,171 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 16:12:57,245 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 16:12:57,291 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 16:12:57,292 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 16:12:57,293 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 16:12:57,294 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 16:12:57,294 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 16:12:57,831 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 50390.
2018-02-05 16:12:57,852 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 16:12:57,909 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 16:12:57,912 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 16:12:57,913 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 16:12:57,924 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-31ba04d0-1027-42cc-b755-b22a933c29d4
2018-02-05 16:12:57,945 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 16:12:57,988 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 16:12:58,070 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2935ms
2018-02-05 16:12:58,136 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 16:12:58,151 INFO[org.spark_project.jetty.server.Server:403] - Started @3018ms
2018-02-05 16:12:58,174 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@7e52e37c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 16:12:58,174 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 16:12:58,197 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c80e49b{/jobs,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,199 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55e8ec2f{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,199 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d24ffa1{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,200 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@878452d{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,200 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c51bb7{/stages,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,201 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42a9e5d1{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,201 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@773cbf4f{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,203 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f603e89{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,204 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@350ec41e{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,204 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71984c3{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,205 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5536379e{/storage,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,205 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6c8bca63{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,206 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,207 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,207 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f1c5af9{/environment,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,207 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,208 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/executors,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,208 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,209 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,210 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,217 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@38f57b3d{/static,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,218 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,219 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/api,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,220 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,220 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ddae9b5{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,222 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 16:12:58,326 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Connecting to master spark://192.168.11.25:7077...
2018-02-05 16:12:58,392 INFO[org.apache.spark.network.client.TransportClientFactory:254] - Successfully created connection to /192.168.11.25:7077 after 50 ms (0 ms spent in bootstraps)
2018-02-05 16:12:58,504 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Connected to Spark cluster with app ID app-20180205161349-0009
2018-02-05 16:12:58,508 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor added: app-20180205161349-0009/0 on worker-20180205151248-192.168.11.25-42981 (192.168.11.25:42981) with 4 cores
2018-02-05 16:12:58,509 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Granted executor ID app-20180205161349-0009/0 on hostPort 192.168.11.25:42981 with 4 cores, 1024.0 MB RAM
2018-02-05 16:12:58,511 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205161349-0009/0 is now RUNNING
2018-02-05 16:12:58,525 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50412.
2018-02-05 16:12:58,526 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:50412
2018-02-05 16:12:58,528 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 16:12:58,530 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 50412, None)
2018-02-05 16:12:58,534 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:50412 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 50412, None)
2018-02-05 16:12:58,538 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 50412, None)
2018-02-05 16:12:58,539 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 50412, None)
2018-02-05 16:12:58,793 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7dd712e8{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 16:12:58,814 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2018-02-05 16:12:58,823 INFO[org.apache.spark.SparkContext:54] - Added JAR hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar at hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1517818378823
2018-02-05 16:12:59,601 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 316.2 KB, free 631.5 MB)
2018-02-05 16:12:59,697 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 631.5 MB)
2018-02-05 16:12:59,704 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:50412 (size: 27.2 KB, free: 631.8 MB)
2018-02-05 16:12:59,707 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from textFile at SparkWordCount.java:28
2018-02-05 16:12:59,791 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.25:59012) with ID 0
2018-02-05 16:12:59,826 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.25:58987 with 366.3 MB RAM, BlockManagerId(0, 192.168.11.25, 58987, None)
2018-02-05 16:13:07,402 INFO[org.apache.hadoop.mapred.FileInputFormat:256] - Total input files to process : 1
2018-02-05 16:13:07,457 INFO[org.apache.spark.SparkContext:54] - Starting job: collect at SparkWordCount.java:45
2018-02-05 16:13:07,467 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (collect at SparkWordCount.java:45) with 2 output partitions
2018-02-05 16:13:07,468 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (collect at SparkWordCount.java:45)
2018-02-05 16:13:07,469 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-05 16:13:07,470 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-05 16:13:07,477 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:30), which has no missing parents
2018-02-05 16:13:07,498 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 631.5 MB)
2018-02-05 16:13:07,504 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 631.5 MB)
2018-02-05 16:13:07,505 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:50412 (size: 2.2 KB, free: 631.8 MB)
2018-02-05 16:13:07,506 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:13:07,519 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:30) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:13:07,520 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 2 tasks
2018-02-05 16:13:07,553 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 16:13:07,555 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, 192.168.11.25, executor 0, partition 1, ANY, 4889 bytes)
2018-02-05 16:13:08,279 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.25:58987 (size: 2.2 KB, free: 366.3 MB)
2018-02-05 16:13:08,394 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.25:58987 (size: 27.2 KB, free: 366.3 MB)
2018-02-05 16:13:08,769 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 1223 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 16:13:08,782 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 1228 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 16:13:08,783 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 16:13:08,786 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (collect at SparkWordCount.java:45) finished in 1.246 s
2018-02-05 16:13:08,797 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: collect at SparkWordCount.java:45, took 1.340337 s
2018-02-05 16:13:08,871 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:50412 in memory (size: 2.2 KB, free: 631.8 MB)
2018-02-05 16:13:08,873 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.25:58987 in memory (size: 2.2 KB, free: 366.3 MB)
2018-02-05 16:13:08,924 INFO[org.apache.spark.SparkContext:54] - Starting job: countByKey at SparkWordCount.java:49
2018-02-05 16:13:08,930 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (countByKey at SparkWordCount.java:49)
2018-02-05 16:13:08,931 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (countByKey at SparkWordCount.java:49) with 2 output partitions
2018-02-05 16:13:08,931 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (countByKey at SparkWordCount.java:49)
2018-02-05 16:13:08,931 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-05 16:13:08,932 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-05 16:13:08,934 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:49), which has no missing parents
2018-02-05 16:13:08,940 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 4.8 KB, free 631.5 MB)
2018-02-05 16:13:08,944 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 631.5 MB)
2018-02-05 16:13:08,947 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:50412 (size: 2.7 KB, free: 631.8 MB)
2018-02-05 16:13:08,947 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:13:08,949 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:49) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:13:08,949 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 2 tasks
2018-02-05 16:13:08,951 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 2, 192.168.11.25, executor 0, partition 0, ANY, 4878 bytes)
2018-02-05 16:13:08,952 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 3, 192.168.11.25, executor 0, partition 1, ANY, 4878 bytes)
2018-02-05 16:13:08,970 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.25:58987 (size: 2.7 KB, free: 366.3 MB)
2018-02-05 16:13:09,166 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_2_0 in memory on 192.168.11.25:58987 (size: 2.4 MB, free: 363.9 MB)
2018-02-05 16:13:09,167 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_2_1 in memory on 192.168.11.25:58987 (size: 2.4 MB, free: 361.4 MB)
2018-02-05 16:13:09,246 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 3) in 294 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 16:13:09,249 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 2) in 299 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 16:13:09,251 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (countByKey at SparkWordCount.java:49) finished in 0.301 s
2018-02-05 16:13:09,252 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-05 16:13:09,252 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-05 16:13:09,252 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-05 16:13:09,253 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-05 16:13:09,253 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-05 16:13:09,256 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:49), which has no missing parents
2018-02-05 16:13:09,259 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 631.5 MB)
2018-02-05 16:13:09,263 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1812.0 B, free 631.5 MB)
2018-02-05 16:13:09,263 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:50412 (size: 1812.0 B, free: 631.8 MB)
2018-02-05 16:13:09,264 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:13:09,265 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:49) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:13:09,265 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 2 tasks
2018-02-05 16:13:09,267 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 4, 192.168.11.25, executor 0, partition 0, NODE_LOCAL, 4625 bytes)
2018-02-05 16:13:09,268 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 5, 192.168.11.25, executor 0, partition 1, NODE_LOCAL, 4625 bytes)
2018-02-05 16:13:09,284 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.25:58987 (size: 1812.0 B, free: 361.4 MB)
2018-02-05 16:13:09,299 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - Asked to send map output locations for shuffle 0 to 192.168.11.25:59012
2018-02-05 16:13:09,301 INFO[org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 0 is 153 bytes
2018-02-05 16:13:09,327 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 5) in 59 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 16:13:09,327 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 4) in 61 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 16:13:09,327 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-05 16:13:09,328 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (countByKey at SparkWordCount.java:49) finished in 0.062 s
2018-02-05 16:13:09,328 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: countByKey at SparkWordCount.java:49, took 0.404254 s
2018-02-05 16:13:09,337 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-05 16:13:09,342 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@7e52e37c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 16:13:09,344 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-05 16:13:09,348 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Shutting down all executors
2018-02-05 16:13:09,349 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Asking each executor to shut down
2018-02-05 16:13:09,359 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-05 16:13:09,383 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-05 16:13:09,384 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-05 16:13:09,387 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-05 16:13:09,392 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-05 16:13:09,397 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-05 16:13:09,398 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-05 16:13:09,399 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-930cf80e-c446-42f1-adb2-c4619f997817
2018-02-05 16:42:58,246 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 16:42:59,220 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 16:42:59,264 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 16:42:59,265 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 16:42:59,266 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 16:42:59,267 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 16:42:59,268 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 16:42:59,934 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 51032.
2018-02-05 16:42:59,965 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 16:43:00,024 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 16:43:00,028 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 16:43:00,028 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 16:43:00,043 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-b93fef1e-5d7d-4da1-861c-b81d21a2b4ce
2018-02-05 16:43:00,074 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 16:43:00,127 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 16:43:00,245 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @3328ms
2018-02-05 16:43:00,329 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 16:43:00,345 INFO[org.spark_project.jetty.server.Server:403] - Started @3429ms
2018-02-05 16:43:00,372 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@2d358d99{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 16:43:00,372 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 16:43:00,404 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@458342d3{/jobs,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,405 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4f2613d1{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,406 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@65a4798f{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,407 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@426b6a74{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,408 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@83298d7{/stages,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,408 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5b080f3a{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,409 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6b54655f{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,410 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2756c0a7{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,411 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@69637b10{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,412 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@165b2f7f{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,412 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@277f7dd3{/storage,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,413 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2364305a{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,414 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1bc715b8{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,414 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2b491fee{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,415 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c55f277{/environment,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,415 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e8f7922{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,416 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/executors,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,419 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,420 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,421 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,432 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@51850751{/static,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,433 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,434 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/api,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,435 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,435 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@427b5f92{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 16:43:00,438 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 16:43:00,575 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Connecting to master spark://192.168.11.25:7077...
2018-02-05 16:43:00,664 INFO[org.apache.spark.network.client.TransportClientFactory:254] - Successfully created connection to /192.168.11.25:7077 after 70 ms (0 ms spent in bootstraps)
2018-02-05 16:43:00,806 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Connected to Spark cluster with app ID app-20180205164351-0010
2018-02-05 16:43:00,817 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor added: app-20180205164351-0010/0 on worker-20180205151248-192.168.11.25-42981 (192.168.11.25:42981) with 4 cores
2018-02-05 16:43:00,819 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Granted executor ID app-20180205164351-0010/0 on hostPort 192.168.11.25:42981 with 4 cores, 1024.0 MB RAM
2018-02-05 16:43:00,825 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205164351-0010/0 is now RUNNING
2018-02-05 16:43:00,833 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51054.
2018-02-05 16:43:00,834 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:51054
2018-02-05 16:43:00,837 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 16:43:00,841 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 51054, None)
2018-02-05 16:43:00,847 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:51054 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 51054, None)
2018-02-05 16:43:00,851 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 51054, None)
2018-02-05 16:43:00,853 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 51054, None)
2018-02-05 16:43:01,156 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c282004{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 16:43:01,182 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2018-02-05 16:43:01,199 INFO[org.apache.spark.SparkContext:54] - Added JAR hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar at hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1517820181198
2018-02-05 16:43:02,369 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.25:36425) with ID 0
2018-02-05 16:43:02,406 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.25:44736 with 366.3 MB RAM, BlockManagerId(0, 192.168.11.25, 44736, None)
2018-02-05 16:43:02,486 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 316.2 KB, free 631.5 MB)
2018-02-05 16:43:02,567 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 631.5 MB)
2018-02-05 16:43:02,572 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:51054 (size: 27.2 KB, free: 631.8 MB)
2018-02-05 16:43:02,579 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from textFile at SparkWordCount.java:28
2018-02-05 16:43:10,129 INFO[org.apache.hadoop.mapred.FileInputFormat:256] - Total input files to process : 1
2018-02-05 16:43:10,197 INFO[org.apache.spark.SparkContext:54] - Starting job: collect at SparkWordCount.java:45
2018-02-05 16:43:10,211 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (collect at SparkWordCount.java:45) with 2 output partitions
2018-02-05 16:43:10,212 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (collect at SparkWordCount.java:45)
2018-02-05 16:43:10,213 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-05 16:43:10,215 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-05 16:43:10,225 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:30), which has no missing parents
2018-02-05 16:43:10,255 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 3.8 KB, free 631.5 MB)
2018-02-05 16:43:10,261 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.2 KB, free 631.5 MB)
2018-02-05 16:43:10,265 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:51054 (size: 2.2 KB, free: 631.8 MB)
2018-02-05 16:43:10,266 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:43:10,284 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:30) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:43:10,286 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 2 tasks
2018-02-05 16:43:10,334 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 16:43:10,344 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, 192.168.11.25, executor 0, partition 1, ANY, 4889 bytes)
2018-02-05 16:43:11,089 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.25:44736 (size: 2.2 KB, free: 366.3 MB)
2018-02-05 16:43:11,203 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.25:44736 (size: 27.2 KB, free: 366.3 MB)
2018-02-05 16:43:11,450 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 1105 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 16:43:11,469 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 1155 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 16:43:11,474 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 16:43:11,481 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (collect at SparkWordCount.java:45) finished in 1.164 s
2018-02-05 16:43:11,487 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: collect at SparkWordCount.java:45, took 1.289233 s
2018-02-05 16:43:11,586 INFO[org.apache.spark.SparkContext:54] - Starting job: countByKey at SparkWordCount.java:49
2018-02-05 16:43:11,595 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (countByKey at SparkWordCount.java:49)
2018-02-05 16:43:11,596 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (countByKey at SparkWordCount.java:49) with 2 output partitions
2018-02-05 16:43:11,596 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (countByKey at SparkWordCount.java:49)
2018-02-05 16:43:11,596 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-05 16:43:11,596 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-05 16:43:11,601 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:49), which has no missing parents
2018-02-05 16:43:11,610 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 4.8 KB, free 631.5 MB)
2018-02-05 16:43:11,613 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.7 KB, free 631.5 MB)
2018-02-05 16:43:11,616 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:51054 (size: 2.7 KB, free: 631.8 MB)
2018-02-05 16:43:11,616 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:43:11,619 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:49) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:43:11,619 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 2 tasks
2018-02-05 16:43:11,622 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 2, 192.168.11.25, executor 0, partition 0, ANY, 4878 bytes)
2018-02-05 16:43:11,623 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 3, 192.168.11.25, executor 0, partition 1, ANY, 4878 bytes)
2018-02-05 16:43:11,648 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.25:44736 (size: 2.7 KB, free: 366.3 MB)
2018-02-05 16:43:11,830 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_2_0 in memory on 192.168.11.25:44736 (size: 1317.5 KB, free: 365.0 MB)
2018-02-05 16:43:11,838 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_2_1 in memory on 192.168.11.25:44736 (size: 1428.2 KB, free: 363.6 MB)
2018-02-05 16:43:11,906 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 3) in 283 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 16:43:11,913 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 2) in 293 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 16:43:11,916 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (countByKey at SparkWordCount.java:49) finished in 0.296 s
2018-02-05 16:43:11,917 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-05 16:43:11,918 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-05 16:43:11,919 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-05 16:43:11,919 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-05 16:43:11,920 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-05 16:43:11,925 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:49), which has no missing parents
2018-02-05 16:43:11,930 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 631.4 MB)
2018-02-05 16:43:11,948 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1812.0 B, free 631.4 MB)
2018-02-05 16:43:11,949 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:51054 (size: 1812.0 B, free: 631.8 MB)
2018-02-05 16:43:11,950 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:43:11,951 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:49) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:43:11,951 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 2 tasks
2018-02-05 16:43:11,953 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 4, 192.168.11.25, executor 0, partition 0, NODE_LOCAL, 4625 bytes)
2018-02-05 16:43:11,954 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 5, 192.168.11.25, executor 0, partition 1, NODE_LOCAL, 4625 bytes)
2018-02-05 16:43:11,965 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:51054 in memory (size: 2.2 KB, free: 631.8 MB)
2018-02-05 16:43:11,966 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.25:44736 in memory (size: 2.2 KB, free: 363.6 MB)
2018-02-05 16:43:11,968 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.25:44736 (size: 1812.0 B, free: 363.6 MB)
2018-02-05 16:43:11,986 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - Asked to send map output locations for shuffle 0 to 192.168.11.25:36425
2018-02-05 16:43:11,989 INFO[org.apache.spark.MapOutputTrackerMaster:54] - Size of output statuses for shuffle 0 is 153 bytes
2018-02-05 16:43:12,029 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 4) in 78 ms on 192.168.11.25 (executor 0) (1/2)
2018-02-05 16:43:12,031 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 5) in 77 ms on 192.168.11.25 (executor 0) (2/2)
2018-02-05 16:43:12,031 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-05 16:43:12,032 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (countByKey at SparkWordCount.java:49) finished in 0.081 s
2018-02-05 16:43:12,033 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: countByKey at SparkWordCount.java:49, took 0.447213 s
2018-02-05 16:43:12,048 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-05 16:43:12,054 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@2d358d99{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 16:43:12,056 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-05 16:43:12,062 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Shutting down all executors
2018-02-05 16:43:12,063 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Asking each executor to shut down
2018-02-05 16:43:12,073 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-05 16:43:12,089 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-05 16:43:12,090 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-05 16:43:12,092 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-05 16:43:12,094 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-05 16:43:12,097 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-05 16:43:12,098 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-05 16:43:12,099 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-5f143871-4155-4949-b1c7-a31acb4d55ff
2018-02-05 16:47:24,856 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 16:47:25,784 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 16:47:25,824 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 16:47:25,825 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 16:47:25,829 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 16:47:25,830 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 16:47:25,831 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 16:47:26,311 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 51228.
2018-02-05 16:47:26,331 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 16:47:26,382 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 16:47:26,384 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 16:47:26,385 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 16:47:26,394 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-5136e9d6-a19a-4e45-bc22-425b7915c275
2018-02-05 16:47:26,419 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 16:47:26,459 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 16:47:26,540 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2670ms
2018-02-05 16:47:26,619 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 16:47:26,642 INFO[org.spark_project.jetty.server.Server:403] - Started @2772ms
2018-02-05 16:47:26,664 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@57748eaf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 16:47:26,665 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 16:47:26,690 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c80e49b{/jobs,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,691 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55e8ec2f{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,692 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d24ffa1{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,693 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@878452d{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,694 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c51bb7{/stages,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,695 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42a9e5d1{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,695 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@773cbf4f{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,697 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f603e89{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,699 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@350ec41e{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,700 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71984c3{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,701 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5536379e{/storage,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,702 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6c8bca63{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,703 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,703 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,704 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f1c5af9{/environment,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,705 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,706 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/executors,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,707 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,707 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,708 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,716 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@38f57b3d{/static,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,717 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,719 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/api,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,720 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,721 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ddae9b5{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 16:47:26,723 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 16:47:26,819 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Connecting to master spark://192.168.11.25:7077...
2018-02-05 16:47:26,871 INFO[org.apache.spark.network.client.TransportClientFactory:254] - Successfully created connection to /192.168.11.25:7077 after 39 ms (0 ms spent in bootstraps)
2018-02-05 16:47:26,965 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Connected to Spark cluster with app ID app-20180205164817-0011
2018-02-05 16:47:26,966 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor added: app-20180205164817-0011/0 on worker-20180205151248-192.168.11.25-42981 (192.168.11.25:42981) with 4 cores
2018-02-05 16:47:26,967 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Granted executor ID app-20180205164817-0011/0 on hostPort 192.168.11.25:42981 with 4 cores, 1024.0 MB RAM
2018-02-05 16:47:26,979 INFO[org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint:54] - Executor updated: app-20180205164817-0011/0 is now RUNNING
2018-02-05 16:47:26,986 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51251.
2018-02-05 16:47:26,987 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:51251
2018-02-05 16:47:26,989 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 16:47:26,990 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 51251, None)
2018-02-05 16:47:26,993 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:51251 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 51251, None)
2018-02-05 16:47:26,996 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 51251, None)
2018-02-05 16:47:26,996 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 51251, None)
2018-02-05 16:47:27,176 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7dd712e8{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 16:47:27,191 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2018-02-05 16:47:27,200 INFO[org.apache.spark.SparkContext:54] - Added JAR hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar at hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1517820447199
2018-02-05 16:47:28,084 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 316.2 KB, free 631.5 MB)
2018-02-05 16:47:28,146 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 631.5 MB)
2018-02-05 16:47:28,150 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:51251 (size: 27.2 KB, free: 631.8 MB)
2018-02-05 16:47:28,155 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from textFile at SparkWordCount.java:29
2018-02-05 16:47:28,256 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.11.25:38335) with ID 0
2018-02-05 16:47:28,288 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.25:36976 with 366.3 MB RAM, BlockManagerId(0, 192.168.11.25, 36976, None)
2018-02-05 16:47:35,650 INFO[org.apache.hadoop.mapred.FileInputFormat:256] - Total input files to process : 1
2018-02-05 16:47:35,709 INFO[org.apache.spark.SparkContext:54] - Starting job: collect at SparkWordCount.java:49
2018-02-05 16:47:35,720 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (collect at SparkWordCount.java:49) with 2 output partitions
2018-02-05 16:47:35,721 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (collect at SparkWordCount.java:49)
2018-02-05 16:47:35,721 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-05 16:47:35,722 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-05 16:47:35,731 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:33), which has no missing parents
2018-02-05 16:47:35,761 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 4.6 KB, free 631.5 MB)
2018-02-05 16:47:35,770 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 631.5 MB)
2018-02-05 16:47:35,771 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:51251 (size: 2.6 KB, free: 631.8 MB)
2018-02-05 16:47:35,772 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:47:35,786 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:33) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:47:35,787 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 2 tasks
2018-02-05 16:47:35,834 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 16:47:35,837 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, 192.168.11.25, executor 0, partition 1, ANY, 4889 bytes)
2018-02-05 16:47:36,548 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.25:36976 (size: 2.6 KB, free: 366.3 MB)
2018-02-05 16:47:36,688 WARN[org.apache.spark.scheduler.TaskSetManager:66] - Lost task 1.0 in stage 0.0 (TID 1, 192.168.11.25, executor 0): java.io.InvalidClassException: com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1; local class incompatible: stream classdesc serialVersionUID = -9144893213477040661, local class serialVersionUID = 4117003407220635035
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:80)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

2018-02-05 16:47:36,690 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 0.0 in stage 0.0 (TID 0) on 192.168.11.25, executor 0: java.io.InvalidClassException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1; local class incompatible: stream classdesc serialVersionUID = -9144893213477040661, local class serialVersionUID = 4117003407220635035) [duplicate 1]
2018-02-05 16:47:36,691 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.1 in stage 0.0 (TID 2, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 16:47:36,692 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.1 in stage 0.0 (TID 3, 192.168.11.25, executor 0, partition 1, ANY, 4889 bytes)
2018-02-05 16:47:36,715 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 0.1 in stage 0.0 (TID 2) on 192.168.11.25, executor 0: java.io.InvalidClassException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1; local class incompatible: stream classdesc serialVersionUID = -9144893213477040661, local class serialVersionUID = 4117003407220635035) [duplicate 2]
2018-02-05 16:47:36,716 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.2 in stage 0.0 (TID 4, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 16:47:36,717 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 1.1 in stage 0.0 (TID 3) on 192.168.11.25, executor 0: java.io.InvalidClassException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1; local class incompatible: stream classdesc serialVersionUID = -9144893213477040661, local class serialVersionUID = 4117003407220635035) [duplicate 3]
2018-02-05 16:47:36,717 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.2 in stage 0.0 (TID 5, 192.168.11.25, executor 0, partition 1, ANY, 4889 bytes)
2018-02-05 16:47:36,729 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 0.2 in stage 0.0 (TID 4) on 192.168.11.25, executor 0: java.io.InvalidClassException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1; local class incompatible: stream classdesc serialVersionUID = -9144893213477040661, local class serialVersionUID = 4117003407220635035) [duplicate 4]
2018-02-05 16:47:36,732 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.3 in stage 0.0 (TID 6, 192.168.11.25, executor 0, partition 0, ANY, 4889 bytes)
2018-02-05 16:47:36,735 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 1.2 in stage 0.0 (TID 5) on 192.168.11.25, executor 0: java.io.InvalidClassException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1; local class incompatible: stream classdesc serialVersionUID = -9144893213477040661, local class serialVersionUID = 4117003407220635035) [duplicate 5]
2018-02-05 16:47:36,736 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.3 in stage 0.0 (TID 7, 192.168.11.25, executor 0, partition 1, ANY, 4889 bytes)
2018-02-05 16:47:36,748 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 1.3 in stage 0.0 (TID 7) on 192.168.11.25, executor 0: java.io.InvalidClassException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1; local class incompatible: stream classdesc serialVersionUID = -9144893213477040661, local class serialVersionUID = 4117003407220635035) [duplicate 6]
2018-02-05 16:47:36,749 ERROR[org.apache.spark.scheduler.TaskSetManager:70] - Task 1 in stage 0.0 failed 4 times; aborting job
2018-02-05 16:47:36,754 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Lost task 0.3 in stage 0.0 (TID 6) on 192.168.11.25, executor 0: java.io.InvalidClassException (com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1; local class incompatible: stream classdesc serialVersionUID = -9144893213477040661, local class serialVersionUID = 4117003407220635035) [duplicate 7]
2018-02-05 16:47:36,755 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 16:47:36,756 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Cancelling stage 0
2018-02-05 16:47:36,757 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (collect at SparkWordCount.java:49) failed in 0.950 s due to Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, 192.168.11.25, executor 0): java.io.InvalidClassException: com.lovecws.mumu.spark.rdd.wordcount.SparkWordCount$1; local class incompatible: stream classdesc serialVersionUID = -9144893213477040661, local class serialVersionUID = 4117003407220635035
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:80)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
2018-02-05 16:47:36,762 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 failed: collect at SparkWordCount.java:49, took 1.052668 s
2018-02-05 16:47:36,784 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-05 16:47:36,791 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@57748eaf{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 16:47:36,794 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-05 16:47:36,799 INFO[org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend:54] - Shutting down all executors
2018-02-05 16:47:36,799 INFO[org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint:54] - Asking each executor to shut down
2018-02-05 16:47:36,811 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-05 16:47:36,826 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-05 16:47:36,827 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-05 16:47:36,831 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-05 16:47:36,833 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-05 16:47:36,836 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-05 16:47:36,837 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-05 16:47:36,838 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-587075b1-153f-4a87-ba02-0e847de5a1f1
2018-02-05 16:48:31,240 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 16:48:31,979 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 16:48:32,019 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 16:48:32,020 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 16:48:32,021 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 16:48:32,022 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 16:48:32,024 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 16:48:32,504 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 51316.
2018-02-05 16:48:32,522 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 16:48:32,570 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 16:48:32,573 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 16:48:32,574 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 16:48:32,583 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-372e5b83-7b3f-49cb-82cb-b26c0ab79b89
2018-02-05 16:48:32,607 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 16:48:32,665 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 16:48:32,750 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2528ms
2018-02-05 16:48:32,823 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 16:48:32,846 INFO[org.spark_project.jetty.server.Server:403] - Started @2625ms
2018-02-05 16:48:32,866 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@3abfc4ed{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 16:48:32,866 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 16:48:32,893 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1c80e49b{/jobs,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,895 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@55e8ec2f{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,896 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6d24ffa1{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,897 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@878452d{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,898 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c51bb7{/stages,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,898 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42a9e5d1{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,900 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@773cbf4f{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,902 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f603e89{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,903 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@350ec41e{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,906 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71984c3{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,907 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5536379e{/storage,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,908 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6c8bca63{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,909 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@470a696f{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,911 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@292d1c71{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,916 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f1c5af9{/environment,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,921 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,922 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@63192798{/executors,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,922 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58e6d4b8{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,923 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@376a312c{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,924 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6ca0256d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,932 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@38f57b3d{/static,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,933 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4ba534b0{/,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,934 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2c104774{/api,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,935 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1d7f7be7{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,935 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1ddae9b5{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 16:48:32,937 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 16:48:33,055 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-05 16:48:33,087 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51329.
2018-02-05 16:48:33,088 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:51329
2018-02-05 16:48:33,089 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 16:48:33,091 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 51329, None)
2018-02-05 16:48:33,097 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:51329 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 51329, None)
2018-02-05 16:48:33,101 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 51329, None)
2018-02-05 16:48:33,102 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 51329, None)
2018-02-05 16:48:33,288 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@53bc1328{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 16:48:33,309 INFO[org.apache.spark.SparkContext:54] - Added JAR hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar at hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1517820513308
2018-02-05 16:48:34,081 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 316.2 KB, free 631.5 MB)
2018-02-05 16:48:34,137 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.2 KB, free 631.5 MB)
2018-02-05 16:48:34,140 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:51329 (size: 27.2 KB, free: 631.8 MB)
2018-02-05 16:48:34,146 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from textFile at SparkWordCount.java:29
2018-02-05 16:48:41,675 INFO[org.apache.hadoop.mapred.FileInputFormat:256] - Total input files to process : 1
2018-02-05 16:48:41,738 INFO[org.apache.spark.SparkContext:54] - Starting job: collect at SparkWordCount.java:49
2018-02-05 16:48:41,750 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (collect at SparkWordCount.java:49) with 2 output partitions
2018-02-05 16:48:41,750 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (collect at SparkWordCount.java:49)
2018-02-05 16:48:41,750 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-05 16:48:41,752 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-05 16:48:41,758 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:33), which has no missing parents
2018-02-05 16:48:41,774 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 4.6 KB, free 631.5 MB)
2018-02-05 16:48:41,781 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.6 KB, free 631.5 MB)
2018-02-05 16:48:41,782 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:51329 (size: 2.6 KB, free: 631.8 MB)
2018-02-05 16:48:41,782 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:48:41,792 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at flatMapToPair at SparkWordCount.java:33) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:48:41,793 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 2 tasks
2018-02-05 16:48:41,835 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 4885 bytes)
2018-02-05 16:48:41,837 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 4885 bytes)
2018-02-05 16:48:41,842 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-05 16:48:41,844 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 0.0 (TID 1)
2018-02-05 16:48:41,848 INFO[org.apache.spark.executor.Executor:54] - Fetching hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar with timestamp 1517820513308
2018-02-05 16:48:41,890 INFO[org.apache.spark.util.Utils:54] - Fetching hdfs://192.168.11.25:9000/mumu/spark/jar/mumu-spark.jar to C:\Users\Administrator\AppData\Local\Temp\spark-aa5bf95c-da18-4453-9701-9b5ced9b36d4\userFiles-ed2bf9df-4ee1-4c9b-ac69-5d0acf3e5849\fetchFileTemp8475097004379340434.tmp
2018-02-05 16:48:42,686 INFO[org.apache.spark.executor.Executor:54] - Adding file:/C:/Users/Administrator/AppData/Local/Temp/spark-aa5bf95c-da18-4453-9701-9b5ced9b36d4/userFiles-ed2bf9df-4ee1-4c9b-ac69-5d0acf3e5849/mumu-spark.jar to class loader
2018-02-05 16:48:42,737 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171207:169984+169985
2018-02-05 16:48:42,737 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171207:0+169984
2018-02-05 16:48:42,920 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 371977 bytes result sent to driver
2018-02-05 16:48:42,947 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 0.0 (TID 1). 388999 bytes result sent to driver
2018-02-05 16:48:42,969 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 1146 ms on localhost (executor driver) (1/2)
2018-02-05 16:48:42,989 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 1153 ms on localhost (executor driver) (2/2)
2018-02-05 16:48:42,992 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 16:48:42,995 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (collect at SparkWordCount.java:49) finished in 1.183 s
2018-02-05 16:48:43,001 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: collect at SparkWordCount.java:49, took 1.262396 s
2018-02-05 16:48:43,098 INFO[org.apache.spark.SparkContext:54] - Starting job: countByKey at SparkWordCount.java:53
2018-02-05 16:48:43,108 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (countByKey at SparkWordCount.java:53)
2018-02-05 16:48:43,109 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (countByKey at SparkWordCount.java:53) with 2 output partitions
2018-02-05 16:48:43,109 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 2 (countByKey at SparkWordCount.java:53)
2018-02-05 16:48:43,109 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 1)
2018-02-05 16:48:43,109 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 1)
2018-02-05 16:48:43,118 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:53), which has no missing parents
2018-02-05 16:48:43,126 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 5.7 KB, free 631.5 MB)
2018-02-05 16:48:43,134 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.1 KB, free 631.4 MB)
2018-02-05 16:48:43,136 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:51329 (size: 3.1 KB, free: 631.8 MB)
2018-02-05 16:48:43,137 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:48:43,141 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[3] at countByKey at SparkWordCount.java:53) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:48:43,141 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 2 tasks
2018-02-05 16:48:43,143 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4874 bytes)
2018-02-05 16:48:43,144 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 4874 bytes)
2018-02-05 16:48:43,144 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 2)
2018-02-05 16:48:43,187 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 1.0 (TID 3)
2018-02-05 16:48:43,201 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171207:0+169984
2018-02-05 16:48:43,210 INFO[org.apache.spark.rdd.HadoopRDD:54] - Input split: hdfs://192.168.11.25:9000/mapreduce/nginxlog/access/input/20171207:169984+169985
2018-02-05 16:48:43,329 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Removed broadcast_1_piece0 on 192.168.11.26:51329 in memory (size: 2.6 KB, free: 631.8 MB)
2018-02-05 16:48:43,424 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block rdd_2_1 stored as values in memory (estimated size 1428.2 KB, free 630.1 MB)
2018-02-05 16:48:43,425 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_2_1 in memory on 192.168.11.26:51329 (size: 1428.2 KB, free: 630.4 MB)
2018-02-05 16:48:43,463 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block rdd_2_0 stored as values in memory (estimated size 1317.5 KB, free 628.8 MB)
2018-02-05 16:48:43,464 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added rdd_2_0 in memory on 192.168.11.26:51329 (size: 1317.5 KB, free: 629.1 MB)
2018-02-05 16:48:43,584 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 1.0 (TID 3). 2135 bytes result sent to driver
2018-02-05 16:48:43,584 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 2). 2178 bytes result sent to driver
2018-02-05 16:48:43,587 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 2) in 445 ms on localhost (executor driver) (1/2)
2018-02-05 16:48:43,588 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 1.0 (TID 3) in 444 ms on localhost (executor driver) (2/2)
2018-02-05 16:48:43,589 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-05 16:48:43,589 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 1 (countByKey at SparkWordCount.java:53) finished in 0.448 s
2018-02-05 16:48:43,590 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-05 16:48:43,590 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-05 16:48:43,591 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 2)
2018-02-05 16:48:43,591 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-05 16:48:43,597 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:53), which has no missing parents
2018-02-05 16:48:43,602 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 3.1 KB, free 628.8 MB)
2018-02-05 16:48:43,606 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 1812.0 B, free 628.8 MB)
2018-02-05 16:48:43,607 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:51329 (size: 1812.0 B, free: 629.1 MB)
2018-02-05 16:48:43,609 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-05 16:48:43,610 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ResultStage 2 (ShuffledRDD[4] at countByKey at SparkWordCount.java:53) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 16:48:43,610 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 2 tasks
2018-02-05 16:48:43,612 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 4621 bytes)
2018-02-05 16:48:43,613 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 4621 bytes)
2018-02-05 16:48:43,613 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 4)
2018-02-05 16:48:43,613 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 2.0 (TID 5)
2018-02-05 16:48:43,650 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2018-02-05 16:48:43,650 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2018-02-05 16:48:43,652 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
2018-02-05 16:48:43,652 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 7 ms
2018-02-05 16:48:43,702 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 2.0 (TID 5). 24433 bytes result sent to driver
2018-02-05 16:48:43,703 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 4). 24103 bytes result sent to driver
2018-02-05 16:48:43,704 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 4) in 93 ms on localhost (executor driver) (1/2)
2018-02-05 16:48:43,705 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 5) in 92 ms on localhost (executor driver) (2/2)
2018-02-05 16:48:43,705 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-05 16:48:43,706 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 2 (countByKey at SparkWordCount.java:53) finished in 0.096 s
2018-02-05 16:48:43,707 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: countByKey at SparkWordCount.java:53, took 0.609001 s
2018-02-05 16:48:43,727 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-05 16:48:43,735 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@3abfc4ed{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 16:48:43,737 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-05 16:48:43,744 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-05 16:48:43,783 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-05 16:48:43,784 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-05 16:48:43,785 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-05 16:48:43,787 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-05 16:48:43,791 WARN[org.apache.spark.SparkEnv:87] - Exception while deleting Spark temp dir: C:\Users\Administrator\AppData\Local\Temp\spark-aa5bf95c-da18-4453-9701-9b5ced9b36d4\userFiles-ed2bf9df-4ee1-4c9b-ac69-5d0acf3e5849
java.io.IOException: Failed to delete: C:\Users\Administrator\AppData\Local\Temp\spark-aa5bf95c-da18-4453-9701-9b5ced9b36d4\userFiles-ed2bf9df-4ee1-4c9b-ac69-5d0acf3e5849
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1937)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1936)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:581)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
2018-02-05 16:48:43,793 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-05 16:48:43,794 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-05 16:48:43,795 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-aa5bf95c-da18-4453-9701-9b5ced9b36d4\userFiles-ed2bf9df-4ee1-4c9b-ac69-5d0acf3e5849
2018-02-05 16:48:43,797 ERROR[org.apache.spark.util.ShutdownHookManager:91] - Exception while deleting Spark temp dir: C:\Users\Administrator\AppData\Local\Temp\spark-aa5bf95c-da18-4453-9701-9b5ced9b36d4\userFiles-ed2bf9df-4ee1-4c9b-ac69-5d0acf3e5849
java.io.IOException: Failed to delete: C:\Users\Administrator\AppData\Local\Temp\spark-aa5bf95c-da18-4453-9701-9b5ced9b36d4\userFiles-ed2bf9df-4ee1-4c9b-ac69-5d0acf3e5849
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
2018-02-05 16:48:43,798 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-aa5bf95c-da18-4453-9701-9b5ced9b36d4
2018-02-05 16:48:43,801 ERROR[org.apache.spark.util.ShutdownHookManager:91] - Exception while deleting Spark temp dir: C:\Users\Administrator\AppData\Local\Temp\spark-aa5bf95c-da18-4453-9701-9b5ced9b36d4
java.io.IOException: Failed to delete: C:\Users\Administrator\AppData\Local\Temp\spark-aa5bf95c-da18-4453-9701-9b5ced9b36d4
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1031)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1954)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
2018-02-05 17:01:20,471 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 17:01:21,020 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 17:01:21,055 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 17:01:21,056 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 17:01:21,057 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 17:01:21,058 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 17:01:21,059 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 17:01:21,491 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 51942.
2018-02-05 17:01:21,509 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 17:01:21,559 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 17:01:21,562 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 17:01:21,562 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 17:01:21,573 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-3730755f-ceab-4ec6-b79a-809e9964706e
2018-02-05 17:01:21,595 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 17:01:21,652 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 17:01:21,734 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2675ms
2018-02-05 17:01:21,805 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 17:01:21,821 INFO[org.spark_project.jetty.server.Server:403] - Started @2763ms
2018-02-05 17:01:21,844 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@4b3c354a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 17:01:21,844 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 17:01:21,864 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c51bb7{/jobs,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,865 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,865 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,866 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,867 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/stages,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,867 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,868 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,869 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64df9a61{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,869 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e260766{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,870 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a97744{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,870 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c56e013{/storage,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,871 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40258c2f{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,871 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6731787b{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,872 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,873 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/environment,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,874 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,875 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/executors,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,876 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,876 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,877 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5860f3d7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,884 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/static,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,885 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@59d2103b{/,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,886 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e4de19b{/api,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,887 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c6daf0{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,887 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@659eef7{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 17:01:21,889 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 17:01:21,970 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-05 17:01:21,993 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 51955.
2018-02-05 17:01:21,994 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:51955
2018-02-05 17:01:21,996 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 17:01:21,999 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 51955, None)
2018-02-05 17:01:22,002 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:51955 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 51955, None)
2018-02-05 17:01:22,004 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 51955, None)
2018-02-05 17:01:22,004 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 51955, None)
2018-02-05 17:01:22,199 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@df5f5c0{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 17:01:22,303 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-05 17:01:22,304 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-05 17:01:22,312 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@88a8218{/SQL,null,AVAILABLE,@Spark}
2018-02-05 17:01:22,314 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4163f1cd{/SQL/json,null,AVAILABLE,@Spark}
2018-02-05 17:01:22,315 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5f574cc2{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-05 17:01:22,316 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a9c84a5{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-05 17:01:22,318 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@26f1249d{/static/sql,null,AVAILABLE,@Spark}
2018-02-05 17:01:23,509 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-05 17:01:23,987 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 218.76647 ms
2018-02-05 17:01:25,618 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 10.984964 ms
2018-02-05 17:01:25,649 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 17.282246 ms
2018-02-05 17:01:25,678 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-05 17:01:25,683 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@4b3c354a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 17:01:25,685 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-05 17:01:25,692 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-05 17:01:25,700 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-05 17:01:25,701 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-05 17:01:25,705 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-05 17:01:25,708 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-05 17:01:25,711 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-05 17:01:25,711 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-05 17:01:25,712 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-519e6484-b6dc-441d-8c0e-90dd1d40837d
2018-02-05 17:05:09,822 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 17:05:10,399 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 17:05:10,425 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 17:05:10,425 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 17:05:10,426 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 17:05:10,426 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 17:05:10,427 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 17:05:10,783 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 52034.
2018-02-05 17:05:10,801 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 17:05:10,852 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 17:05:10,855 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 17:05:10,856 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 17:05:10,866 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-bfd2d814-9536-4407-a1d9-bcd956b44348
2018-02-05 17:05:10,888 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 17:05:10,933 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 17:05:11,009 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2710ms
2018-02-05 17:05:11,077 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 17:05:11,092 INFO[org.spark_project.jetty.server.Server:403] - Started @2794ms
2018-02-05 17:05:11,113 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@6b7935d3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 17:05:11,113 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 17:05:11,134 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c51bb7{/jobs,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,135 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,136 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,137 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,138 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/stages,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,138 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,139 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,141 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64df9a61{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,141 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e260766{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,142 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a97744{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,143 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c56e013{/storage,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,144 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40258c2f{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,145 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6731787b{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,146 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,146 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/environment,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,147 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,148 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/executors,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,149 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,150 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,152 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5860f3d7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,158 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/static,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,159 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@59d2103b{/,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,161 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e4de19b{/api,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,161 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c6daf0{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,162 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@659eef7{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,165 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 17:05:11,252 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-05 17:05:11,275 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52047.
2018-02-05 17:05:11,276 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:52047
2018-02-05 17:05:11,277 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 17:05:11,278 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 52047, None)
2018-02-05 17:05:11,281 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:52047 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 52047, None)
2018-02-05 17:05:11,283 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 52047, None)
2018-02-05 17:05:11,284 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 52047, None)
2018-02-05 17:05:11,443 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@df5f5c0{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,504 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-05 17:05:11,505 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-05 17:05:11,510 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@88a8218{/SQL,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,511 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4163f1cd{/SQL/json,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,512 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5f574cc2{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,512 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a9c84a5{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-05 17:05:11,515 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@26f1249d{/static/sql,null,AVAILABLE,@Spark}
2018-02-05 17:05:12,573 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-05 17:05:14,398 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 180.747578 ms
2018-02-05 17:05:14,439 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 23.397447 ms
2018-02-05 17:05:14,459 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: person
2018-02-05 17:05:14,621 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select *from person
2018-02-05 17:05:14,770 INFO[org.apache.spark.sql.execution.SparkSqlParser:54] - Parsing command: select max(age) from person
2018-02-05 17:05:15,241 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 19.954566 ms
2018-02-05 17:05:15,256 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 11.248644 ms
2018-02-05 17:05:15,265 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 8.165763 ms
2018-02-05 17:05:15,393 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 0
2018-02-05 17:05:15,394 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 2
2018-02-05 17:05:15,394 INFO[org.apache.spark.ContextCleaner:54] - Cleaned accumulator 1
2018-02-05 17:05:15,516 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:69
2018-02-05 17:05:15,538 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 3 (show at SchemaRDDSparkSQL.java:69)
2018-02-05 17:05:15,540 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (show at SchemaRDDSparkSQL.java:69) with 1 output partitions
2018-02-05 17:05:15,540 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at SchemaRDDSparkSQL.java:69)
2018-02-05 17:05:15,540 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 0)
2018-02-05 17:05:15,542 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 0)
2018-02-05 17:05:15,546 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at SchemaRDDSparkSQL.java:69), which has no missing parents
2018-02-05 17:05:15,697 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 8.1 KB, free 631.8 MB)
2018-02-05 17:05:15,738 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.3 KB, free 631.8 MB)
2018-02-05 17:05:15,742 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:52047 (size: 4.3 KB, free: 631.8 MB)
2018-02-05 17:05:15,744 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-05 17:05:15,756 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at SchemaRDDSparkSQL.java:69) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 17:05:15,757 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 2 tasks
2018-02-05 17:05:15,816 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 4992 bytes)
2018-02-05 17:05:15,822 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 5025 bytes)
2018-02-05 17:05:15,828 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-05 17:05:15,828 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 0.0 (TID 1)
2018-02-05 17:05:15,985 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 0.0 (TID 1). 1546 bytes result sent to driver
2018-02-05 17:05:15,985 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1546 bytes result sent to driver
2018-02-05 17:05:15,994 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 204 ms on localhost (executor driver) (1/2)
2018-02-05 17:05:15,996 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 0.0 (TID 1) in 176 ms on localhost (executor driver) (2/2)
2018-02-05 17:05:15,999 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 17:05:16,006 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 0 (show at SchemaRDDSparkSQL.java:69) finished in 0.229 s
2018-02-05 17:05:16,006 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-05 17:05:16,007 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-05 17:05:16,007 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 1)
2018-02-05 17:05:16,008 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-05 17:05:16,012 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[6] at show at SchemaRDDSparkSQL.java:69), which has no missing parents
2018-02-05 17:05:16,023 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 7.1 KB, free 631.8 MB)
2018-02-05 17:05:16,030 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.8 KB, free 631.8 MB)
2018-02-05 17:05:16,031 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:52047 (size: 3.8 KB, free: 631.8 MB)
2018-02-05 17:05:16,032 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-05 17:05:16,035 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at show at SchemaRDDSparkSQL.java:69) (first 15 tasks are for partitions Vector(0))
2018-02-05 17:05:16,035 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-05 17:05:16,041 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 4726 bytes)
2018-02-05 17:05:16,043 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 2)
2018-02-05 17:05:16,057 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2018-02-05 17:05:16,059 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 4 ms
2018-02-05 17:05:16,078 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 2). 1514 bytes result sent to driver
2018-02-05 17:05:16,079 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 2) in 40 ms on localhost (executor driver) (1/1)
2018-02-05 17:05:16,079 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-05 17:05:16,080 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at SchemaRDDSparkSQL.java:69) finished in 0.042 s
2018-02-05 17:05:16,086 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: show at SchemaRDDSparkSQL.java:69, took 0.568714 s
2018-02-05 17:05:16,101 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 9.434883 ms
2018-02-05 17:05:16,150 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 12.483524 ms
2018-02-05 17:05:16,164 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 9.984003 ms
2018-02-05 17:05:16,228 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:72
2018-02-05 17:05:16,229 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Registering RDD 10 (show at SchemaRDDSparkSQL.java:72)
2018-02-05 17:05:16,230 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at SchemaRDDSparkSQL.java:72) with 1 output partitions
2018-02-05 17:05:16,230 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 3 (show at SchemaRDDSparkSQL.java:72)
2018-02-05 17:05:16,230 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List(ShuffleMapStage 2)
2018-02-05 17:05:16,230 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List(ShuffleMapStage 2)
2018-02-05 17:05:16,230 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at show at SchemaRDDSparkSQL.java:72), which has no missing parents
2018-02-05 17:05:16,240 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2 stored as values in memory (estimated size 8.1 KB, free 631.8 MB)
2018-02-05 17:05:16,241 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.3 KB, free 631.8 MB)
2018-02-05 17:05:16,245 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_2_piece0 in memory on 192.168.11.26:52047 (size: 4.3 KB, free: 631.8 MB)
2018-02-05 17:05:16,246 INFO[org.apache.spark.SparkContext:54] - Created broadcast 2 from broadcast at DAGScheduler.scala:1006
2018-02-05 17:05:16,246 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at show at SchemaRDDSparkSQL.java:72) (first 15 tasks are for partitions Vector(0, 1))
2018-02-05 17:05:16,247 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 2.0 with 2 tasks
2018-02-05 17:05:16,248 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 2.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 4992 bytes)
2018-02-05 17:05:16,248 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 1.0 in stage 2.0 (TID 4, localhost, executor driver, partition 1, PROCESS_LOCAL, 5025 bytes)
2018-02-05 17:05:16,248 INFO[org.apache.spark.executor.Executor:54] - Running task 1.0 in stage 2.0 (TID 4)
2018-02-05 17:05:16,248 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 2.0 (TID 3)
2018-02-05 17:05:16,273 INFO[org.apache.spark.executor.Executor:54] - Finished task 1.0 in stage 2.0 (TID 4). 1460 bytes result sent to driver
2018-02-05 17:05:16,280 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 1.0 in stage 2.0 (TID 4) in 32 ms on localhost (executor driver) (1/2)
2018-02-05 17:05:16,280 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 2.0 (TID 3). 1460 bytes result sent to driver
2018-02-05 17:05:16,282 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 2.0 (TID 3) in 34 ms on localhost (executor driver) (2/2)
2018-02-05 17:05:16,282 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 2.0, whose tasks have all completed, from pool 
2018-02-05 17:05:16,282 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ShuffleMapStage 2 (show at SchemaRDDSparkSQL.java:72) finished in 0.035 s
2018-02-05 17:05:16,282 INFO[org.apache.spark.scheduler.DAGScheduler:54] - looking for newly runnable stages
2018-02-05 17:05:16,282 INFO[org.apache.spark.scheduler.DAGScheduler:54] - running: Set()
2018-02-05 17:05:16,282 INFO[org.apache.spark.scheduler.DAGScheduler:54] - waiting: Set(ResultStage 3)
2018-02-05 17:05:16,283 INFO[org.apache.spark.scheduler.DAGScheduler:54] - failed: Set()
2018-02-05 17:05:16,283 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 3 (MapPartitionsRDD[13] at show at SchemaRDDSparkSQL.java:72), which has no missing parents
2018-02-05 17:05:16,286 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3 stored as values in memory (estimated size 7.1 KB, free 631.8 MB)
2018-02-05 17:05:16,288 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.8 KB, free 631.8 MB)
2018-02-05 17:05:16,289 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_3_piece0 in memory on 192.168.11.26:52047 (size: 3.8 KB, free: 631.8 MB)
2018-02-05 17:05:16,289 INFO[org.apache.spark.SparkContext:54] - Created broadcast 3 from broadcast at DAGScheduler.scala:1006
2018-02-05 17:05:16,290 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at show at SchemaRDDSparkSQL.java:72) (first 15 tasks are for partitions Vector(0))
2018-02-05 17:05:16,290 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 3.0 with 1 tasks
2018-02-05 17:05:16,291 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 3.0 (TID 5, localhost, executor driver, partition 0, ANY, 4726 bytes)
2018-02-05 17:05:16,291 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 3.0 (TID 5)
2018-02-05 17:05:16,295 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Getting 2 non-empty blocks out of 2 blocks
2018-02-05 17:05:16,295 INFO[org.apache.spark.storage.ShuffleBlockFetcherIterator:54] - Started 0 remote fetches in 0 ms
2018-02-05 17:05:16,297 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 3.0 (TID 5). 1557 bytes result sent to driver
2018-02-05 17:05:16,298 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 3.0 (TID 5) in 7 ms on localhost (executor driver) (1/1)
2018-02-05 17:05:16,298 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2018-02-05 17:05:16,298 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 3 (show at SchemaRDDSparkSQL.java:72) finished in 0.008 s
2018-02-05 17:05:16,299 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at SchemaRDDSparkSQL.java:72, took 0.071494 s
2018-02-05 17:05:16,307 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-05 17:05:16,312 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@6b7935d3{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 17:05:16,314 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-05 17:05:16,322 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-05 17:05:16,376 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-05 17:05:16,377 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-05 17:05:16,382 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-05 17:05:16,384 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-05 17:05:16,387 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-05 17:05:16,388 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-05 17:05:16,390 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-ae59ca20-c68d-4a61-b75a-cd89fca48871
2018-02-05 17:08:19,947 INFO[org.apache.spark.SparkContext:54] - Running Spark version 2.2.0
2018-02-05 17:08:20,686 INFO[org.apache.spark.SparkContext:54] - Submitted application: mumuSpark
2018-02-05 17:08:20,711 INFO[org.apache.spark.SecurityManager:54] - Changing view acls to: Administrator
2018-02-05 17:08:20,711 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls to: Administrator
2018-02-05 17:08:20,712 INFO[org.apache.spark.SecurityManager:54] - Changing view acls groups to: 
2018-02-05 17:08:20,713 INFO[org.apache.spark.SecurityManager:54] - Changing modify acls groups to: 
2018-02-05 17:08:20,713 INFO[org.apache.spark.SecurityManager:54] - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Administrator); groups with view permissions: Set(); users  with modify permissions: Set(Administrator); groups with modify permissions: Set()
2018-02-05 17:08:21,102 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'sparkDriver' on port 52100.
2018-02-05 17:08:21,122 INFO[org.apache.spark.SparkEnv:54] - Registering MapOutputTracker
2018-02-05 17:08:21,175 INFO[org.apache.spark.SparkEnv:54] - Registering BlockManagerMaster
2018-02-05 17:08:21,178 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-02-05 17:08:21,179 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - BlockManagerMasterEndpoint up
2018-02-05 17:08:21,190 INFO[org.apache.spark.storage.DiskBlockManager:54] - Created local directory at C:\Users\Administrator\AppData\Local\Temp\blockmgr-243f2e7b-a642-4172-9b3b-9efc37e14b62
2018-02-05 17:08:21,216 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore started with capacity 631.8 MB
2018-02-05 17:08:21,266 INFO[org.apache.spark.SparkEnv:54] - Registering OutputCommitCoordinator
2018-02-05 17:08:21,351 INFO[org.spark_project.jetty.util.log:192] - Logging initialized @2675ms
2018-02-05 17:08:21,423 INFO[org.spark_project.jetty.server.Server:345] - jetty-9.3.z-SNAPSHOT
2018-02-05 17:08:21,440 INFO[org.spark_project.jetty.server.Server:403] - Started @2765ms
2018-02-05 17:08:21,462 INFO[org.spark_project.jetty.server.AbstractConnector:270] - Started ServerConnector@4b3c354a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 17:08:21,463 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'SparkUI' on port 4040.
2018-02-05 17:08:21,486 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c51bb7{/jobs,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,487 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3f1c5af9{/jobs/json,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,489 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ddabb18{/jobs/job,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,490 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@50eca7c6{/jobs/job/json,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,491 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@1de5f0ef{/stages,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,492 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@28d6290{/stages/json,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,492 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5ef0d29e{/stages/stage,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,494 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@64df9a61{/stages/stage/json,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,495 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@e260766{/stages/pool,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,496 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@34a97744{/stages/pool/json,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,497 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7c56e013{/storage,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,498 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@40258c2f{/storage/json,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,498 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6731787b{/storage/rdd,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,501 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7adf16aa{/storage/rdd/json,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,502 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@58bf8650{/environment,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,503 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@71ae31b0{/environment/json,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,503 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6f0ca692{/executors,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,504 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@2cb3d0f7{/executors/json,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,505 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@44e3760b{/executors/threadDump,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,506 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@5860f3d7{/executors/threadDump/json,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,512 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@42f3156d{/static,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,513 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@59d2103b{/,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,515 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@6e4de19b{/api,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,516 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4c6daf0{/jobs/job/kill,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,516 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@659eef7{/stages/stage/kill,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,518 INFO[org.apache.spark.ui.SparkUI:54] - Bound SparkUI to 0.0.0.0, and started at http://192.168.11.26:4040
2018-02-05 17:08:21,602 INFO[org.apache.spark.executor.Executor:54] - Starting executor ID driver on host localhost
2018-02-05 17:08:21,633 INFO[org.apache.spark.util.Utils:54] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 52113.
2018-02-05 17:08:21,634 INFO[org.apache.spark.network.netty.NettyBlockTransferService:54] - Server created on 192.168.11.26:52113
2018-02-05 17:08:21,636 INFO[org.apache.spark.storage.BlockManager:54] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-02-05 17:08:21,637 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registering BlockManager BlockManagerId(driver, 192.168.11.26, 52113, None)
2018-02-05 17:08:21,641 INFO[org.apache.spark.storage.BlockManagerMasterEndpoint:54] - Registering block manager 192.168.11.26:52113 with 631.8 MB RAM, BlockManagerId(driver, 192.168.11.26, 52113, None)
2018-02-05 17:08:21,644 INFO[org.apache.spark.storage.BlockManagerMaster:54] - Registered BlockManager BlockManagerId(driver, 192.168.11.26, 52113, None)
2018-02-05 17:08:21,645 INFO[org.apache.spark.storage.BlockManager:54] - Initialized BlockManager: BlockManagerId(driver, 192.168.11.26, 52113, None)
2018-02-05 17:08:21,841 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@df5f5c0{/metrics/json,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,910 INFO[org.apache.spark.sql.internal.SharedState:54] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/').
2018-02-05 17:08:21,911 INFO[org.apache.spark.sql.internal.SharedState:54] - Warehouse path is 'file:/E:/IntelliJWorkspaceMumu/mumu-spark/spark-warehouse/'.
2018-02-05 17:08:21,918 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@4163f1cd{/SQL,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,918 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@3e681bc{/SQL/json,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,919 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@7a9c84a5{/SQL/execution,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,920 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@48d7ad8b{/SQL/execution/json,null,AVAILABLE,@Spark}
2018-02-05 17:08:21,922 INFO[org.spark_project.jetty.server.handler.ContextHandler:781] - Started o.s.j.s.ServletContextHandler@a68df9{/static/sql,null,AVAILABLE,@Spark}
2018-02-05 17:08:22,979 INFO[org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef:54] - Registered StateStoreCoordinator endpoint
2018-02-05 17:08:24,521 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 187.95622 ms
2018-02-05 17:08:25,059 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 8.170243 ms
2018-02-05 17:08:25,204 INFO[org.apache.spark.SparkContext:54] - Starting job: json at SchemaRDDSparkSQL.java:86
2018-02-05 17:08:25,217 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 0 (json at SchemaRDDSparkSQL.java:86) with 1 output partitions
2018-02-05 17:08:25,218 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 0 (json at SchemaRDDSparkSQL.java:86)
2018-02-05 17:08:25,218 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-05 17:08:25,219 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-05 17:08:25,224 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at SchemaRDDSparkSQL.java:86), which has no missing parents
2018-02-05 17:08:25,442 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0 stored as values in memory (estimated size 5.8 KB, free 631.8 MB)
2018-02-05 17:08:25,476 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.6 KB, free 631.8 MB)
2018-02-05 17:08:25,481 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_0_piece0 in memory on 192.168.11.26:52113 (size: 3.6 KB, free: 631.8 MB)
2018-02-05 17:08:25,483 INFO[org.apache.spark.SparkContext:54] - Created broadcast 0 from broadcast at DAGScheduler.scala:1006
2018-02-05 17:08:25,497 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at SchemaRDDSparkSQL.java:86) (first 15 tasks are for partitions Vector(0))
2018-02-05 17:08:25,498 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 0.0 with 1 tasks
2018-02-05 17:08:25,551 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5067 bytes)
2018-02-05 17:08:25,566 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 0.0 (TID 0)
2018-02-05 17:08:25,663 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 0.0 (TID 0). 1754 bytes result sent to driver
2018-02-05 17:08:25,673 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 0.0 (TID 0) in 140 ms on localhost (executor driver) (1/1)
2018-02-05 17:08:25,678 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2018-02-05 17:08:25,682 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 0 (json at SchemaRDDSparkSQL.java:86) finished in 0.165 s
2018-02-05 17:08:25,693 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 0 finished: json at SchemaRDDSparkSQL.java:86, took 0.482593 s
2018-02-05 17:08:25,866 INFO[org.apache.spark.SparkContext:54] - Starting job: show at SchemaRDDSparkSQL.java:87
2018-02-05 17:08:25,870 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Got job 1 (show at SchemaRDDSparkSQL.java:87) with 1 output partitions
2018-02-05 17:08:25,870 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Final stage: ResultStage 1 (show at SchemaRDDSparkSQL.java:87)
2018-02-05 17:08:25,870 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Parents of final stage: List()
2018-02-05 17:08:25,873 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Missing parents: List()
2018-02-05 17:08:25,874 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting ResultStage 1 (MapPartitionsRDD[10] at show at SchemaRDDSparkSQL.java:87), which has no missing parents
2018-02-05 17:08:25,897 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1 stored as values in memory (estimated size 9.8 KB, free 631.8 MB)
2018-02-05 17:08:25,899 INFO[org.apache.spark.storage.memory.MemoryStore:54] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.5 KB, free 631.8 MB)
2018-02-05 17:08:25,900 INFO[org.apache.spark.storage.BlockManagerInfo:54] - Added broadcast_1_piece0 in memory on 192.168.11.26:52113 (size: 5.5 KB, free: 631.8 MB)
2018-02-05 17:08:25,901 INFO[org.apache.spark.SparkContext:54] - Created broadcast 1 from broadcast at DAGScheduler.scala:1006
2018-02-05 17:08:25,902 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at show at SchemaRDDSparkSQL.java:87) (first 15 tasks are for partitions Vector(0))
2018-02-05 17:08:25,902 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Adding task set 1.0 with 1 tasks
2018-02-05 17:08:25,903 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, PROCESS_LOCAL, 5067 bytes)
2018-02-05 17:08:25,903 INFO[org.apache.spark.executor.Executor:54] - Running task 0.0 in stage 1.0 (TID 1)
2018-02-05 17:08:25,927 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 7.155843 ms
2018-02-05 17:08:25,962 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 14.225925 ms
2018-02-05 17:08:25,970 INFO[org.apache.spark.executor.Executor:54] - Finished task 0.0 in stage 1.0 (TID 1). 1082 bytes result sent to driver
2018-02-05 17:08:25,971 INFO[org.apache.spark.scheduler.TaskSetManager:54] - Finished task 0.0 in stage 1.0 (TID 1) in 69 ms on localhost (executor driver) (1/1)
2018-02-05 17:08:25,971 INFO[org.apache.spark.scheduler.TaskSchedulerImpl:54] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2018-02-05 17:08:25,972 INFO[org.apache.spark.scheduler.DAGScheduler:54] - ResultStage 1 (show at SchemaRDDSparkSQL.java:87) finished in 0.070 s
2018-02-05 17:08:25,972 INFO[org.apache.spark.scheduler.DAGScheduler:54] - Job 1 finished: show at SchemaRDDSparkSQL.java:87, took 0.106497 s
2018-02-05 17:08:26,009 INFO[org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator:54] - Code generated in 24.187208 ms
2018-02-05 17:08:26,026 INFO[org.apache.spark.SparkContext:54] - Invoking stop() from shutdown hook
2018-02-05 17:08:26,031 INFO[org.spark_project.jetty.server.AbstractConnector:310] - Stopped Spark@4b3c354a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
2018-02-05 17:08:26,035 INFO[org.apache.spark.ui.SparkUI:54] - Stopped Spark web UI at http://192.168.11.26:4040
2018-02-05 17:08:26,044 INFO[org.apache.spark.MapOutputTrackerMasterEndpoint:54] - MapOutputTrackerMasterEndpoint stopped!
2018-02-05 17:08:26,056 INFO[org.apache.spark.storage.memory.MemoryStore:54] - MemoryStore cleared
2018-02-05 17:08:26,056 INFO[org.apache.spark.storage.BlockManager:54] - BlockManager stopped
2018-02-05 17:08:26,060 INFO[org.apache.spark.storage.BlockManagerMaster:54] - BlockManagerMaster stopped
2018-02-05 17:08:26,063 INFO[org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54] - OutputCommitCoordinator stopped!
2018-02-05 17:08:26,066 INFO[org.apache.spark.SparkContext:54] - Successfully stopped SparkContext
2018-02-05 17:08:26,066 INFO[org.apache.spark.util.ShutdownHookManager:54] - Shutdown hook called
2018-02-05 17:08:26,067 INFO[org.apache.spark.util.ShutdownHookManager:54] - Deleting directory C:\Users\Administrator\AppData\Local\Temp\spark-a9fa22a4-532e-44d6-a8cf-4d77843906ef
